# app/bot.py
import re
import os
import html
import json
import logging
import asyncio
import time
import math
from typing import Iterable, AsyncIterable, Optional, List, Tuple

from aiogram import Bot, Dispatcher, F, types
from aiogram.filters import Command
from aiogram.exceptions import TelegramBadRequest
from aiogram.enums import ChatAction
from aiogram.types import FSInputFile, InputMediaPhoto

from .ooxml_lite import (
    build_index as oox_build_index,
    figure_lookup as oox_fig_lookup,
    table_lookup as oox_tbl_lookup,
)

# ---------- answer builder: –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å —Å—Ç—Ä–∏–º–æ–≤—É—é –≤–µ—Ä—Å–∏—é, —Ñ–æ–ª–±—ç–∫ –Ω–∞ –Ω–µ—Å—Ç—Ä–∏–º–æ–≤—É—é ----------
try:
    from .answer_builder import generate_answer, generate_answer_stream  # type: ignore
except Exception:
    from .answer_builder import generate_answer  # type: ignore
    generate_answer_stream = None  # —Å—Ç—Ä–∏–º–∞ –Ω–µ—Ç ‚Äî –±—É–¥–µ–º —Ñ–æ–ª–±—ç–∫–∞—Ç—å

from .config import Cfg, ProcessingState
from .db import (
    ensure_user, get_conn,
    set_document_indexer_version, get_document_indexer_version,
    CURRENT_INDEXER_VERSION,
    update_document_meta, delete_document_chunks,
    set_user_active_doc, get_user_active_doc,
    # ‚Üì –Ω–æ–≤–æ–µ –¥–ª—è FSM/–æ—á–µ—Ä–µ–¥–∏
    enqueue_pending_query, dequeue_all_pending_queries,
    get_processing_state, start_downloading,
)
from .parsing import parse_docx, parse_doc, save_upload
from .indexing import index_document
from .retrieval import (
    retrieve, build_context, invalidate_cache,
    retrieve_coverage, build_context_coverage,
    describe_figures_by_numbers,
)
from .intents import detect_intents

# ‚Üì –¥–æ–±–∞–≤–∏–ª–∏ –º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç –ø–æ-–ø–æ–¥–ø—É–Ω–∫—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑ ace
try:
    from .ace import plan_subtasks, answer_subpoint, _merge_subanswers as merge_subanswers  # type: ignore
except Exception:
    try:
        # –±—ç–∫–∞–ø: –µ—Å–ª–∏ –≤ ace —Ñ—É–Ω–∫—Ü–∏–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã —Å –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ–º
        from .ace import _plan_subtasks as plan_subtasks, _answer_subpoint as answer_subpoint, _merge_subanswers as merge_subanswers  # type: ignore
    except Exception:
        plan_subtasks = None   # type: ignore
        answer_subpoint = None # type: ignore
        merge_subanswers = None # type: ignore

# ---------- polza client: –ø—Ä–æ–±—É–µ–º —Å—Ç—Ä–∏–º, —Ñ–æ–ª–±—ç–∫ –Ω–∞ –æ–±—ã—á–Ω—ã–π —á–∞—Ç ----------
try:
    from .polza_client import (
        probe_embedding_dim,
        chat_with_gpt,
        chat_with_gpt_stream,
        vision_extract_values,
        # NEW: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –æ–±—ë—Ä—Ç–∫–∏ (—Ç–µ–∫—Å—Ç + –∫–∞—Ä—Ç–∏–Ω–∫–∏)
        chat_with_gpt_multimodal,
        chat_with_gpt_stream_multimodal,
    )  # type: ignore

    # NEW: –ø—Ä—è–º–æ–π –∏–Ω–¥–µ–∫—Å —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
    from .figures import (
        index_document as fig_index_document,
        load_index   as fig_load_index,
        find_figure  as fig_find,
        figure_display_name,
    )
except Exception:
    from .polza_client import probe_embedding_dim, chat_with_gpt  # type: ignore
    chat_with_gpt_stream = None
    vision_extract_values = None  # —Ñ–æ–ª–±—ç–∫: –µ—Å–ª–∏ –Ω–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏, –Ω–µ –ø–∞–¥–∞–µ–º
    # NEW: –º—è–≥–∫–∏–µ —Ñ–æ–ª–±—ç–∫–∏
    chat_with_gpt_multimodal = None  # type: ignore
    chat_with_gpt_stream_multimodal = None  # type: ignore

    # –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è figures, —á—Ç–æ–±—ã –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –Ω–µ –ø–∞–¥–∞–ª
    fig_index_document = None       # type: ignore
    fig_load_index = None           # type: ignore

    def fig_find(*args, **kwargs):  # type: ignore
        return []

    def figure_display_name(rec):   # type: ignore
        rec = rec or {}
        return str(
            rec.get("title")
            or rec.get("caption")
            or rec.get("num")
            or "–†–∏—Å—É–Ω–æ–∫"
        )



# –ù–û–í–û–ï: –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –ø—Ä–∏—ë–º–∞/–æ–±–æ–≥–∞—â–µ–Ω–∏—è (OCR —Ç–∞–±–ª–∏—Ü-–∫–∞—Ä—Ç–∏–Ω–æ–∫, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–µ–ª)
from .ingest_orchestrator import enrich_sections, ingest_document
# –ù–û–í–û–ï: –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ç–∞–±–ª–∏—Ü
from .analytics import analyze_table_by_num

# —É—Ç–∏–ª–∏—Ç—ã
from .utils import safe_filename, sha256_bytes, split_for_telegram, infer_doc_kind

# –≥–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: —Å–µ–º–∞–Ω—Ç–∏–∫–∞ + FTS/LIKE
from .lexsearch import best_context

# —Å—Ä–∞–∑—É –ø–æ–¥ —Ç–µ–∫—É—â–∏–º–∏ import‚Äô–∞–º–∏
from .paywall_stub import setup_paywall

# –≥–¥–µ —É –≤–∞—Å —Å–æ–∑–¥–∞—é—Ç—Å—è –æ–±—ä–µ–∫—Ç—ã –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞:
bot = Bot(Cfg.TG_TOKEN)
dp = Dispatcher()
# –¥–æ–±–∞–≤—å—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É (–æ–¥–∏–Ω —Ä–∞–∑):
setup_paywall(dp, bot)


# --------------------- –ü–ê–†–ê–ú–ï–¢–†–´ –°–¢–†–ò–ú–ò–ù–ì–ê (—Å –¥–µ—Ñ–æ–ª—Ç–∞–º–∏) ---------------------

STREAM_ENABLED: bool = getattr(Cfg, "STREAM_ENABLED", True)
STREAM_EDIT_INTERVAL_MS: int = getattr(Cfg, "STREAM_EDIT_INTERVAL_MS", 900)  # –∫–∞–∫ —á–∞—Å—Ç–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ
STREAM_MIN_CHARS: int = getattr(Cfg, "STREAM_MIN_CHARS", 120)               # –º–∏–Ω. –ø—Ä–∏—Ä–∞—â–µ–Ω–∏–µ –º–µ–∂–¥—É –∞–ø–¥–µ–π—Ç–∞–º–∏
STREAM_MODE: str = getattr(Cfg, "STREAM_MODE", "edit")                       # "edit" | "multi"
TG_MAX_CHARS: int = getattr(Cfg, "TG_MAX_CHARS", 3900)
FIG_MEDIA_LIMIT: int = getattr(Cfg, "FIG_MEDIA_LIMIT", 12)

# ‚Üì –ù–æ–≤–æ–µ: —É–ø—Ä–∞–≤–ª—è–µ–º ¬´–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π¬ª –¥–∞–∂–µ –∫–æ–≥–¥–∞ –Ω–µ —É–ø–∏—Ä–∞–µ–º—Å—è –≤ 4096
TG_SPLIT_TARGET: int = getattr(Cfg, "TG_SPLIT_TARGET", 1600)   # —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏
TG_SPLIT_MAX_PARTS: int = getattr(Cfg, "TG_SPLIT_MAX_PARTS", 3)  # –Ω–µ –±–æ–ª—å—à–µ 3 —Å–æ–æ–±—â–µ–Ω–∏–π
_SPLIT_ANCHOR_RE = re.compile(
    r"(?m)^(?:### .+|## .+|\*\*[^\n]+?\*\*|\d+[).] .+|- .+)$"
)  # –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã (–∑–∞–≥–æ–ª–æ–≤–∫–∏/—Å–ø–∏—Å–∫–∏)
STREAM_HEAD_START_MS: int = getattr(Cfg, "STREAM_HEAD_START_MS", 250)        # –ø–µ—Ä–≤—ã–π –∞–ø–¥–µ–π—Ç –±—ã—Å—Ç—Ä–µ–µ
FINAL_MAX_TOKENS: int = getattr(Cfg, "FINAL_MAX_TOKENS", 1600)
TYPE_INDICATION_EVERY_MS: int = getattr(Cfg, "TYPE_INDICATION_EVERY_MS", 2000)
# NEW: —Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º –¥–ª—è —Ä–∏—Å—É–Ω–∫–æ–≤ ‚Äî –Ω–µ –æ—Ç–¥–∞—ë–º —á–∏—Å–ª–∞ –±–µ–∑ –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
FIG_STRICT: bool = getattr(Cfg, "FIG_STRICT", True)
# ‚Üì –Ω–æ–≤–æ–µ: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–π –ø–æ–¥–∞—á–µ–π
MULTI_STEP_SEND_ENABLED: bool = getattr(Cfg, "MULTI_STEP_SEND_ENABLED", True)
MULTI_STEP_MIN_ITEMS: int = getattr(Cfg, "MULTI_STEP_MIN_ITEMS", 2)
MULTI_STEP_MAX_ITEMS: int = getattr(Cfg, "MULTI_STEP_MAX_ITEMS", 8)
MULTI_STEP_FINAL_MERGE: bool = getattr(Cfg, "MULTI_STEP_FINAL_MERGE", True)
MULTI_STEP_PAUSE_MS: int = getattr(Cfg, "MULTI_STEP_PAUSE_MS", 120)  # –º/—É –±–ª–æ–∫–∞–º–∏
MULTI_PASS_SCORE: int = getattr(Cfg, "MULTI_PASS_SCORE", 85)         # –ø–æ—Ä–æ–≥ –∫—Ä–∏—Ç–∏–∫–∞ –≤ ace

# --------------------- —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ ---------------------

# Markdown ‚Üí HTML (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ-–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ: **bold**, __bold__, *italic*, _italic_, `code`)
# --------------------- —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ ---------------------

# Markdown ‚Üí HTML (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ-–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ: –∑–∞–≥–æ–ª–æ–≤–∫–∏, **bold**, *italic*, `code`)
_MD_H_RE       = re.compile(r"(?m)^\s{0,3}#{1,6}\s+(.+?)\s*$")
_MD_BOLD_RE    = re.compile(r"\*\*(.+?)\*\*", re.DOTALL)
_MD_BOLD2_RE   = re.compile(r"__(.+?)__", re.DOTALL)
_MD_ITALIC_RE  = re.compile(r"(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)", re.DOTALL)
_MD_ITALIC2_RE = re.compile(r"(?<!_)_(?!_)(.+?)(?<!_)_(?!_)", re.DOTALL)
_MD_CODE_RE    = re.compile(r"`([^`]+)`")

def _to_html(text: str) -> str:
    if not text:
        return ""
    original = text

    # 0) –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–º–µ–Ω–∏–º –∫–æ–¥–æ–≤—ã–µ —Å–ø–∞–Ω—ã –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞–º–∏
    code_buf = []
    def _stash(m):
        code_buf.append(m.group(1))
        return f"@@CODE{len(code_buf)-1}@@"

    txt = _MD_CODE_RE.sub(_stash, original)
    txt = html.escape(txt)

    # 1) –∑–∞–≥–æ–ª–æ–≤–∫–∏/–∂–∏—Ä–Ω—ã–π/–∫—É—Ä—Å–∏–≤
    txt = _MD_H_RE.sub(r"<b>\1</b>", txt)
    txt = _MD_BOLD_RE.sub(r"<b>\1</b>", txt)
    txt = _MD_BOLD2_RE.sub(r"<b>\1</b>", txt)
    txt = _MD_ITALIC_RE.sub(r"<i>\1</i>", txt)
    txt = _MD_ITALIC2_RE.sub(r"<i>\1</i>", txt)

    # 2) –∑–∞—á–∏—Å—Ç–∫–∞ ¬´–≤–∏—Å—è—á–∏—Ö¬ª ** ‚Äî —É–∂–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ (–≤ –∫–æ–¥–µ –∏—Ö –Ω–µ—Ç)
    txt = re.sub(r"(?<!\*)\*\*(?!\*)", "", txt)

    # 3) –≤–µ—Ä–Ω—É—Ç—å –∫–æ–¥–æ–≤—ã–µ —Å–ø–∞–Ω—ã, —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–≤ –∏—Ö –∫–æ–Ω—Ç–µ–Ω—Ç
    def _restore(m):
        i = int(m.group(1))
        return f"<code>{html.escape(code_buf[i])}</code>"
    txt = re.sub(r"@@CODE(\d+)@@", _restore, txt)

    return txt


# -------- –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ --------
_GREET_RE = re.compile(
    r"(?i)\b(–ø—Ä–∏–≤–µ—Ç|–∑–¥—Ä–∞–≤—Å—Ç–≤|–¥–æ–±—Ä—ã–π\s*(–¥–µ–Ω—å|–≤–µ—á–µ—Ä|—É—Ç—Ä–æ)|hello|hi|hey|—Ö–∞–π|—Å–∞–ª—é—Ç|–∫—É)\b"
)

def _is_greeting(text: str) -> bool:
    t = (text or "").strip()
    if not t:
        return False
    # –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ —Ñ—Ä–∞–∑—ã, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
    return bool(_GREET_RE.search(t))


def _split_multipart(text: str,
                     *,
                     target: int = TG_SPLIT_TARGET,
                     max_parts: int = TG_SPLIT_MAX_PARTS,  # –ø–∞—Ä–∞–º–µ—Ç—Ä –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                     hard: int = TG_MAX_CHARS) -> list[str]:
    s = text or ""
    if not s:
        return []
    parts: list[str] = []
    rest = s

    # —Ä–µ–∂–µ–º –ø–æ ¬´–∫—Ä–∞—Å–∏–≤—ã–º¬ª –≥—Ä–∞–Ω–∏—Ü–∞–º —Å—Ç–æ–ª—å–∫–æ —Ä–∞–∑, —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ
    while len(rest) > target:
        cut = -1
        for m in _SPLIT_ANCHOR_RE.finditer(rest[: min(len(rest), hard)]):
            if m.start() < target:
                cut = m.start()
        if cut <= 0:
            cut = _smart_cut_point(rest, min(hard, target))
        parts.append(rest[:cut].rstrip())
        rest = rest[cut:].lstrip()

    # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ö–≤–æ—Å—Ç –∏ —Å–≤–µ—Ä—Ö–∂—ë—Å—Ç–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ª–∏–º–∏—Ç—É Telegram
    while rest:
        parts.append(rest[:hard])
        rest = rest[hard:]
    return parts


async def _send(m: types.Message, text: str):
    """–ë–µ—Ä–µ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞—Å—Ç—è–º–∏ –≤ HTML-—Ä–µ–∂–∏–º–µ (–Ω–µ—Å—Ç—Ä–∏–º–æ–≤—ã–π —Ñ–æ–ª–±—ç–∫)."""
    for chunk in _split_multipart(text or ""):
        await m.answer(_to_html(chunk), parse_mode="HTML", disable_web_page_preview=True)


# ---- Verbosity helpers ----
def _detect_verbosity(text: str) -> str:
    t = (text or "").lower()
    detailed = re.search(r"\b(–ø–æ–¥—Ä–æ–±–Ω|–¥–µ—Ç–∞–ª|—Ä–∞–∑–≤—ë—Ä–Ω—É—Ç|—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç|—Ä–∞–∑–±–æ—Ä|explain in detail|detailed)\b", t)
    brief    = re.search(r"\b(–∫—Ä–∞—Ç–∫|–≤\s*–¥–≤—É—Ö\s*—Å–ª–æ–≤|–∫–æ—Ä–æ—Ç–∫|–≤—ã–∂–∏–º–∫|summary|brief)\b", t)
    if detailed:
        return "detailed"
    if brief:
        return "brief"
    # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ ‚Äî —Å–∫–æ—Ä–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç
    if len(t) > 600:
        return "detailed"
    return "normal"


def _verbosity_addendum(verbosity: str, what: str = "–æ—Ç–≤–µ—Ç") -> str:
    """
    –ù–µ–±–æ–ª—å—à–∞—è –ø—Ä–∏–ø–∏—Å–∫–∞ –∫ –ø—Ä–æ–º–ø—Ç—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç—Ä–µ–±—É–µ–º–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏.
    `what` ‚Äî —á—Ç–æ –∏–º–µ–Ω–Ω–æ –Ω—É–∂–Ω–æ –æ–ø–∏—Å—ã–≤–∞—Ç—å: '–æ—Ç–≤–µ—Ç', '–æ–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤' –∏ —Ç.–ø.
    """
    what = (what or "–æ—Ç–≤–µ—Ç").strip()

    if verbosity == "short":
        # –ø—Ä–∏–º–µ—Ä: "–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ (–ø–æ –æ–ø–∏—Å–∞–Ω–∏—é —Ä–∏—Å—É–Ω–∫–æ–≤)."
        return f" –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ (–ø–æ {what})."

    if verbosity == "detailed":
        # –ø—Ä–∏–º–µ—Ä: "–î–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ, –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–∏—Å—É–Ω–∫–æ–≤."
        return f" –î–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ, –ø–æ–¥—Ä–æ–±–Ω–æ–µ {what}."

    # default ‚Äî –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —É–∫–∞–∑–∞–Ω–∏–π
    return ""

# --------------------- STREAM: –≤—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ---------------------

def _now_ms() -> int:
    return int(time.time() * 1000)

async def _typing_loop(chat_id: int, stop_event: asyncio.Event):
    try:
        while not stop_event.is_set():
            await bot.send_chat_action(chat_id, ChatAction.TYPING)
            try:
                await asyncio.wait_for(stop_event.wait(), timeout=TYPE_INDICATION_EVERY_MS / 1000)
            except asyncio.TimeoutError:
                # –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ü–∏–∫–ª, —á—Ç–æ–±—ã –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–ª–∞—Ç—å "typing"
                pass
    except Exception:
        # –≥–ª—É—à–∏–º –ª—é–±—ã–µ –Ω–µ—Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏, —á—Ç–æ–±—ã –Ω–µ —Ä–æ–Ω—è—Ç—å —Å—Ç—Ä–∏–º
        pass



def _section_context(owner_id: int, doc_id: int, sec: str, *, max_chars: int = 9000) -> str:
    # 1) –≥–µ–Ω–µ—Ä–∏–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø–∏—Å–∏ –Ω–æ–º–µ—Ä–∞
    base = (sec or "").strip()
    variants = {
        base,
        base.replace(" ", ""),
        base.replace(" ", "").replace(",", "."),
        base.replace(" ", "").replace(".", ","),
    }
    prefixes = ["", "–†–∞–∑–¥–µ–ª ", "–ü—É–Ω–∫—Ç ", "–ì–ª–∞–≤–∞ ", "–ü–æ–¥—Ä–∞–∑–¥–µ–ª "]
    patterns = [f"%{v}%" for v in variants]
    patterns += [f"%{p}{base}%" for p in prefixes]
    # —É–±–µ—Ä—ë–º –¥—É–±–ª–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–º —Ä–∞–∑—É–º–Ω—ã–º —á–∏—Å–ª–æ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤
    patterns = list(dict.fromkeys(patterns))[:8]

    con = get_conn()
    cur = con.cursor()

    rows = []
    if patterns:
        placeholders = " OR ".join(["section_path LIKE ?"] * len(patterns))
        cur.execute(
            f"""
            SELECT page, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=? AND ({placeholders})
            ORDER BY page ASC, id ASC
            """,
            (owner_id, doc_id, *patterns),
        )
        rows = cur.fetchall() or []

    # 2) —Ñ–æ–ª–±—ç–∫: –Ω–∞–π–¥—ë–º heading —Å –Ω–æ–º–µ—Ä–æ–º –∏ –≤–æ–∑—å–º—ë–º –µ–≥–æ —Å–µ–∫—Ü–∏—é
    if not rows:
        has_et = _table_has_columns(con, "chunks", ["element_type"])
        if has_et:
            cur.execute(
                """
                SELECT section_path
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                AND (element_type='heading' OR element_type IS NULL)
                AND (section_path LIKE ? OR text LIKE ?)
                ORDER BY page ASC, id ASC LIMIT 1
                """,
                (owner_id, doc_id, f"%{base}%", f"%{base}%"),
            )
        else:
            # —Å—Ç–∞—Ä–∞—è —Å—Ö–µ–º–∞ ‚Äî –±–µ–∑ —É—Å–ª–æ–≤–∏—è –ø–æ element_type
            cur.execute(
                """
                SELECT section_path
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                AND (section_path LIKE ? OR text LIKE ?)
                ORDER BY page ASC, id ASC LIMIT 1
                """,
                (owner_id, doc_id, f"%{base}%", f"%{base}%"),
            )
        h = cur.fetchone()
        if h and h["section_path"]:
            cur.execute(
                """
                SELECT page, section_path, text
                FROM chunks
                WHERE owner_id=? AND doc_id=? AND section_path=?
                ORDER BY page ASC, id ASC
                """,
                (owner_id, doc_id, h["section_path"]),
            )
            rows = cur.fetchall() or []

    con.close()
    if not rows:
        return ""

    parts, total = [], 0
    header_inserted = False
    for r in rows:
        secpath = (r["section_path"] or "").strip()
        t = (r["text"] or "").strip()
        if not t:
            continue
        chunk = (f"[{secpath}]\n{t}") if not header_inserted else t
        header_inserted = True
        if total + len(chunk) > max_chars:
            parts.append(chunk[: max_chars - total])
            break
        parts.append(chunk)
        total += len(chunk)
    return "\n\n".join(parts)


def _ensure_iterable(stream_obj) -> Iterable[str]:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ (a)—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏—Ç–µ—Ä–∞—Ç–æ—Ä —Å—Ç—Ä–æ–∫; –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞ –ø—Ä–∏–ª–µ—Ç–µ–ª–∞ –∫–æ—Ä—É—Ç–∏–Ω–∞."""
    import inspect

    # –ï—Å–ª–∏ —ç—Ç–æ –∫–æ—Ä—É—Ç–∏–Ω–∞ ‚Äî –æ–±–µ—Ä–Ω—ë–º –≤ async-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∞—á–∞–ª–∞ –µ—ë await-–∏—Ç,
    # –∞ –ø–æ—Ç–æ–º —É–∂–µ –∏—Ç–µ—Ä–∏—Ä—É–µ—Ç—Å—è –ø–æ —Ä–µ–∞–ª—å–Ω–æ–º—É —Å—Ç—Ä–∏–º—É.
    if inspect.iscoroutine(stream_obj):
        async def _await_then_iter():
            real = await stream_obj
            if hasattr(real, "__aiter__"):
                async for chunk in real:
                    yield chunk
            else:
                for chunk in real:
                    yield chunk
        return _await_then_iter()

    if hasattr(stream_obj, "__aiter__"):
        async def _drain_to_queue(q: asyncio.Queue):
            try:
                async for chunk in stream_obj:  # type: ignore
                    await q.put(chunk or "")
            except Exception:
                pass
            finally:
                await q.put(None)

        queue: asyncio.Queue = asyncio.Queue()

        async def _producer():
            await _drain_to_queue(queue)

        asyncio.create_task(_producer())

        async def _async_iter():
            while True:
                item = await queue.get()
                if item is None:
                    break
                yield item
        return _async_iter()

    return stream_obj

async def _iterate_chunks(stream_obj) -> AsyncIterable[str]:
    """–ï–¥–∏–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —á–∞–Ω–∫–æ–≤ (—É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∏ —Å sync-, –∏ —Å async-–∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞–º–∏)."""
    if hasattr(stream_obj, "__aiter__"):
        async for ch in stream_obj:
            if ch:
                yield str(ch)
        return
    for ch in stream_obj:
        if ch:
            yield str(ch)

def _smart_cut_point(s: str, limit: int) -> int:
    """–ò—â–µ–º ¬´–∫—Ä–∞—Å–∏–≤–æ–µ¬ª –º–µ—Å—Ç–æ —Ä–∞–∑—Ä–µ–∑–∞ <= limit (–ø–æ –ø–µ—Ä–µ–Ω–æ—Å—É/—Ç–æ—á–∫–µ/–ø—Ä–æ–±–µ–ª—É)."""
    if len(s) <= limit:
        return len(s)
    cut = s.rfind("\n", 0, limit)
    if cut == -1:
        cut = s.rfind(". ", 0, limit)
    if cut == -1:
        cut = s.rfind(" ", 0, limit)
    if cut == -1:
        cut = limit
    return max(1, cut)

# --- [VISION] helpers: –≤—ã–±—Ä–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ –ø–∞—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π ---
def _pick_images_from_hits(hits: list[dict], limit: int = 3) -> list[str]:
    acc: list[str] = []
    for h in hits or []:
        attrs = (h.get("attrs") or {})
        for p in (attrs.get("images") or []):
            if p and os.path.exists(p) and p not in acc:
                acc.append(p)
            if len(acc) >= limit:
                return acc
    return acc

def _pairs_to_bullets(pairs: list[dict]) -> str:
    lines = []
    for r in (pairs or []):
        lab = (str(r.get("label") or "")).strip()
        val = (str(r.get("value") or "")).strip()
        unit = (str(r.get("unit") or "")).strip()
        if lab or val:
            lines.append(f"‚Äî {lab}: {val}" + (f" {unit}" if unit else ""))
    return "\n".join(lines)

async def _stream_to_telegram(m: types.Message, stream, head_text: str = "‚åõÔ∏è –ü–µ—á–∞—Ç–∞—é –æ—Ç–≤–µ—Ç‚Ä¶") -> None:
    current_text = ""
    sent_parts = 0
    initial = await m.answer(_to_html(head_text), parse_mode="HTML", disable_web_page_preview=True)
    last_edit_at = _now_ms() - STREAM_HEAD_START_MS
    stop_typer = asyncio.Event()
    typer_task = asyncio.create_task(_typing_loop(m.chat.id, stop_event=stop_typer))

    # üîß –Ω–æ–≤–æ–µ: –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –≤ multi –±–æ–ª—å—à–µ –Ω–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º initial
    freeze_initial = False

    try:
        async for delta in _iterate_chunks(_ensure_iterable(stream)):
            current_text += delta

            # 3.a) –º—É–ª—å—Ç–∏-—Ä–µ–∂–∏–º: —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–æ—Ä—Ü–∏—è–º–∏
            if STREAM_MODE == "multi" and len(current_text) >= TG_SPLIT_TARGET:
                cut = -1
                for mm in _SPLIT_ANCHOR_RE.finditer(current_text[: min(len(current_text), TG_MAX_CHARS)]):
                    if mm.start() < TG_SPLIT_TARGET:
                        cut = mm.start()
                if cut <= 0:
                    cut = _smart_cut_point(current_text, min(TG_MAX_CHARS, TG_SPLIT_TARGET))

                part = current_text[:cut].rstrip()
                try:
                    if sent_parts == 0:
                        await initial.edit_text(_to_html(part), parse_mode="HTML", disable_web_page_preview=True)
                        freeze_initial = True  # <- –±–æ–ª—å—à–µ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º initial
                    else:
                        await m.answer(_to_html(part), parse_mode="HTML", disable_web_page_preview=True)
                except TelegramBadRequest:
                    await m.answer(_to_html(part), parse_mode="HTML", disable_web_page_preview=True)

                sent_parts += 1
                current_text = current_text[cut:].lstrip()
                last_edit_at = _now_ms()
                continue

            # 3.b) –∑–∞—â–∏—Ç–∞ –æ—Ç –ª–∏–º–∏—Ç–∞
            if len(current_text) >= TG_MAX_CHARS:
                cut = _smart_cut_point(current_text, TG_MAX_CHARS)
                final_part = current_text[:cut]

                if STREAM_MODE == "multi" and (freeze_initial or sent_parts > 0):
                    # üîß –≤ multi –Ω–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º initial –ø–æ—Å–ª–µ 1-–π —á–∞—Å—Ç–∏
                    await m.answer(_to_html(final_part), parse_mode="HTML", disable_web_page_preview=True)
                else:
                    try:
                        await initial.edit_text(_to_html(final_part), parse_mode="HTML", disable_web_page_preview=True)
                    except TelegramBadRequest:
                        await m.answer(_to_html(final_part), parse_mode="HTML", disable_web_page_preview=True)

                current_text = current_text[cut:].lstrip()
                # üîß –Ω–æ–≤—ã–π –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –≤ edit-—Ä–µ–∂–∏–º–µ
                if STREAM_MODE == "edit":
                    initial = await m.answer(_to_html("‚Ä¶"), parse_mode="HTML", disable_web_page_preview=True)
                last_edit_at = _now_ms()
                continue

            # 3.c) –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∫–∏ ‚Äî üîß –¢–û–õ–¨–ö–û –≤ —Ä–µ–∂–∏–º–µ edit
            now = _now_ms()
            if STREAM_MODE == "edit" and (now - last_edit_at) >= STREAM_EDIT_INTERVAL_MS and len(current_text) >= STREAM_MIN_CHARS:
                try:
                    await initial.edit_text(_to_html(current_text), parse_mode="HTML", disable_web_page_preview=True)
                    last_edit_at = now
                except TelegramBadRequest:
                    pass

        # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ö–≤–æ—Å—Ç
        if current_text:
            try:
                if STREAM_MODE == "multi" and sent_parts > 0:
                    await m.answer(_to_html(current_text), parse_mode="HTML", disable_web_page_preview=True)
                else:
                    await initial.edit_text(_to_html(current_text), parse_mode="HTML", disable_web_page_preview=True)
            except TelegramBadRequest:
                await m.answer(_to_html(current_text), parse_mode="HTML", disable_web_page_preview=True)

    finally:
        stop_typer.set()
        try:
            await typer_task
        except Exception:
            pass


async def _run_multistep_answer(
    m: types.Message,
    uid: int,
    doc_id: int,
    q_text: str,
    *,
    discovered_items: list[dict] | None = None,
) -> bool:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º: –ø–ª–∞–Ω ‚Üí –ø–æ –∫–∞–∂–¥–æ–º—É –ø–æ–¥–ø—É–Ω–∫—Ç—É –æ—Ç–¥–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç ‚Üí (–æ–ø—Ü.) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π merge.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –ø—É—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –Ω–∏—á–µ–≥–æ –¥–∞–ª—å—à–µ –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ.
    """
    if not MULTI_STEP_SEND_ENABLED:
        return False
    if not (plan_subtasks and answer_subpoint and merge_subanswers):
        # –Ω–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ ace ‚Äî –≤—ã—Ö–æ–¥–∏–º
        return False

    # –ø–ª–∞–Ω –∏–∑ coverage –∏–ª–∏ —Å—Ç—Ä–æ–∏–º –ø–ª–∞–Ω–µ—Äo–º
            # –ø–ª–∞–Ω –∏–∑ coverage –∏–ª–∏ —Å—Ç—Ä–æ–∏–º –ø–ª–∞–Ω–µ—Ä–æ–º
    items = (discovered_items or [])
    if not items:
        try:
            items = plan_subtasks(q_text) or []
        except Exception:
            items = []

    # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∏ dict, –∏ str
    norm_items: list[dict] = []
    for idx, it in enumerate(items, start=1):
        if isinstance(it, str):
            norm_items.append({"id": idx, "ask": it.strip()})
        elif isinstance(it, dict):
            ask = (it.get("ask") or it.get("text") or it.get("q") or "").strip()
            if ask:
                norm_items.append({"id": it.get("id") or idx, "ask": ask})
    items = [it for it in norm_items if (it.get("ask") or "").strip()]
    if len(items) < MULTI_STEP_MIN_ITEMS:
        return False

    # –æ—Ç—Å–µ—á—ë–º —Ö–≤–æ—Å—Ç –ø–æ –ª–∏–º–∏—Ç—É
    items = items[:MULTI_STEP_MAX_ITEMS]

    # –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–æ–Ω—Å
    preview = "\n".join([f"{i+1}) {(it['ask'] or '').strip()}" for i, it in enumerate(items)])
    await _send(m, f"–í–æ–ø—Ä–æ—Å –º–Ω–æ–≥–æ—á–∞—Å—Ç–Ω—ã–π. –û—Ç–≤–µ—á–∞—é –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º ({len(items)} —à—Ç.):\n\n{preview}")

    subanswers: list[str] = []

    # coverage-aware —Ä–∞–∑–¥–∞—á–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: —Ä–∞–∑–ª–æ–∂–∏–º –≤—ã–∂–∏–º–∫–∏ –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º
    cov = None
    try:
        cov = retrieve_coverage(owner_id=uid, doc_id=doc_id, question=q_text)
    except Exception:
        cov = None
    cov_map = (cov or {}).get("by_item") or {}


    # –ø–æ –æ—á–µ—Ä–µ–¥–∏: A ‚Üí send, B ‚Üí send, ...
    for i, it in enumerate(items, start=1):
        ask = (it.get("ask") or "").strip()
        # –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞
        ctx_text = ""
        try:
            # –µ—Å–ª–∏ –µ—Å—Ç—å coverage-–±–∞–∫–µ—Ç ‚Äî —Å–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä—è–º–æ –∏–∑ —á–∞–Ω–∫–æ–≤ –ø–æ–¥–ø—É–Ω–∫—Ç–∞
            bucket = cov_map.get(str(it.get("id") or i)) or []
            if bucket:
                ctx_text = build_context_coverage(bucket, items_count=1)
        except Exception:
            ctx_text = ""
        # 2) —Ñ–æ–ª–±—ç–∫–∏
        if not ctx_text:
            ctx_text = best_context(uid, doc_id, ask, max_chars=6000) or ""
        if not ctx_text:
            hits = retrieve(uid, doc_id, ask, top_k=8)
            if hits:
                ctx_text = build_context(hits)
        if not ctx_text:
            ctx_text = _first_chunks_context(uid, doc_id, n=12, max_chars=6000)

        # –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç—É (–∫–∞—Å—Ç–æ–º–Ω–∞—è –ø–æ–¥—Å–∫–∞–∑–∫–∞ –≤ ace + –∫—Ä–∏—Ç–∏–∫–∞/–ø—Ä–∞–≤–∫–∞)
        try:
            part = answer_subpoint(ask, ctx_text, MULTI_PASS_SCORE).strip()
        except Exception as e:
            logging.exception("answer_subpoint failed: %s", e)
            part = ""

        # –æ—Ç–ø—Ä–∞–≤–∫–∞ –±–ª–æ–∫–∞
        header = f"**{i}. {ask}**\n\n"
        await _send(m, header + (part or "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –ø–æ —ç—Ç–æ–º—É –ø–æ–¥–ø—É–Ω–∫—Ç—É."))
        subanswers.append(f"{header}{part}")

        # –º–∏–∫—Ä–æ–ø–∞—É–∑a, —á—Ç–æ–±—ã –Ω–µ —É–ø–µ—Ä–µ—Ç—å—Å—è –≤ rate/—á–∞—Ç—ã
        await asyncio.sleep(MULTI_STEP_PAUSE_MS / 1000)

    # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –±–ª–æ–∫
    if MULTI_STEP_FINAL_MERGE:
        try:
            merged = merge_subanswers(q_text, items, subanswers).strip()
            if merged:
                await _send(m, "**–ò—Ç–æ–≥–æ–≤—ã–π —Å–≤–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç**\n\n" + merged)
        except Exception as e:
            logging.exception("merge_subanswers failed: %s", e)

    return True

# summarizer (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç)
try:
    from .summarizer import is_summary_intent, overview_context  # –º–æ–≥—É—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å ‚Äî –µ—Å—Ç—å —Ñ–æ–ª–±—ç–∫–∏ –Ω–∏–∂–µ
except Exception:
    def is_summary_intent(text: str) -> bool:
        return bool(re.search(
            r"\b(—Å—É—Ç—å|–∫—Ä–∞—Ç–∫–æ|–æ—Å–Ω–æ–≤–Ω|–≥–ª–∞–≤–Ω|summary|overview|–∏—Ç–æ–≥|–≤—ã–≤–æ–¥)\w*\b",
            text or "",
            re.IGNORECASE,
        ))

    def overview_context(owner_id: int, doc_id: int, max_chars: int = 6000) -> str:
        con = get_conn()
        cur = con.cursor()
        cur.execute(
            """
            SELECT page, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
              AND (text LIKE '[–ó–∞–≥–æ–ª–æ–≤–æ–∫]%%'
                   OR text LIKE '%%–¶–µ–ª—å%%'
                   OR text LIKE '%%–ó–∞–¥–∞—á%%'
                   OR text LIKE '%%–í–≤–µ–¥–µ–Ω%%'
                   OR text LIKE '%%–ó–∞–∫–ª—é—á–µ–Ω%%'
                   OR text LIKE '%%–í—ã–≤–æ–¥%%')
            ORDER BY page ASC, id ASC
            LIMIT 14
            """,
            (owner_id, doc_id),
        )
        rows = cur.fetchall()
        con.close()
        if not rows:
            return ""
        parts, total = [], 0
        for r in rows:
            block = f"{(r['text'] or '').strip()}"
            if total + len(block) > max_chars:
                break
            parts.append(block)
            total += len(block)
        return "\n\n".join(parts)

# NEW: —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ (—á–µ—Ä–µ–∑ vision-–∞–Ω–∞–ª–∏–∑ –≤ summarizer.py)
try:
    from .summarizer import extract_figure_value as summ_extract_figure_value  # type: ignore
except Exception:
    # –µ—Å–ª–∏ summarizer –∏–ª–∏ —Å–∞–º–∞ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã ‚Äî –ø—Ä–æ—Å—Ç–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ–ª–±—ç–∫ –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º
    summ_extract_figure_value = None  # type: ignore

# vision-–æ–ø–∏—Å–∞–Ω–∏–µ —Ä–∏—Å—É–Ω–∫–æ–≤ (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –æ—Ç–≤–µ—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ñ–æ–ª–±—ç–∫–æ–º)
try:
    from .summarizer import describe_figures as vision_describe_figures
except Exception:
    def vision_describe_figures(owner_id: int, doc_id: int, numbers: list[str]) -> str:
        if not numbers:
            return "–ù–µ —É–∫–∞–∑–∞–Ω—ã –Ω–æ–º–µ—Ä–∞ —Ä–∏—Å—É–Ω–∫–æ–≤."
        return "–û–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (vision-–º–æ–¥—É–ª—å –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω)."


# NEW: —Ç–æ—á–µ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏ (—Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç + —á–∏—Å–ª–∞)
try:
    from .vision_analyzer import analyze_figure as va_analyze_figure  # type: ignore
except Exception:
    va_analyze_figure = None  # type: ignore

# –ì–û–°–¢-–≤–∞–ª–∏–¥–∞—Ç–æ—Ä (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç)
try:
    from .validators_gost import validate_gost, render_report
except Exception:
    validate_gost = None
    render_report = None


# ¬´–ê–∫—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç¬ª –≤ –ø–∞–º—è—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
ACTIVE_DOC: dict[int, int] = {}  # user_id -> doc_id
# NEW: –∫–æ—Ä–æ—Ç–∫–∞—è ¬´–ø–∞–º—è—Ç—å¬ª –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É–ø–æ–º—è–Ω—É—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
LAST_REF: dict[int, dict] = {}   # {uid: {"figure_nums": list[str], "area": "3.2"}}
FIG_INDEX: dict[int, dict] = {}
OOXML_INDEX: dict[int, dict] = {}

# NEW: –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–æ–º–µ—Ä–∞ —Ä–∞–∑–¥–µ–ª–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –∏ –¥–ª—è –∞–Ω–∞—Ñ–æ—Ä—ã ¬´—ç—Ç–æ—Ç –ø—É–Ω–∫—Ç/—Ä–∏—Å—É–Ω–æ–∫¬ª
_SECTION_NUM_RE = re.compile(
    r"(?i)\b(?:–≥–ª–∞–≤–∞\w*|—Ä–∞–∑–¥–µ–ª\w*|–ø—É–Ω–∫—Ç\w*|–ø–æ–¥—Ä–∞–∑–¥–µ–ª\w*|sec(?:tion)?\.?|chapter)"
    r"\s*(?:‚Ññ\s*)?((?:[A-Za-z–ê-–Ø–∞-—è](?=[\.\d]))?\s*\d+(?:[.,]\d+)*)"
)
_ANAPH_HINT_RE = re.compile(r"(?i)\b(—ç—Ç–æ—Ç|—ç—Ç–∞|—ç—Ç–æ|–¥–∞–Ω–Ω\w+|–ø—Ä–æ –Ω–µ–≥–æ|–ø—Ä–æ –Ω–µ—ë|–ø—Ä–æ –Ω–µ–µ)\b")


# ------------------------ –ì–∞—Ä–¥—Ä–µ–π–ª—ã ------------------------

_BANNED_PATTERNS = [
    r"jail ?break|system\s*prompt|developer\s*mode|dan\b|ignore (all|previous) (rules|instructions)",
    r"\b–≤–∑–ª–æ–º|—Ö–∞–∫–∏?|–∫–µ–π–≥–µ–Ω|–∫—Ä—è–∫|—Å–æ—Ü–∏–∞–ª—å–Ω(–∞—è|—ã–µ) –∏–Ω–∂–µ–Ω–µ—Ä–∏—è",
    r"\b–≤–∏—Ä—É—Å|–≤—Ä–µ–¥–æ–Ω–æ—Å|—ç–∫—Å–ø–ª–æ–π—Ç|–±–æ—Ç–Ω–µ—Ç|ddos\b",
    r"\b–æ—Ä—É–∂–∏|–≤–∑—Ä—ã–≤—á–∞—Ç|–±–æ–º–±|–Ω–∞—Ä–∫–æ—Ç|–ø–æ—Ä–Ω–æ|—ç—Ä–æ—Ç–∏–∫|18\+",
    r"\b–ø–∞—Å–ø–æ—Ä—Ç|—Å–Ω–∏–ª—Å|–∏–Ω–Ω\b.*(—Å–≥–µ–Ω–µ—Ä|–ø–æ–¥–¥–µ–ª)",
    r"\b–æ–±–æ–π(–¥|—Ç–∏)\b.*–∞–Ω—Ç–∏–ø–ª–∞–≥|–∞–Ω—Ç–∏–ø–ª–∞–≥–∏–∞—Ç|antiplagiat",
    r"sql.?–∏–Ω—ä–µ–∫|–∏–Ω—ä–µ–∫—Ü–∏(—è|–∏) sql",
]

def safety_check(text: str) -> str | None:
    t = (text or "").lower()
    for p in _BANNED_PATTERNS:
        if re.search(p, t, flags=re.IGNORECASE):
            return ("–ó–∞–ø—Ä–æ—Å –Ω–∞—Ä—É—à–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ "
                    "(–≤–∑–ª–æ–º/–≤—Ä–µ–¥–æ–Ω–æ—Å/–æ–±—Ö–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π/NSFW/–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ).")
    return None

_ALLOWED_HINT_WORDS = [
    "–≤–∫—Ä", "–¥–∏–ø–ª–æ–º", "–∫—É—Ä—Å–æ–≤", "–º–µ—Ç–æ–¥–æ–ª–æ–≥", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–ª–∏—Ç–æ–±–∑–æ—Ä",
    "–≥–∏–ø–æ—Ç–µ–∑", "—Ü–µ–ª—å", "–∑–∞–¥–∞—á", "–≤–≤–µ–¥–µ–Ω–∏–µ", "–∑–∞–∫–ª—é—á–µ–Ω", "–æ–±–∑–æ—Ä",
    "–æ—Ñ–æ—Ä–º–ª–µ–Ω", "–≥–æ—Å—Ç", "—Ç–∞–±–ª–∏—Ü", "—Ä–∏—Å—É–Ω–∫", "–∞–Ω—Ç–∏–ø–ª–∞–≥", "–ø–ª–∞–≥–∏–∞—Ç",
    "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü", "–∑–∞—â–∏—Ç—É", "–æ–ø—Ä–æ—Å", "–∞–Ω–∫–µ—Ç–∞", "–º–µ—Ç–æ–¥—ã", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫",
]

def topical_check(text: str) -> str | None:
    """
    –ú—è–≥–∫–æ–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–õ–¨–ö–û –∫–∞–∫ –ø–æ–¥—Å–∫–∞–∑–∫—É,
    –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    """
    t = (text or "").lower()
    if not any(w in t for w in _ALLOWED_HINT_WORDS):
        return ("–ü–æ–¥—Å–∫–∞–∑–∫–∞: —Å–∏–ª—å–Ω–µ–µ –≤—Å–µ–≥–æ –æ—Ç–≤–µ—á–∞—é –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –í–ö–† (–≥–ª–∞–≤—ã, —Ç–∞–±–ª–∏—Ü—ã, —Ä–∏—Å—É–Ω–∫–∏, –≤—ã–≤–æ–¥—ã). "
                "–ï—Å–ª–∏ –ø—Ä–∏—à–ª—ë—Ç–µ —Ñ–∞–π–ª –¥–∏–ø–ª–æ–º–∞ ‚Äî —Å–º–æ–≥—É –æ–±—ä—è—Å–Ω—è—Ç—å –ø—Ä—è–º–æ –ø–æ –≤–∞—à–µ–º—É —Ç–µ–∫—Å—Ç—É.")
    return None

# --------------------- –ë–î / —É—Ç–∏–ª–∏—Ç—ã ---------------------

def _table_has_columns(con, table: str, cols: list[str]) -> bool:
    cur = con.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    have = {row[1] for row in cur.fetchall()}
    return all(c in have for c in cols)



# --------------------- –¢–∞–±–ª–∏—Ü—ã: –ø–∞—Ä—Å–∏–Ω–≥/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ---------------------

_TABLE_ANY = re.compile(r"\b—Ç–∞–±–ª–∏—Ü\w*|\b—Ç–∞–±–ª\.\b|\b—Ç–∞–±–ª–∏—Ü–∞\w*|(?:^|\s)table(s)?\b", re.IGNORECASE)
# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º: 2.1, 3, A.1, –ê.1, –ü1.2
_TABLE_TITLE_RE = re.compile(r"(?i)\b—Ç–∞–±–ª–∏—Ü–∞\s+(\d+(?:[.,]\d+)*|[a-z–∞-—è]\.?\s*\d+(?:[.,]\d+)*)\b(?:\s*[‚Äî\-‚Äì]\s*(.+))?")
_COUNT_HINT = re.compile(r"\b—Å–∫–æ–ª—å–∫–æ\b|how many", re.IGNORECASE)
_WHICH_HINT = re.compile(r"\b–∫–∞–∫–∏(–µ|—Ö)\b|\b—Å–ø–∏—Å–æ–∫\b|\b–ø–µ—Ä–µ—á–∏—Å–ª\w*\b|\b–Ω–∞–∑–æ–≤\w*\b", re.IGNORECASE)

def _plural_tables(n: int) -> str:
    n_abs = abs(n) % 100
    n1 = n_abs % 10
    if 11 <= n_abs <= 14:
        return "—Ç–∞–±–ª–∏—Ü"
    if n1 == 1:
        return "—Ç–∞–±–ª–∏—Ü–∞"
    if 2 <= n1 <= 4:
        return "—Ç–∞–±–ª–∏—Ü—ã"
    return "—Ç–∞–±–ª–∏—Ü"

def _strip_table_prefix(s: str) -> str:
    return re.sub(r"^\[\s*—Ç–∞–±–ª–∏—Ü–∞\s*\]\s*", "", s or "", flags=re.IGNORECASE)

def _last_segment(name: str) -> str:
    s = (name or "").strip()
    if "/" in s:
        s = s.split("/")[-1].strip()
    s = _strip_table_prefix(s)
    s = re.sub(r"\s*[-‚Äì‚Äî]\s*", " ‚Äî ", s)
    s = re.sub(r"\s+", " ", s).strip(" .")
    return s

def _parse_table_title(text: str) -> tuple[str | None, str | None]:
    t = (text or "").strip()
    m = _TABLE_TITLE_RE.search(t)
    if not m:
        return (None, None)
    num = (m.group(1) or "").strip() or None
    title = (m.group(2) or "").strip() or None
    return (num, title)

def _shorten(s: str, limit: int = 120) -> str:
    # –ù–∏—á–µ–≥–æ –Ω–µ —Ä–µ–∂–µ–º ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å.
    return (s or "").strip()



# -------- –¢–∞–±–ª–∏—Ü—ã: –ø–æ–¥—Å—á—ë—Ç –∏ —Å–ø–∏—Å–æ–∫ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –ë–î) --------

def _distinct_table_basenames(uid: int, doc_id: int) -> list[str]:
    """
    –°–æ–±–∏—Ä–∞–µ–º ¬´–±–∞–∑–æ–≤—ã–µ¬ª –∏–º–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü (section_path –±–µ–∑ —Ö–≤–æ—Å—Ç–∞ ' [row ‚Ä¶]').
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ —Å –Ω–æ–≤—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ (table_row) –∏ —Å–æ —Å—Ç–∞—Ä—ã–º–∏.
    """
    con = get_conn()
    cur = con.cursor()

    # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–ø–µ—Ä–µ—Ç—å—Å—è –Ω–∞ —Ç–∏–ø—ã
    if _table_has_columns(con, "chunks", ["element_type"]):
        cur.execute(
            """
            SELECT DISTINCT
                CASE
                    WHEN instr(section_path, ' [row ')>0
                        THEN substr(section_path, 1, instr(section_path,' [row ')-1)
                    ELSE section_path
                END AS base_name
            FROM chunks
            WHERE doc_id=? AND owner_id=? AND element_type IN ('table','table_row')
            """,
            (doc_id, uid),
        )
    else:
        # –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å ‚Äî —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        cur.execute(
            """
            SELECT DISTINCT
                CASE
                    WHEN instr(section_path, ' [row ')>0
                        THEN substr(section_path, 1, instr(section_path,' [row ')-1)
                    ELSE section_path
                END AS base_name
            FROM chunks
            WHERE doc_id=? AND owner_id=? AND (
                  lower(section_path) LIKE '%—Ç–∞–±–ª–∏—Ü–∞%'
               OR lower(text)        LIKE '%—Ç–∞–±–ª–∏—Ü–∞%'
               OR section_path LIKE '–¢–∞–±–ª–∏—Ü–∞ %' COLLATE NOCASE
               OR text        LIKE '[–¢–∞–±–ª–∏—Ü–∞]%' COLLATE NOCASE
               OR lower(section_path) LIKE '%table %'
               OR lower(text)        LIKE '%table %'
            )
            """,
            (doc_id, uid),
        )

    base_items = [r["base_name"] for r in cur.fetchall() if r["base_name"]]
    con.close()
    base_items = sorted(set(base_items), key=lambda s: s.lower())
    return base_items

def _count_tables(uid: int, doc_id: int) -> int:
    return len(_distinct_table_basenames(uid, doc_id))

def _compose_display_from_attrs(attrs_json: str | None, base: str, first_row_text: str | None) -> str:
    """
    –ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è:
      1) –µ—Å—Ç—å caption_num ‚Üí '–¢–∞–±–ª–∏—Ü–∞ N ‚Äî tail/header/firstrow' (–≤—Å–µ–≥–¥–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å).
      2) –Ω–µ—Ç –Ω–æ–º–µ—Ä–∞ ‚Üí –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ: caption_tail/ header_preview/ first_row.
      3) —Ñ–æ–ª–±—ç–∫: –ø–∞—Ä—Å–∏–º –Ω–æ–º–µ—Ä –∏ —Ö–≤–æ—Å—Ç –∏–∑ base ('–¢–∞–±–ª–∏—Ü–∞ N ‚Äî ...') –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å –Ω–æ–º–µ—Ä–æ–º.
      4) –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã—à–ª–æ ‚Äî –±–µ—Ä—ë–º –∫–æ—Ä–æ—Ç–∫–∏–π base –±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤.
    """
    num = None
    tail = None
    header_preview = None
    if attrs_json:
        try:
            a = json.loads(attrs_json or "{}")
            num = a.get("caption_num") or a.get("label")
            tail = a.get("caption_tail") or a.get("title")
            header_preview = a.get("header_preview")
        except Exception:
            pass

    if num:
        num = str(num).replace(",", ".").strip()
        tail_like = (tail or header_preview or first_row_text or "").strip()
        return f"–¢–∞–±–ª–∏—Ü–∞ {num}" + (f" ‚Äî {_shorten(tail_like, 160)}" if tail_like else "")

    # –±–µ–∑ –Ω–æ–º–µ—Ä–∞ –≤ attrs ‚Äî –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∏–∑ base –∏ –ø–æ–∫–∞–∑–∞—Ç—å –° –Ω–æ–º–µ—Ä–æ–º
    num_b, title_b = _parse_table_title(_last_segment(base))
    if num_b:
        text_tail = title_b or first_row_text or header_preview
        return f"–¢–∞–±–ª–∏—Ü–∞ {num_b}" + (f" ‚Äî {_shorten(text_tail, 160)}" if text_tail else "")

    # –±–µ–∑ –Ω–æ–º–µ—Ä–∞ ‚Äî —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ
    if tail:
        return _shorten(str(tail), 160)
    if header_preview:
        return _shorten(str(header_preview), 160)
    if first_row_text:
        return _shorten(first_row_text, 160)

    s = _last_segment(base)
    s = re.sub(r"(?i)^\s*—Ç–∞–±–ª–∏—Ü–∞\s+\d+(?:\.\d+)*\s*", "", s).strip(" ‚Äî‚Äì-")
    return _shorten(s or "–¢–∞–±–ª–∏—Ü–∞", 160)


# ------------------------------ –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ------------------------------

_SOURCES_HINT = re.compile(
    r"\b(–∏—Å—Ç–æ—á–Ω–∏–∫(?:–∏|–æ–≤)?|—Å–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã|—Å–ø–∏—Å–æ–∫\s+–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤|–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ\w*|references?|bibliograph\w*)\b",
    re.IGNORECASE
)
_REF_LINE_RE = re.compile(r"^\s*(?:\[\d+\]|\d+[.)])\s+.+", re.MULTILINE)

def _count_sources(uid: int, doc_id: int) -> int:
    """
    –ü–æ–¥—Å—á—ë—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
      1) –µ—Å–ª–∏ –≤ –ë–î –µ—Å—Ç—å element_type='reference' ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ;
      2) –∏–Ω–∞—á–µ —Å–æ–±–∏—Ä–∞–µ–º –ª—é–±—ã–µ –Ω–µ–ø—É—Å—Ç—ã–µ –∞–±–∑–∞—Ü—ã –≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–π ¬´–∏—Å—Ç–æ—á–Ω–∏–∫–∏/–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞/‚Ä¶¬ª
         (–±–µ–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–ª–∞—Å—å —Å –Ω–æ–º–µ—Ä–∞).
    """
    con = get_conn()
    cur = con.cursor()
    has_type = _table_has_columns(con, "chunks", ["element_type"])

    total = 0
    if has_type:
        cur.execute(
            "SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=? AND element_type='reference'",
            (uid, doc_id),
        )
        row = cur.fetchone()
        total = int(row["c"] or 0)

    if total == 0:
        items = set()
        cur.execute(
            """
            SELECT element_type, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
            ORDER BY page ASC, id ASC
            """,
            (uid, doc_id),
        )
        raw_rows = cur.fetchall()
        for r in raw_rows:
            sec = (r["section_path"] or "").lower()
            if not any(k in sec for k in ("–∏—Å—Ç–æ—á–Ω–∏–∫", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ", "reference", "bibliograph")):
                continue
            et = (r["element_type"] or "").lower()
            if et in ("heading", "table", "figure", "table_row"):
                continue
            t = (r["text"] or "").strip()
            if not t:
                continue
            k = re.sub(r"\s+", " ", t).strip().rstrip(".").lower()
            if len(k) >= 5:
                items.add(k)
        total = len(items)

    con.close()
    return total


# --------- –±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç: –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏ ---------
_PRACTICAL_Q = re.compile(r"(–µ—Å—Ç—å –ª–∏|–Ω–∞–ª–∏—á–∏–µ|–ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ª–∏|–∏–º–µ–µ—Ç—Å—è –ª–∏).{0,40}–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫", re.IGNORECASE)

def _has_practical_part(uid: int, doc_id: int) -> bool:
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        """
        SELECT 1
        FROM chunks
        WHERE owner_id=? AND doc_id=? AND (
            lower(section_path) LIKE '%–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫%' OR
            lower(text)         LIKE '%–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫%'
        )
        LIMIT 1
        """,
        (uid, doc_id),
    )
    row = cur.fetchone()
    con.close()
    return row is not None


# ------------- –ì–û–°–¢-–∏–Ω—Ç–µ–Ω—Ç –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ -------------

_GOST_HINT = re.compile(r"\b(–≥–æ—Å—Ç|–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏|—à—Ä–∏—Ñ—Ç|–º–µ–∂—Å—Ç—Ä–æ—á|–∫–µ–≥–ª|–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏|–ø–æ–ª—è|–æ—Ñ–æ—Ä–º–∏—Ç—å)\w*\b", re.IGNORECASE)

async def _maybe_run_gost(m: types.Message, uid: int, doc_id: int, text: str) -> bool:
    """–ï—Å–ª–∏ –ø–æ—Ö–æ–∂–µ, —á—Ç–æ –ø—Ä–æ—Å—è—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –ì–û–°–¢. –í–æ–∑–≤—Ä–∞—â–∞–µ–º True, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç–∏–ª–∏."""
    if not validate_gost or not render_report:
        return False
    if not _GOST_HINT.search(text or ""):
        return False

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    if not row:
        return False

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
    except Exception:
        return False

    report = validate_gost(sections)
    text_rep = render_report(report, max_issues=25)
    await _send(m, text_rep)
    return True

def _cap(s: str, limit: int = 950) -> str:
    """–û–±—Ä–µ–∑–∞–µ–º caption –¥–ª—è media (—É TG –ª–∏–º–∏—Ç ~1024 —Å–∏–º–≤–æ–ª–∞)."""
    s = (s or "").strip()
    if len(s) <= limit:
        return s
    return s[:limit - 1].rstrip() + "‚Ä¶"

def _safe_fs_input(path: str) -> FSInputFile | None:
    try:
        p = os.path.abspath(path or "")
        if not os.path.isfile(p):
            return None
        return FSInputFile(p)
    except Exception:
        return None

def _media_groups_from_cards(cards: list[dict], *, per_group: int = 10, per_figure: int = 4) -> list[list[InputMediaPhoto]]:
    """
    –°–æ–±–∏—Ä–∞–µ–º InputMediaPhoto –∏–∑ –∫–∞—Ä—Ç–æ—á–µ–∫ describe_figures_by_numbers.
    –ù–µ –±–æ–ª—å—à–µ FIG_MEDIA_LIMIT –≤—Å–µ–≥–æ, per_group ‚Äî –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Telegram (10).
    """
    media: list[InputMediaPhoto] = []
    total = 0
    for c in cards or []:
        disp = c.get("display") or f"–†–∏—Å—É–Ω–æ–∫ {c.get('num') or ''}".strip()
        imgs = (c.get("images") or [])[:per_figure]
        if not imgs:
            continue
        cap = _cap(disp)
        first = True
        for img in imgs:
            if total >= FIG_MEDIA_LIMIT:
                break
            fh = _safe_fs_input(img)
            if not fh:
                continue
            # caption —Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–µ —Ñ–æ—Ç–æ —Ä–∏—Å—É–Ω–∫–∞ (TG best-practice)
            media.append(InputMediaPhoto(media=fh, caption=cap if first else None))
            total += 1
            first = False
        if total >= FIG_MEDIA_LIMIT:
            break

    # —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—É
    groups: list[list[InputMediaPhoto]] = []
    for i in range(0, len(media), per_group):
        groups.append(media[i:i + per_group])
    return groups

async def _send_media_from_cards(m: types.Message, cards: list[dict]) -> bool:
    """
    –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–µ–¥–∏–∞–≥—Ä—É–ø–ø—ã –ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏.
    """
    groups = _media_groups_from_cards(cards)
    sent_any = False
    for g in groups:
        if not g:
            continue
        try:
            await m.answer_media_group(g)
            sent_any = True
        except TelegramBadRequest:
            # –µ—Å–ª–∏ –º–µ–¥–∏–∞-–≥—Ä—É–ø–ø–∞ –Ω–µ –∑–∞—à–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–¥–Ω–æ —Ñ–æ—Ç–æ) ‚Äî –æ—Ç–ø—Ä–∞–≤–∏–º –ø–æ—à—Ç—É—á–Ω–æ
            for item in g:
                try:
                    await m.answer_photo(item.media, caption=item.caption)
                    sent_any = True
                except Exception:
                    pass
        except Exception:
            pass
    return sent_any

# ------------------------------ helpers ------------------------------

def _parse_by_ext(path: str) -> list[dict]:
    fname = (os.path.basename(path) or "").lower()
    if fname.endswith(".docx"):
        return parse_docx(path)
    if fname.endswith(".doc"):
        return parse_doc(path)
    raise RuntimeError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é —Ç–æ–ª—å–∫–æ .doc –∏ .docx.")

def _first_chunks_context(owner_id: int, doc_id: int, n: int = 10, max_chars: int = 6000) -> str:
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT page, section_path, text FROM chunks "
        "WHERE owner_id=? AND doc_id=? "
        "ORDER BY page ASC, id ASC LIMIT ?",
        (owner_id, doc_id, n)
    )
    rows = cur.fetchall()
    con.close()
    if not rows:
        return ""
    parts, total = [], 0
    for r in rows:
        block = f"{(r['text'] or '').strip()}"
        if total + len(block) > max_chars:
            break
        parts.append(block)
        total += len(block)
    return "\n\n".join(parts)


def _ooxml_get_index(doc_id: int) -> dict | None:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç OOXML-–∏–Ω–¥–µ–∫—Å –∏–∑ –ø–∞–º—è—Ç–∏ –∏–ª–∏ —Å –¥–∏—Å–∫–∞. –°–Ω–∞—á–∞–ª–∞ runtime/indexes/<doc_id>.json,
    –∑–∞—Ç–µ–º —Ñ–æ–ª–±—ç–∫ ‚Äî –∏—â–µ–º json —Å —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º meta.file (–ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É)."""
    idx = OOXML_INDEX.get(doc_id)
    if idx:
        return idx

    p = os.path.join("runtime", "indexes", f"{doc_id}.json")
    if os.path.isfile(p):
        try:
            with open(p, "r", encoding="utf-8") as f:
                idx = json.load(f)
            OOXML_INDEX[doc_id] = idx
            return idx
        except Exception:
            pass

    # —Ñ–æ–ª–±—ç–∫: –ø–æ–¥–æ–±—Ä–∞—Ç—å –∏–Ω–¥–µ–∫—Å –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
    try:
        con = get_conn()
        cur = con.cursor()
        cur.execute("SELECT path FROM documents WHERE id=?", (doc_id,))
        row = cur.fetchone()
        con.close()
        doc_path = os.path.abspath(row["path"]) if row else None
        if doc_path:
            idx_dir = os.path.join("runtime", "indexes")
            if os.path.isdir(idx_dir):
                for name in os.listdir(idx_dir):
                    if not name.endswith(".json"):
                        continue
                    try:
                        with open(os.path.join(idx_dir, name), "r", encoding="utf-8") as f:
                            cand = json.load(f)
                        if (cand.get("meta") or {}).get("file") == doc_path:
                            OOXML_INDEX[doc_id] = cand
                            return cand
                    except Exception:
                        continue
    except Exception:
        pass
    return None


def _ooxml_find_figure_by_label(idx: dict, num_str: str) -> dict | None:
    """
    –ò—â–µ–º –∑–∞–ø–∏—Å—å –æ —Ä–∏—Å—É–Ω–∫–µ –ø–æ –Ω–æ–º–µ—Ä—É –≤–∏–¥–∞ '2.3' –∏–∑ –ø–æ–¥–ø–∏—Å–∏.
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–º–µ–Ω–Ω–æ —Ç–µ–∫—Å—Ç –≤ caption/title, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Ü–µ–ª—É—é —á–∞—Å—Ç—å.
    """
    target = _num_norm_fig(num_str)
    if not target:
        return None
    figs = (idx or {}).get("figures") or []
    for f in figs:
        cap = (f.get("caption") or f.get("title") or "").strip()
        m = _FIG_TITLE_RE.search(cap)
        if not m:
            continue
        cap_num = _num_norm_fig(m.group(2))
        if cap_num == target:
            return f
    return None


# ---------- verbatim fallback –ø–æ —Ü–∏—Ç–∞—Ç–µ (—à–∏–Ω–≥–ª—ã + LIKE/NOCASE) ----------

def _normalize_for_like(s: str) -> str:
    s = (s or "")
    s = s.replace("\u00A0", " ")  # NBSP -> –ø—Ä–æ–±–µ–ª
    s = s.replace("¬´", '"').replace("¬ª", '"').replace("‚Äú", '"').replace("‚Äù", '"')
    s = s.replace("‚Äô", "'").replace("‚Äò", "'")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _make_shingles(s: str, min_len: int = 30, max_len: int = 90, step: int = 25) -> list[str]:
    s = _normalize_for_like(s)
    if not s:
        return []
    if len(s) <= max_len:
        return [s]
    out = []
    i = 0
    while i < len(s):
        chunk = s[i:i + max_len]
        if len(chunk) >= min_len:
            out.append(chunk)
        i += step
    return out[:6]

def verbatim_find(owner_id: int, doc_id: int, q_text: str, max_hits: int = 3) -> list[dict]:
    shingles = _make_shingles(q_text)
    if not shingles:
        return []
    con = get_conn()
    cur = con.cursor()
    hits: list[dict] = []
    for sh in shingles:
        pattern = f"%{sh}%"
        cur.execute(
            """
            SELECT page, section_path, text FROM chunks
            WHERE owner_id=? AND doc_id=? AND
                  replace(text, char(160), ' ') LIKE ? COLLATE NOCASE
            ORDER BY page ASC, id ASC
            LIMIT ?
            """,
            (owner_id, doc_id, pattern, max_hits - len(hits)),
        )
        for r in cur.fetchall():
            t = (r["text"] or "")
            t_norm = _normalize_for_like(t)
            pos = t_norm.lower().find(_normalize_for_like(sh).lower())
            if pos >= 0:
                s = max(pos - 120, 0)
                e = min(pos + len(sh) + 120, len(t_norm))
                hits.append({
                    "page": r["page"],
                    "section_path": r["section_path"],
                    "snippet": t_norm[s:e].strip(),
                })
            if len(hits) >= max_hits:
                con.close()
                return hits
    con.close()
    return hits


# ------------------------------ /start ------------------------------

@dp.message(Command("start"))
async def start(m: types.Message):
    ensure_user(str(m.from_user.id))
    await _send(m,
        "–ü—Ä–∏–≤–µ—Ç! –Ø —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ —Ç–≤–æ–µ–π –í–ö–†. –ü—Ä–∏—à–ª–∏ —Ñ–∞–π–ª –í–ö–† ‚Äî —è –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä—É—é –∏ –±—É–¥—É –æ–±—ä—è—Å–Ω—è—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: –≥–ª–∞–≤—ã –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, —Å–º—ã—Å–ª —Ç–∞–±–ª–∏—Ü/—Ä–∏—Å—É–Ω–∫–æ–≤, –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –∫ –∑–∞—â–∏—Ç–µ. –ú–æ–∂–µ—à—å –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç—å –≤–æ–ø—Ä–æ—Å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –Ω–∞–ø–∏—Å–∞—Ç—å –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º."
    )


# ------------------------------ /diag ------------------------------

@dp.message(Command("diag"))
async def cmd_diag(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid) or get_user_active_doc(uid)
    if not doc_id:
        await _send(m, "–ê–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ—Ç. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª –í–ö–† —Å–Ω–∞—á–∞–ª–∞.")
        return

    # –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ë–î
    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    path = row["path"] if row else "‚Äî"

    cur.execute("SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=?", (uid, doc_id))
    chunks_cnt = int(cur.fetchone()["c"])

    con.close()

    tables_cnt = _count_tables(uid, doc_id)
    figures_cnt = _list_figures_db(uid, doc_id, limit=999999)["count"]
    # NEW: –µ—Å–ª–∏ –≤ –ë–î 0 ‚Äî –≤–æ–∑—å–º—ë–º —á–∏—Å–ª–æ —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ OOXML-–∏–Ω–¥–µ–∫—Å–∞
    if figures_cnt == 0:
        idx_oox = _ooxml_get_index(doc_id)
        if idx_oox:
            figures_cnt = len(idx_oox.get("figures", []))
    sources_cnt = _count_sources(uid, doc_id)

    indexer_ver = get_document_indexer_version(doc_id) or 0

    txt = (
        f"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ #{doc_id}\n"
        f"‚Äî –ü—É—Ç—å: {path}\n"
        f"‚Äî –ß–∞–Ω–∫–æ–≤: {chunks_cnt}\n"
        f"‚Äî –¢–∞–±–ª–∏—Ü: {tables_cnt}\n"
        f"‚Äî –†–∏—Å—É–Ω–∫–æ–≤: {figures_cnt}\n"
        f"‚Äî –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {sources_cnt}\n"
        f"‚Äî –í–µ—Ä—Å–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: {indexer_ver} (—Ç–µ–∫—É—â–∞—è {CURRENT_INDEXER_VERSION})\n"
    )
    await _send(m, txt)


# ------------------------------ /reindex ------------------------------

@dp.message(Command("reindex"))
async def cmd_reindex(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid) or get_user_active_doc(uid)
    if not doc_id:
        await _send(m, "–ê–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ—Ç. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª —Å–Ω–∞—á–∞–ª–∞.")
        return

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()

    if not row:
        await _send(m, "–ù–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–Ω–æ–≤–æ.")
        return

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
        # –æ–±–æ–≥–∞—â–∞–µ–º —Å–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–æ–º
        sections = enrich_sections(sections, doc_kind=os.path.splitext(path)[1].lower().strip("."))
        delete_document_chunks(doc_id, uid)
        index_document(uid, doc_id, sections)
        invalidate_cache(uid, doc_id)
        set_document_indexer_version(doc_id, CURRENT_INDEXER_VERSION)
        update_document_meta(doc_id, layout_profile=_current_embedding_profile())
        await _send(m, f"–î–æ–∫—É–º–µ–Ω—Ç #{doc_id} –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω.")
    except Exception as e:
        logging.exception("reindex failed: %s", e)
        await _send(m, f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}")



# ---------- –†–∏—Å—É–Ω–∫–∏: –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–ª–æ–∫–∞–ª—å–Ω—ã–µ, –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç retrieval.py) ----------

_FIG_TITLE_RE = re.compile(
    r"(?i)\b(—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|—Å—Ö–µ–º(?:–∞|—ã)?|–∫–∞—Ä—Ç–∏–Ω(?:–∫–∞|–∫–∏)?|figure|fig\.?|picture|pic\.?)"
    r"\s*(?:‚Ññ\s*)?(\d+(?:[.,]\d+)*)\b(?:\s*[‚Äî\-‚Äì:\u2013\u2014]\s*(.+))?"
)

# –í–∫–ª—é—á–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å –∫–∞—Ä—Ç–∏–Ω–æ–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
FIG_VALUES_DEFAULT: bool = getattr(Cfg, "FIG_VALUES_DEFAULT", True)

def _compose_figure_display(attrs_json: str | None, section_path: str, title_text: str | None) -> str:
    """–î–µ–ª–∞–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∏—Å—É–Ω–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º."""
    num = None
    tail = None
    if attrs_json:
        try:
            a = json.loads(attrs_json or "{}")
            num  = str(a.get("caption_num") or a.get("label") or "").strip()
            tail = str(a.get("caption_tail") or a.get("title") or "").strip()
        except Exception:
            pass

    if not num or not num.strip():
        cand = title_text or section_path or ""
        m = _FIG_TITLE_RE.search(cand)
        if m:
            num = (m.group(2) or "").replace(",", ".").strip()
            if not tail:
                tail = (m.group(3) or "").strip()

    if num:
        return f"–†–∏—Å—É–Ω–æ–∫ {num}" + (f" ‚Äî {_shorten(tail, 160)}" if tail else "")
    base = title_text or _last_segment(section_path or "")
    base = re.sub(
    r"(?i)^\s*(—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|—Å—Ö–µ–º(?:–∞|—ã)?|–∫–∞—Ä—Ç–∏–Ω(?:–∫–∞|–∫–∏)?|figure|fig\.?|picture|pic\.?)\s*",
        "", base
    ).strip(" ‚Äî‚Äì-")
    return _shorten(base or "–†–∏—Å—É–Ω–æ–∫", 160)

# ---------- NEW: —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ DOCX-–≥—Ä–∞—Ñ–∏–∫–æ–≤ (chart_data) ----------

def _fetch_figure_row_by_num(uid: int, doc_id: int, num: str):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É chunks –¥–ª—è —Ä–∏—Å—É–Ω–∫–∞ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –Ω–æ–º–µ—Ä–æ–º (–µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–∞),
    –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ —Ç—É, –≥–¥–µ –≤ attrs –ª–µ–∂–∏—Ç caption_num/label.
    """
    con = get_conn()
    cur = con.cursor()
    like1 = f'%\"caption_num\": \"{num}\"%'
    like2 = f'%\"label\": \"{num}\"%'
    row = None

    # 1) –ø–æ –Ω–æ–º–µ—Ä—É –≤ attrs
    try:
        cur.execute(
            """
            SELECT page, section_path, attrs, text
            FROM chunks
            WHERE owner_id=? AND doc_id=? AND element_type='figure'
              AND (attrs LIKE ? OR attrs LIKE ?)
            ORDER BY id ASC LIMIT 1
            """,
            (uid, doc_id, like1, like2),
        )
        row = cur.fetchone()
    except Exception:
        row = None

    # 2) —Ñ–æ–ª–±—ç–∫ ‚Äî –ø–æ section_path
    if not row:
        try:
            cur.execute(
                """
                SELECT page, section_path, attrs, text
                FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='figure'
                  AND section_path LIKE ? COLLATE NOCASE
                ORDER BY id ASC LIMIT 1
                """,
                (uid, doc_id, f'%–†–∏—Å—É–Ω–æ–∫ {num}%'),
            )
            row = cur.fetchone()
        except Exception:
            row = None

    con.close()
    return row


def _parse_chart_data(attrs_json: str | None) -> tuple[list | None, str | None, dict]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å—Ö–µ–º attrs.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (data_rows, chart_type, attrs_dict), –≥–¥–µ data_rows ‚Äî —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    –≤–∏–¥–∞ {"label": ..., "value": ..., "unit": ...}.
    """
    try:
        a = json.loads(attrs_json or "{}")

        # —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        raw = (a.get("chart_data")
            or (a.get("chart") or {}).get("data")
            or a.get("data")
            or a.get("series"))
        ctype = (a.get("chart_type")
                or (a.get("chart") or {}).get("type")
                or a.get("type"))


        # –£–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ [{label, value, unit?}]
        if isinstance(raw, list) and raw:
            return raw, ctype, a

        # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞: {"categories":[...], "series":[{"name":..., "values":[...], "unit":"%"}]}
        if isinstance(raw, dict) and raw.get("categories") and raw.get("series"):
            cats = list(raw.get("categories") or [])
            s0   = (raw.get("series") or [{}])[0] or {}
            vals = list(s0.get("values") or s0.get("data") or [])
            unit = s0.get("unit")
            rows = []
            for i in range(min(len(cats), len(vals))):
                rows.append({
                    "label": str(cats[i]),
                    "value": vals[i],
                    "unit": unit
                })
            if rows:
                return rows, (ctype or s0.get("type") or "chart"), a
    except Exception:
        pass
    return None, None, {}



def _format_chart_values(chart_data: list) -> str:
    rows = chart_data or []

    # –°–æ–±–µ—Ä—ë–º —á–∏—Å–ª–∞ –∏ –º–µ—Ç–∫–∏
    labels, nums, units = [], [], []
    all_numeric = True
    for r in rows:
        labels.append((str(r.get("label") or r.get("name") or r.get("category") or "")).strip())
        val = r.get("value")
        if val is None:
            val = r.get("y") or r.get("x") or r.get("v") or r.get("count")
        units.append(r.get("unit") or "")
        try:
            nums.append(float(str(val).replace(",", ".")))
        except Exception:
            all_numeric = False
            break

    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ "—ç—Ç–æ –ø—Ä–æ—Ü–µ–Ω—Ç—ã":
    #  - –µ–¥–∏–Ω–∏—Ü—ã —Å–æ–¥–µ—Ä–∂–∞—Ç '%' –ò–õ–ò
    #  - –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ [0..1.2] –∏ —Å—É–º–º–∞ ‚âà 1 (–¥–æ–ª–∏) –ò–õ–ò
    #  - –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ [0..100] –∏ —Å—É–º–º–∞ ‚âà 100 (–ø–æ—á—Ç–∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã)
    if all_numeric and rows:
        total = sum(nums)
        unit_has_percent = any(isinstance(u, str) and "%" in u for u in units)
        looks_fraction = all(0 <= v <= 1.2 for v in nums) and 0.98 <= total <= 1.02
        looks_percent  = all(0 <= v <= 100 for v in nums) and 99 <= total <= 101

        if unit_has_percent or looks_fraction or looks_percent:
            base = [v * 100 for v in nums] if looks_fraction else nums[:]
            # –û–∫—Ä—É–≥–ª—è–µ–º —Ç–∞–∫, —á—Ç–æ–±—ã —Å—É–º–º–∞ –±—ã–ª–∞ —Ä–æ–≤–Ω–æ 100 (–º–µ—Ç–æ–¥ –Ω–∞–∏–±–æ–ª—å—à–∏—Ö –æ—Å—Ç–∞—Ç–∫–æ–≤)
            floors = [int(math.floor(x)) for x in base]
            need = int(round(100 - sum(floors)))
            remainders = [x - f for x, f in zip(base, floors)]
            order = sorted(range(len(base)), key=lambda i: remainders[i], reverse=True)
            for i in order[:max(0, abs(need))]:
                floors[i] += 1 if need > 0 else -1
            return "\n".join([f"‚Äî {labels[i]}: {floors[i]}%" for i in range(len(floors))])

    # –§–æ–ª–±—ç–∫: –∫–∞–∫ –±—ã–ª–æ
    lines = []
    for i, r in enumerate(rows):
        label = labels[i] if i < len(labels) else (str(r.get("label") or r.get("name") or r.get("category") or "")).strip()
        val = r.get("value")
        if val is None:
            val = r.get("y") or r.get("x") or r.get("v") or r.get("count")
        unit = r.get("unit")
        unit_s = f" {unit}" if isinstance(unit, str) and unit.strip() else ""
        if label or val is not None:
            lines.append(f"‚Äî {label}: {val}{unit_s}".strip())
    return "\n".join(lines) if lines else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–≤–æ–¥–∞."

# --- –Ω–µ–±–æ–ª—å—à–∞—è –∫–æ—Å–º–µ—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∏–∑ OOXML-–≥—Ä–∞—Ñ–∏–∫–æ–≤ ---

# –¥–≤–æ–µ—Ç–æ—á–∏–µ + –ø—Ä–æ–±–µ–ª—ã, —Å—Ä–∞–∑—É –ø–µ—Ä–µ–¥ ';' –∏–ª–∏ –∫–æ–Ω—Ü–æ–º —Å—Ç—Ä–æ–∫–∏
_EMPTY_PERCENT_RE = re.compile(r"(:\s*)(?=;|$)")

def _fill_empty_percents(text: str) -> str:
    """
    'label:' –∏–ª–∏ 'label: ;' ‚Üí 'label: 0%' –ø–µ—Ä–µ–¥ ';' –∏–ª–∏ –∫–æ–Ω—Ü–æ–º —Å—Ç—Ä–æ–∫–∏.
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è –∫—É—Å–æ—á–∫–æ–≤ –≤–∏–¥–∞ '‚Ä¶; 3:' –∏ '‚Ä¶; 3: ;'.
    """
    return _EMPTY_PERCENT_RE.sub(lambda m: m.group(1) + "0%", text)


def _list_figures_db(uid: int, doc_id: int, limit: int = 25) -> dict:
    """–°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ –ë–î (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏)."""
    con = get_conn()
    cur = con.cursor()
    has_type = _table_has_columns(con, "chunks", ["element_type", "attrs"])

    if has_type:
        cur.execute(
            "SELECT DISTINCT section_path, attrs, text FROM chunks "
            "WHERE owner_id=? AND doc_id=? AND element_type='figure' "
            "ORDER BY id ASC",
            (uid, doc_id),
        )
    else:
        # —Å—Ç–∞—Ä—ã–µ –∏–Ω–¥–µ–∫—Å—ã ‚Äî –∫–æ–ª–æ–Ω–∫–∏ attrs –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å, –Ω–µ –≤—ã–±–∏—Ä–∞–µ–º –µ—ë
        cur.execute(
            "SELECT DISTINCT section_path, text FROM chunks "
            "WHERE owner_id=? AND doc_id=? AND (text LIKE '[–†–∏—Å—É–Ω–æ–∫]%' OR lower(section_path) LIKE '%—Ä–∏—Å—É–Ω–æ–∫%') "
            "ORDER BY id ASC",
            (uid, doc_id),
        )
    rows = cur.fetchall() or []
    con.close()

    items: list[str] = []
    for r in rows:
        section_path = r["section_path"] or ""
        attrs_json = r["attrs"] if ("attrs" in r.keys()) else None  # –≤ else –µ—ë –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç ‚Äî –æ–∫
        txt = r["text"] or None
        disp = _compose_figure_display(attrs_json, section_path, txt)
        items.append(disp)

    seen = set()
    uniq = []
    for it in items:
        k = it.strip().lower()
        if k and k not in seen:
            seen.add(k)
            uniq.append(it)

    total = len(uniq)
    return {
        "count": total,
        "list": uniq[:limit],
        "more": max(0, total - limit),
    }



# -------- –†–∞–Ω–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤–∏–¥–∞ ¬´—Ä–∏—Å—É–Ω–æ–∫ 2.1¬ª, ¬´—Ä–∏—Å. 3¬ª, ¬´figure 1.2¬ª --------

FIG_NUM_RE = re.compile(
    r"(?i)\b(?:—Ä–∏—Å\w*|—Å—Ö–µ–º\w*|–∫–∞—Ä—Ç–∏–Ω\w*|–¥–∏–∞–≥—Ä–∞–º\w*|–≥–∏—Å—Ç–æ–≥—Ä–∞–º\w*|diagram|chart|figure|fig\.?|picture|pic\.?)"
    r"\s*(?:‚Ññ\s*|no\.?\s*|–Ω–æ–º–µ—Ä\s*)?([A-Za-z–ê-–Ø–∞-—è]?\s*[\d.,\s]+(?:\s*(?:–∏|and)\s*[\d.,\s]+)*)"
)

# –Ω–æ–≤—ã–π —Ö–∏–Ω—Ç –¥–ª—è —Ä–µ–∂–∏–º–∞ ¬´–∏–∑–≤–ª–µ—á—å –∑–Ω–∞—á–µ–Ω–∏—è¬ª
_VALUES_HINT = re.compile(r"(?i)\b(–∑–Ω–∞—á–µ–Ω–∏[—è–µ]|—Ü–∏—Ñ—Ä[–∞—ã]|–ø—Ä–æ—Ü–µ–Ω—Ç[–∞-—è]*|values?|numbers?)\b")
_SPLIT_FIG_LIST_RE = re.compile(r"\s*(?:,|;|\band\b|–∏)\s*", re.IGNORECASE)

def _extract_fig_nums(text: str) -> list[str]:
    nums: list[str] = []
    for mm in FIG_NUM_RE.finditer(text or ""):
        seg = (mm.group(1) or "").strip()
        # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: –∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, "–∏/and"
        parts = _SPLIT_FIG_LIST_RE.split(seg)
        for p in parts:
            p = p.strip()
            if not p:
                continue
            nums.append(p)
    return nums

_ALL_FIGS_HINT = re.compile(r"(?i)\b(–≤—Å–µ\s+—Ä–∏—Å—É–Ω–∫\w*|–≤—Å–µ\s+—Å—Ö–µ–º\w*|–≤—Å–µ\s+–∫–∞—Ä—Ç–∏–Ω\w*|all\s+pictures?|all\s+figs?)\b")

def _num_norm_fig(s: str | None) -> str:
    s = (s or "").strip()
    s = s.replace("\u00A0", " ")   # NBSP -> –ø—Ä–æ–±–µ–ª
    s = s.replace(" ", "")
    s = s.replace(",", ".")        # 4,1 -> 4.1
    s = re.sub(r"[.:;)\]]+$", "", s)  # —Å—Ä–µ–∑ —Ö–≤–æ—Å—Ç–æ–≤–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏: "4." -> "4"
    return s


def _is_pure_figure_request(text: str) -> bool:
    """
    –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∑–∞–ø—Ä–æ—Å –¢–û–õ–¨–ö–û –ø—Ä–æ —Ä–∏—Å—É–Ω–∫–∏ (–æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä–æ–≤),
    –±–µ–∑ —Ç–∞–±–ª–∏—Ü, —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.

    –ò—Å–ø–æ–ª—å–∑—É–µ–º, —á—Ç–æ–±—ã:
    ‚Äî —É–π—Ç–∏ –≤ –µ–¥–∏–Ω—ã–π figure-–ø–∞–π–ø–ª–∞–π–Ω;
    ‚Äî –Ω–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–æ—Ç–æ–º –æ–±—â–∏–π RAG-–ø–∞–π–ø–ª–∞–π–Ω, –∫–æ—Ç–æ—Ä—ã–π –¥—É–±–ª–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã
      –∏ –º–æ–∂–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å ¬´–¥–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ¬ª.
    """
    t = (text or "").strip()
    if not t:
        return False

    # ¬´–≤—Å–µ —Ä–∏—Å—É–Ω–∫–∏¬ª ‚Äî –æ—Ç–¥–µ–ª—å–Ω–∞—è –≤–µ—Ç–∫–∞ (_ALL_FIGS_HINT)
    if _ALL_FIGS_HINT.search(t):
        return False

    # –Ω–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤ ‚Äî –Ω–µ –Ω–∞—à —Å–ª—É—á–∞–π
    if not FIG_NUM_RE.search(t):
        return False

    # –µ—Å–ª–∏ —è–≤–Ω–æ —É–ø–æ–º–∏–Ω–∞—é—Ç —Ç–∞–±–ª–∏—Ü—ã –∏–ª–∏ —Ä–∞–∑–¥–µ–ª—ã/–≥–ª–∞–≤—ã ‚Äî —ç—Ç–æ —É–∂–µ —Å–º–µ—à–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    if _TABLE_ANY.search(t) or _SECTION_NUM_RE.search(t):
        return False

    return True


def _build_figure_records(uid: int, doc_id: int, nums: list[str]) -> list[dict]:
    """
    –ï–¥–∏–Ω–∞—è "—Å–±–æ—Ä–∫–∞" –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∏—Å—É–Ω–∫–∞—Ö:
    ‚Äî –Ω–æ–º–µ—Ä –∏ –∫—Ä–∞—Å–∏–≤—ã–π display;
    ‚Äî –ø—É—Ç–∏ –∫ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º;
    ‚Äî —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∏–∑ chart_data);
    ‚Äî –ø–æ–¥–ø–∏—Å—å –∏ —Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º;
    ‚Äî vision-–æ–ø–∏—Å–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å –∏ –Ω–µ ¬´–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ¬ª).
    """
    if not nums:
        return []

    # –∑–∞—Ä–∞–Ω–µ–µ —Ç—è–Ω–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –∏–∑ retrieval, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å images/—Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º/vision
    try:
        cards = describe_figures_by_numbers(
            uid,
            doc_id,
            nums,
            sample_chunks=2,
            use_vision=True,
            lang="ru",
            vision_first_image_only=True,
        ) or []
    except Exception:
        cards = []

    cards_by_norm: dict[str, dict] = {}
    for c in cards:
        key = _num_norm_fig(str(c.get("num") or ""))
        if key and key not in cards_by_norm:
            cards_by_norm[key] = c

    idx_oox = _ooxml_get_index(doc_id)
    fig_idx = FIG_INDEX.get(doc_id)

    # –∫–ª—é—á: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –Ω–æ–º–µ—Ä —Ä–∏—Å—É–Ω–∫–∞ ‚Üí record
    records_by_num: dict[str, dict] = {}

    for orig in nums:
        norm = _num_norm_fig(orig)
        if not norm:
            continue

        # –µ—Å–ª–∏ —ç—Ç–æ—Ç –Ω–æ–º–µ—Ä —É–∂–µ —Å–æ–±—Ä–∞–Ω ‚Äî –Ω–µ —Å–æ–∑–¥–∞—ë–º –¥—É–±–ª—å
        if norm in records_by_num:
            continue

        card = cards_by_norm.get(norm)
        rec: dict = {
            "owner_id": uid,      # –Ω—É–∂–µ–Ω –¥–ª—è –≤—ã–∑–æ–≤–∞ summarizer.extract_figure_value
            "doc_id": doc_id,     # –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
            "num": norm,
            "orig": orig,
            "display": None,
            "images": [],
            "values": None,
            "near_text": [],
            "caption": None,
            "vision_desc": None,
        }

        # --- 1) –¥–∞–Ω–Ω—ã–µ –∏–∑ RAG-–∫–∞—Ä—Ç–æ—á–µ–∫ ---
        if card:
            rec["display"] = card.get("display") or rec["display"]
            rec["images"] = [p for p in (card.get("images") or []) if p]
            rec["near_text"] = [
                (h or "").strip()
                for h in (card.get("highlights") or [])
                if (h or "").strip()
            ]
            vis = (card.get("vision") or {}).get("description") or ""
            vis_clean = vis.strip()
            low = vis_clean.lower()
            # –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –≤–∏–¥–∞ ¬´—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ)¬ª
            if vis_clean and "–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ" not in low and "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è" not in low:
                rec["vision_desc"] = vis_clean

        # --- 2) OOXML-–∏–Ω–¥–µ–∫—Å: –ø–æ–¥–ø–∏—Å—å –∏ image_path ---
        if idx_oox:
            oox_rec = _ooxml_find_figure_by_label(idx_oox, norm) or _ooxml_find_figure_by_label(idx_oox, orig)
            if oox_rec:
                cap = (oox_rec.get("caption") or "").strip()
                if cap:
                    rec["caption"] = cap
                if not rec["display"]:
                    label = oox_rec.get("n") or norm
                    rec["display"] = f"–†–∏—Å—É–Ω–æ–∫ {label}" + (f" ‚Äî {cap}" if cap else "")
                path = oox_rec.get("image_path")
                if path and path not in rec["images"]:
                    rec["images"].append(path)

        # --- 3) –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å figures.py: –ø—É—Ç—å –∫ –∫–∞—Ä—Ç–∏–Ω–∫–µ + –ø–æ–¥–ø–∏—Å—å ---
        if fig_idx:
            try:
                recs = fig_find(fig_idx, number=orig) or fig_find(fig_idx, number=norm) or []
            except Exception:
                recs = []
            for r in recs:
                if not rec["display"]:
                    rec["display"] = figure_display_name(r)
                ap = r.get("abs_path")
                if ap and ap not in rec["images"]:
                    rec["images"].append(ap)
                cap_text = r.get("caption") or r.get("title")
                if cap_text and not rec["caption"]:
                    rec["caption"] = cap_text
                if not rec["near_text"] and cap_text:
                    rec["near_text"].append(cap_text)

        # --- 4) chart_data –∏–∑ attrs (—Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è) ---
        row = _fetch_figure_row_by_num(uid, doc_id, orig)
        if not row and norm != orig:
            row = _fetch_figure_row_by_num(uid, doc_id, norm)
        if row:
            attrs_json = row["attrs"] if ("attrs" in row.keys()) else None
            cd, _ctype, _attrs = _parse_chart_data(attrs_json)
            if cd:
                rec["values"] = _fill_empty_percents(_format_chart_values(cd))
            if not rec["display"]:
                title_text = row["text"] if ("text" in row.keys()) else None
                rec["display"] = _compose_figure_display(
                    attrs_json,
                    row["section_path"],
                    title_text,
                )

        if not rec["display"]:
            rec["display"] = f"–†–∏—Å—É–Ω–æ–∫ {norm}"

        records_by_num[norm] = rec

    return list(records_by_num.values())


def _fig_values_text_from_records(
    records: list[dict],
    *,
    need_values: bool,
) -> str:
    """
    –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫ —Å —Ç–æ—á–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —Ä–∏—Å—É–Ω–∫–∞–º
    (–±–µ–∑ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–µ—Ñ–∏–∫—Å –∫
    –æ—Å–Ω–æ–≤–Ω–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é.
    """
    lines: list[str] = []

    for rec in records:
        # 1) chart_data —É–∂–µ –º–æ–≥ –±—ã—Ç—å –ø—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ _build_figure_records
        values = (rec.get("values") or "").strip()

        # 2) –§–û–õ–ë–≠–ö ‚Ññ1: –µ—Å–ª–∏ chart_data –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ summarizer.extract_figure_value
        if not values and summ_extract_figure_value is not None:
            owner_id = rec.get("owner_id")
            doc_id = rec.get("doc_id")
            num = rec.get("orig") or rec.get("num")

            if owner_id and doc_id and num:
                try:
                    raw = summ_extract_figure_value(owner_id, doc_id, str(num)) or ""
                    raw = raw.strip()
                except Exception:
                    raw = ""

                # –æ–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–∑ summarizer.extract_figure_value:
                # "**–†–∏—Å—É–Ω–æ–∫ N.** –ó–∞–≥–æ–ª–æ–≤–æ–∫\n<—Ç–∞–±–ª–∏—á–∫–∞/CSV/...>"
                if raw.startswith("**–†–∏—Å—É–Ω–æ–∫"):
                    raw_lines = raw.splitlines()
                    body = "\n".join(raw_lines[1:]).strip() if len(raw_lines) >= 2 else ""
                    if body:
                        values = body
                        rec["values"] = body  # —á—Ç–æ–±—ã GPT-–æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–∂–µ –≤–∏–¥–µ–ª–æ —ç—Ç–∏ —á–∏—Å–ª–∞

        # 3) –§–û–õ–ë–≠–ö ‚Ññ2: –µ—Å–ª–∏ –Ω–µ—Ç –Ω–∏ chart_data, –Ω–∏ summarizer ‚Äî –ø—Ä–æ–±—É–µ–º OOXML-–∏–Ω–¥–µ–∫—Å
        if not values:
            try:
                doc_id = rec.get("doc_id")
                num = rec.get("orig") or rec.get("num")
                idx = _ooxml_get_index(doc_id) if doc_id else None
                body = ""

                if idx and "oox_fig_lookup" in globals() and num:
                    oox_res = oox_fig_lookup(idx, str(num))

                    # –ü–æ–¥—Å—Ç—Ä–æ–π –ø–æ–¥ —Å–≤–æ—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é figure_lookup:
                    # –∑–¥–µ—Å—å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤–µ—Ä–Ω—ë—Ç—Å—è –ª–∏–±–æ —Å—Ç—Ä–æ–∫–∞, –ª–∏–±–æ dict.
                    if isinstance(oox_res, str):
                        body = oox_res.strip()
                    elif isinstance(oox_res, dict):
                        body = (
                            (oox_res.get("values_text")
                             or oox_res.get("text")
                             or "")
                        ).strip()

                if body:
                    values = body
                    rec["values"] = body
            except Exception:
                # –Ω–µ –ª–æ–º–∞–µ–º –≤–µ—Å—å –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ —Å OOXML —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
                pass

        # 4) –µ—Å–ª–∏ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫ —á–∏—Å–µ–ª –Ω–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —Ä–∏—Å—É–Ω–æ–∫
        if not values:
            continue

        disp = rec.get("display") or f"–†–∏—Å—É–Ω–æ–∫ {rec.get('num') or ''}".strip()
        lines.append(f"**{disp} ‚Äî —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**\n\n{values}")

    if lines:
        return "\n\n".join(lines)

    if need_values:
        return (
            "–ü–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º —Ä–∏—Å—É–Ω–∫–∞–º –Ω–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ—á—å —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ "
            "(–Ω–∏ –∏–∑ chart_data, –Ω–∏ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è/OOXML). "
            "–ú–æ–≥—É –¥–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ."
        )

    return ""


async def _send_fig_values_from_records(
    m: types.Message,
    records: list[dict],
    *,
    need_values: bool,
) -> None:
    """
    –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞.
    –í –æ—Å–Ω–æ–≤–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º _fig_values_text_from_records
    –∏ —Å–∫–ª–µ–∏–≤–∞–µ–º —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º.
    """
    text = _fig_values_text_from_records(records, need_values=need_values)
    if text:
        await _send(m, text)


async def _explain_figures_with_gpt(
    m: types.Message,
    records: list[dict],
    question: str,
    *,
    verbosity: str,
    need_values: bool,
    values_prefix: str = "",
) -> None:
    """
    –§–∏–Ω–∞–ª—å–Ω—ã–π —à–∞–≥: GPT –¥–∞—ë—Ç —Å–≤—è–∑–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Ä–∏—Å—É–Ω–∫–∞–º —Å—Ä–∞–∑—É,
    –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–¥–ø–∏—Å–∏, —Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º –∏ —É–∂–µ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.

    –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω values_prefix, —Ç–æ –æ–Ω –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞:
    —Å–Ω–∞—á–∞–ª–∞ –±–ª–æ–∫ ¬´—Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª, –∑–∞—Ç–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è.
    """
    if not (chat_with_gpt or chat_with_gpt_stream):
        return

    if not records:
        return

    ctx_blocks: list[str] = []
    for rec in records:
        disp = rec.get("display") or f"–†–∏—Å—É–Ω–æ–∫ {rec.get('num') or ''}".strip()
        parts: list[str] = [disp]
        if rec.get("caption"):
            parts.append(f"–ü–æ–¥–ø–∏—Å—å: {rec['caption']}")
        if rec.get("near_text"):
            parts.append("–¢–µ–∫—Å—Ç —Ä—è–¥–æ–º: " + " ".join(rec["near_text"][:2]))
        if rec.get("vision_desc"):
            parts.append("–û–ø–∏—Å–∞–Ω–∏–µ –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–µ: " + rec["vision_desc"])
        if rec.get("values"):
            parts.append("–¢–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ):\n" + rec["values"])
        ctx_blocks.append("\n".join(parts))

    ctx = "\n\n---\n\n".join(ctx_blocks)
    if not ctx.strip():
        return

    focus = (
        "—Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –∏—Ö –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é"
        if need_values
        else "–ø–æ–¥—Ä–æ–±–Ω–æ –ø–æ—è—Å–Ω—è—è —Å–º—ã—Å–ª –∏ –≤—ã–≤–æ–¥—ã –ø–æ —Ä–∏—Å—É–Ω–∫–∞–º"
    )

    system_prompt = (
        "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –£ —Ç–µ–±—è –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∏—Å—É–Ω–∫–∞—Ö –¥–∏–ø–ª–æ–º–∞ "
        "(–ø–æ–¥–ø–∏—Å–∏, —Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º, —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º). "
        "–û–±—ä—è—Å–Ω–∏ —Å—Ç—É–¥–µ–Ω—Ç—É, —á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–∏—Å—É–Ω–∫–∏, –∫–∞–∫–∏–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –≤–∏–¥–Ω—ã –∏ –∫–∞–∫–∏–µ –≤—ã–≤–æ–¥—ã –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å. "
        "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ —á–∏—Å–ª–∞ –∏ –Ω–µ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü."
    )
    user_prompt = (
        f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}\n\n"
        f"–°–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ä–∏—Å—É–Ω–∫–∞—Ö, –æ–ø–∏—à–∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏ —Å–¥–µ–ª–∞–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é {focus}.\n"
        f"{_verbosity_addendum(verbosity, '–æ–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤')}"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": f"[–°–æ–±—Ä–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∏—Å—É–Ω–∫–∞—Ö]\n{ctx}"},
        {"role": "user", "content": user_prompt},
    ]

    try:
        ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
    except Exception as e:
        logging.exception("figure explanation failed: %s", e)
        ans = ""

    ans = (ans or "").strip()
    prefix = (values_prefix or "").strip()

    if prefix and ans:
        final = prefix + "\n\n" + ans
    elif prefix:
        final = prefix
    else:
        final = ans

    if final:
        await _send(m, _strip_unwanted_sections(final))

async def _answer_figure_query(
    m: types.Message, uid: int, doc_id: int, text: str, *, verbosity: str = "normal"
) -> bool:
    """
    –ù–æ–≤—ã–π –µ–¥–∏–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π:
    1) –≤—Å–µ–≥–¥–∞ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º –ø–æ —Ä–∏—Å—É–Ω–∫–∞–º (_build_figure_records);
    2) –≤—Å–µ–≥–¥–∞ —Å—Ç–∞—Ä–∞–µ–º—Å—è –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–∞–º–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏;
    3) —Å–æ–±–∏—Ä–∞–µ–º –æ–±—â–∏–π –±–ª–æ–∫ —Å —Ç–æ—á–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å);
    4) –¥–∞—ë–º –æ–¥–Ω–æ —Å–≤—è–∑–Ω–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ GPT, –≤ –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–¥–º–µ—à–∞–Ω –±–ª–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π.

    –ü–æ–≤–µ–¥–µ–Ω–∏–µ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ–≥–æ, —Å–ø—Ä–æ—Å–∏–ª–∏ –ª–∏ ¬´–æ–ø–∏—à–∏ —Ä–∏—Å—É–Ω–æ–∫ 2.3¬ª
    –∏–ª–∏ ¬´–¥–∞–π —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —Ä–∏—Å—É–Ω–∫—É 2.3¬ª ‚Äî –º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∞–∫—Ü–µ–Ω—Ç
    –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–∏.
    """
    # 0) –Ω—É–∂–µ–Ω –ª–∏ –æ—Å–æ–±—ã–π –∞–∫—Ü–µ–Ω—Ç –Ω–∞ —á–∏—Å–ª–∞—Ö
    need_values = bool(_VALUES_HINT.search(text or ""))

    # 1) –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –Ω–æ–º–µ—Ä–∞ —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
    raw_list = _extract_fig_nums(text or "")
    seen: set[str] = set()
    nums: list[str] = []
    for token in raw_list:
        n = _num_norm_fig(token)
        if n and n not in seen:
            seen.add(n)
            nums.append(token)   # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –≤–∏–¥ –Ω–æ–º–µ—Ä–∞

    if not nums:
        return False

    # 2) —Å–æ–±–∏—Ä–∞–µ–º –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ –≤—Å–µ–º —Ä–∏—Å—É–Ω–∫–∞–º
    records = _build_figure_records(uid, doc_id, nums)
    if not records:
        await _send(m, "–£–∫–∞–∑–∞–Ω–Ω—ã–µ —Ä–∏—Å—É–Ω–∫–∏ –≤ —Ä–∞–±–æ—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return True

    # 3) —Å–Ω–∞—á–∞–ª–∞ —Å–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–æ –≤—Å–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–º)
    try:
        cards_for_media = [
            {
                "num": r["num"],
                "display": r.get("display") or f"–†–∏—Å—É–Ω–æ–∫ {r['num']}",
                "images": r.get("images") or [],
            }
            for r in records
        ]
        await _send_media_from_cards(m, cards_for_media)
    except Exception:
        # –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –Ω–µ —Å—Ä—ã–≤–∞–µ–º –¥–∞–ª—å–Ω–µ–π—à–∏–µ —à–∞–≥–∏
        pass

    # 4) —Å–æ–±–∏—Ä–∞–µ–º –æ–±—â–∏–π –±–ª–æ–∫ —Å —Ç–æ—á–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–±–µ–∑ –æ—Ç–ø—Ä–∞–≤–∫–∏)
    values_block = _fig_values_text_from_records(records, need_values=need_values)

    # 5) –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç: ¬´–∑–Ω–∞—á–µ–Ω–∏—è + –æ–ø–∏—Å–∞–Ω–∏–µ¬ª
    await _explain_figures_with_gpt(
        m,
        records,
        text,
        verbosity=verbosity,
        need_values=need_values,
        values_prefix=values_block,
    )

    # 6) –æ–±–Ω–æ–≤–ª—è–µ–º ¬´–ø–æ—Å–ª–µ–¥–Ω–∏–π —É–ø–æ–º—è–Ω—É—Ç—ã–π —Ä–∏—Å—É–Ω–æ–∫¬ª –¥–ª—è –∞–Ω–∞—Ñ–æ—Ä–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    try:
        LAST_REF.setdefault(uid, {})["figure_nums"] = [r["num"] for r in records]
    except Exception:
        pass

    return True


# -------------------------- –°–ê–ú–û–í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ò–ù–î–ï–ö–°–ê --------------------------

def _count_et(con, uid: int, doc_id: int, et: str) -> int:
    cur = con.cursor()
    if _table_has_columns(con, "chunks", ["element_type"]):
        cur.execute(
            "SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=? AND element_type=?",
            (uid, doc_id, et),
        )
        row = cur.fetchone()
        return int(row["c"] or 0)
    return 0

def _need_self_heal(uid: int, doc_id: int, need_refs: bool, need_figs: bool) -> tuple[bool, int, int]:
    """
    –°–∞–º–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–ø–µ—Ä—å –∑–∞–≤—è–∑–∞–Ω–æ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ú–ï–î–ò–ê–î–ê–ù–ù–´–•:
      ‚Äî –µ—Å—Ç—å –ª–∏ –≥–¥–µ-—Ç–æ –≤ attrs —Å–ø–∏—Å–æ–∫ images;
      ‚Äî –µ—Å—Ç—å –ª–∏ chart_data (–¥–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º), –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç element_type.
    –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –Ω–∞–π–¥–µ–Ω–æ ‚Äî —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ ¬´—Ñ–∏–≥—É—Ä—ã –µ—Å—Ç—å¬ª (fc=1).
    –§–æ–ª–±—ç–∫ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –ë–î –±–µ–∑ attrs: —Å—á–∏—Ç–∞–µ–º element_type='figure'.
    """
    con = get_conn()
    rc = _count_et(con, uid, doc_id, "reference") if need_refs else 1

    # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ¬´—Ñ–∏–≥—É—Ä—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç¬ª, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã
    fc = 1
    if need_figs:
        media_found = False
        try:
            cur = con.cursor()
            # –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ attrs ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ø—Ä—è–º—É—é –º–µ–¥–∏–∞–¥–∞–Ω–Ω—ã–µ
            if _table_has_columns(con, "chunks", ["attrs"]):
                cur.execute(
                    "SELECT attrs FROM chunks WHERE owner_id=? AND doc_id=? AND attrs IS NOT NULL",
                    (uid, doc_id),
                )
                rows = cur.fetchall() or []
                for r in rows:
                    attrs_json = r["attrs"] or None
                    if not attrs_json:
                        continue
                    # –±—ã—Å—Ç—Ä—ã–π —á–µ–∫ –Ω–∞ images
                    try:
                        a = json.loads(attrs_json)
                        imgs = a.get("images") or []
                        if isinstance(imgs, list) and any(imgs):
                            media_found = True
                            break
                    except Exception:
                        pass
                    # –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏–º chart_data (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ä—Å–µ—Ä)
                    try:
                        cd, _, _ = _parse_chart_data(attrs_json)  # returns (rows|None, type|None, attrs_dict)
                        if cd:
                            media_found = True
                            break
                    except Exception:
                        # –ø–∞—Ä—Å–µ—Ä –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–µ—à–µ–Ω–∏—è ‚Äî –ø—Ä–æ—Å—Ç–æ –∏–¥—ë–º –¥–∞–ª—å—à–µ
                        pass
            else:
                # –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å –±–µ–∑ attrs ‚Äî —Ñ–æ–ª–±—ç–∫ –∫ figure-—á–∞–Ω–∫–∞–º
                media_found = (_count_et(con, uid, doc_id, "figure") > 0)
        except Exception:
            # –∑–∞—â–∏—Ç–Ω—ã–π —Ñ–æ–ª–±—ç–∫: —Å—á–∏—Ç–∞–µ–º –ø–æ figure-—á–∞–Ω–∫–∞–º
            media_found = (_count_et(con, uid, doc_id, "figure") > 0)

        fc = 1 if media_found else 0

    con.close()
    return (rc == 0 or fc == 0, rc, fc)


def _reindex_with_sections(uid: int, doc_id: int, sections: list[dict]) -> None:
    delete_document_chunks(doc_id, uid)
    index_document(uid, doc_id, sections)
    invalidate_cache(uid, doc_id)
    set_document_indexer_version(doc_id, CURRENT_INDEXER_VERSION)
    update_document_meta(doc_id, layout_profile=_current_embedding_profile())

async def _ensure_modalities_indexed(m: types.Message, uid: int, doc_id: int, intents: dict):
    """–ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å—Ç–∞—Ä—ã–π –∏ –Ω–µ—Ç reference/figure ‚Äî —Ç–∏—Ö–æ –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–º –Ω–æ–≤—ã–º –ø–∞—Ä—Å–µ—Ä–æ–º –∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º."""
    need_refs = bool(intents.get("sources", {}).get("want"))
    need_figs = bool(intents.get("figures", {}).get("want"))
    if not (need_refs or need_figs):
        return

    should, have_refs, have_figs = _need_self_heal(uid, doc_id, need_refs, need_figs)
    if not should:
        return

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    if not row:
        return

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
        sections = enrich_sections(sections, doc_kind=os.path.splitext(path)[1].lower().strip("."))
    except Exception as e:
        logging.exception("re-parse/enrich failed: %s", e)
        return

    new_refs = sum(1 for s in sections if (s.get("element_type") == "reference"))
    new_figs = sum(1 for s in sections if (s.get("element_type") == "figure"))

    do_reindex = False
    if need_refs and have_refs == 0 and new_refs > 0:
        do_reindex = True
    if need_figs and have_figs == 0 and new_figs > 0:
        do_reindex = True

    if do_reindex:
        try:
            _reindex_with_sections(uid, doc_id, sections)
            await _send(m, "–û–±–Ω–æ–≤–∏–ª –∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞: –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ —Ä–∏—Å—É–Ω–∫–∏/–∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–≤–∫–ª—é—á–∞—è OOXML-–¥–∏–∞–≥—Ä–∞–º–º—ã).")
        except Exception as e:
            logging.exception("self-heal reindex failed: %s", e)


# -------------------------- –°–±–æ—Ä —Ñ–∞–∫—Ç–æ–≤ --------------------------

def _gather_facts(uid: int, doc_id: int, intents: dict) -> dict:
    """
    –°–æ–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –ë–î/–∏–Ω–¥–µ–∫—Å–∞, –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.
    """
    facts: dict[str, object] = {"doc_id": doc_id, "owner_id": uid}
    # —Ñ–ª–∞–≥ ¬´—Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–∞ –∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ¬ª
    exact = bool(intents.get("exact_numbers"))
    # –µ—Å–ª–∏ —è–≤–Ω–æ –ø—Ä–æ—Å—è—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–∞–±–ª–∏—Ü—É(—ã) ‚Äî –≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ–º –≤ —Ä–µ–∂–∏–º–µ –¢–û–ß–ù–´–• —á–∏—Å–µ–ª
    if intents.get("tables", {}).get("describe"):
        exact = True
    facts["exact_numbers"] = exact


    # ----- –¢–∞–±–ª–∏—Ü—ã -----
    if intents["tables"]["want"]:
        total_tables = _count_tables(uid, doc_id)
        basenames = _distinct_table_basenames(uid, doc_id)

        con = get_conn()
        cur = con.cursor()
        items: list[str] = []
        for base in basenames:
            cur.execute(
                """
                SELECT attrs FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                  AND section_path LIKE ? || ' [row %'
                ORDER BY id ASC LIMIT 1
                """,
                (uid, doc_id, base),
            )
            r = cur.fetchone()
            attrs_json = r["attrs"] if r else None

            cur.execute(
                """
                SELECT text FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                  AND section_path LIKE ? || ' [row %'
                ORDER BY id ASC LIMIT 2
                """,
                (uid, doc_id, base),
            )
            rows = cur.fetchall() or []
            first_row_text = None
            for rr in rows:
                cand = (rr["text"] or "").split("\n")[0]
                cand = " ‚Äî ".join([c.strip() for c in cand.split(" | ") if c.strip()])
                if cand:
                    first_row_text = cand
                    break

            title = _compose_display_from_attrs(attrs_json, base, first_row_text)
            title = _strip_table_prefix(title)
            items.append(title)

        con.close()
        t_limit = int(intents.get("tables", {}).get("limit", 10))
        facts["tables"] = {
            "count": total_tables,
            "list": items[:t_limit],
            "more": max(0, len(items) - t_limit),
            "describe": [],
        }

        # –ê–≤—Ç–æ-–æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –æ–±—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã
        desc_cards = []
        if not intents.get("tables", {}).get("describe"):
            # –≤–æ–∑—å–º—ë–º –ø–µ—Ä–≤—ã–µ 3‚Äì5 —Ç–∞–±–ª–∏—Ü –∏–∑ —Å–ø–∏—Å–∫–∞
            bases = _distinct_table_basenames(uid, doc_id)[:min(5, t_limit)]
            con = get_conn()
            cur = con.cursor()
            for base in bases:
                # attrs + –ø–µ—Ä–≤—ã–µ 1‚Äì2 —Å—Ç—Ä–æ–∫–∏
                cur.execute("""
                    SELECT page, section_path, attrs FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                    AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 1
                """, (uid, doc_id, base, base))
                row = cur.fetchone()
                if not row:
                    continue

                cur.execute("""
                    SELECT text FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                    AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 2
                """, (uid, doc_id, row["section_path"], row["section_path"]))
                rows = cur.fetchall() or []
                highlights = []
                for r in rows:
                    first = (r["text"] or "").split("\n")[0]
                    if first:
                        highlights.append(" ‚Äî ".join([c.strip() for c in first.split(" | ") if c.strip()]))

                attrs_json = row["attrs"] if row else None
                display = _compose_display_from_attrs(attrs_json, row["section_path"], highlights[0] if highlights else None)
                display = _strip_table_prefix(display)

                # –ø–æ–ø—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å –Ω–æ–º–µ—Ä –¥–ª—è stats
                num, _ = _parse_table_title(display)
                stats = None
                if num:
                    try:
                        stats = analyze_table_by_num(uid, doc_id, num, max_series=6)
                    except Exception:
                        stats = None

                desc_cards.append({
                    "num": num,
                    "display": display,
                    "where": {"page": row["page"], "section_path": row["section_path"]},
                    "highlights": highlights,
                    "stats": stats,
                })
            con.close()

        # –∑–∞–ø–∏—à–µ–º –¥–∞–∂–µ –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç–æ–π ‚Äî –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–∞ —ç—Ç–æ —É—á—Ç—ë—Ç
        facts["tables"]["describe"] = desc_cards
        # describe –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –Ω–æ–º–µ—Ä–∞–º + —Ç–æ—á–Ω—ã–µ —Ä–∞—Å—á–µ—Ç—ã
        desc_cards = []
        if intents["tables"]["describe"]:
            con = get_conn()
            cur = con.cursor()
            for num in intents["tables"]["describe"]:

                like1 = f'%\"caption_num\": \"{num}\"%'
                like2 = f'%\"label\": \"{num}\"%'
                cur.execute(
                    """
                    SELECT page, section_path, attrs FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                      AND (attrs LIKE ? OR attrs LIKE ?)
                    ORDER BY id ASC LIMIT 1
                    """,
                    (uid, doc_id, like1, like2),
                )
                row = cur.fetchone()

                if not row:
                    cur.execute(
                        """
                        SELECT page, section_path, attrs FROM chunks
                        WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                          AND section_path LIKE ? COLLATE NOCASE
                        ORDER BY id ASC LIMIT 1
                        """,
                        (uid, doc_id, f'%–¢–∞–±–ª–∏—Ü–∞ {num}%'),
                    )
                    row = cur.fetchone()

                if not row:
                    continue

                attrs_json = row["attrs"] if row else None
                # 1‚Äì2 –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ highlights
                cur.execute(
                    """
                    SELECT text FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                      AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 2
                    """,
                    (uid, doc_id, row["section_path"], row["section_path"]),
                )
                rows = cur.fetchall()
                highlights = []
                for r in rows or []:
                    first_line = (r["text"] or "").split("\n")[0]
                    if first_line:
                        highlights.append(" ‚Äî ".join([c.strip() for c in first_line.split(" | ") if c.strip()]))

                base = row["section_path"]
                first_row_text = highlights[0] if highlights else None
                display = _compose_display_from_attrs(attrs_json, base, first_row_text)
                display = _strip_table_prefix(display)

                # –ù–û–í–û–ï: —Ç–æ—á–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ —Ç–∞–±–ª–∏—Ü–µ
                stats = None
                try:
                    stats = analyze_table_by_num(uid, doc_id, num, max_series=6)
                except Exception:
                    stats = None

                desc_cards.append({
                    "num": num,
                    "display": display,
                    "where": {"page": row["page"], "section_path": row["section_path"]},
                    "highlights": highlights,
                    "stats": stats,
                })
            con.close()

            facts["tables"]["describe"] = desc_cards

    # ----- –†–∏—Å—É–Ω–∫–∏ -----
    if intents["figures"]["want"]:
        f_limit = int(intents.get("figures", {}).get("limit", 10))
        lst = _list_figures_db(uid, doc_id, limit=f_limit)
        figs_block = {
            "count": int(lst.get("count") or 0),
            "list": list(lst.get("list") or []),
            "more": int(lst.get("more") or 0),
            "describe_lines": [],
        }

        if intents["figures"]["describe"]:
            try:
                cards = describe_figures_by_numbers(
                    uid, doc_id, intents["figures"]["describe"],
                    sample_chunks=2, use_vision=True, lang="ru"
                )
                if not cards:
                    figs_block["describe_lines"] = ["–î–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ."]
                else:
                    lines = []
                    for c in cards:
                        disp = c.get("display") or "–†–∏—Å—É–Ω–æ–∫"
                        vis  = (c.get("vision") or {}).get("description", "") or ""
                        vis_clean = vis.strip()
                        low_vis = vis_clean.lower()
                        if ("–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ" in low_vis
                                or "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è" in low_vis):
                            vis_clean = ""
                        hint = "; ".join([h for h in (c.get("highlights") or []) if h])
                        if vis_clean:
                            lines.append(f"{disp}: {vis_clean}")
                        elif hint:
                            lines.append(f"{disp}: {hint}")
                        else:
                            lines.append(disp)
                    figs_block["describe_lines"] = lines[:25]
            except Exception as e:
                figs_block["describe_lines"] = [f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø–∏—Å–∞—Ç—å —Ä–∏—Å—É–Ω–∫–∏: {e}"]


        facts["figures"] = figs_block

    # ----- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ -----
    if intents["sources"]["want"]:
        con = get_conn()
        cur = con.cursor()
        has_type = _table_has_columns(con, "chunks", ["element_type", "attrs"])
        items: list[str] = []

        if has_type:
            cur.execute(
                "SELECT text FROM chunks WHERE owner_id=? AND doc_id=? AND element_type='reference' ORDER BY id ASC",
                (uid, doc_id),
            )
            items = [(r["text"] or "").strip() for r in cur.fetchall()]

        if not any(items):
            cur.execute(
                """
                SELECT element_type, section_path, text
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                ORDER BY page ASC, id ASC
                """,
                (uid, doc_id),
            )
            raw = []
            for r in cur.fetchall():
                sec = (r["section_path"] or "").lower()
                if not any(k in sec for k in ("–∏—Å—Ç–æ—á–Ω–∏–∫", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ", "reference", "bibliograph")):
                    continue
                et = (r["element_type"] or "").lower()
                if et in ("heading", "table", "figure", "table_row"):
                    continue
                t = (r["text"] or "").strip()
                if t:
                    raw.append(t)

            seen = set()
            items = []
            for t in raw:
                k = re.sub(r"\s+", " ", t).strip().rstrip(".").lower()
                if len(k) < 5 or k in seen:
                    continue
                seen.add(k)
                items.append(t)

        con.close()

        s_limit = int(intents.get("sources", {}).get("limit", 25))
        facts["sources"] = {
            "count": len(items),
            "list": items[:s_limit],
            "more": max(0, len(items) - s_limit),
        }


    # ----- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å -----
    if intents.get("practical"):
        facts["practical_present"] = _has_practical_part(uid, doc_id)

    # ----- Summary -----
    if intents.get("summary"):
        s = overview_context(uid, doc_id, max_chars=6000) or _first_chunks_context(uid, doc_id, n=12, max_chars=6000)
        if s:
            facts["summary_text"] = s

    # ----- –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç / —Ü–∏—Ç–∞—Ç—ã -----
    # app/bot.py (_gather_facts: –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç / —Ü–∏—Ç–∞—Ç—ã)
    if intents.get("general_question"):
        vb = verbatim_find(uid, doc_id, intents["general_question"], max_hits=3)

        cov = retrieve_coverage(
            owner_id=uid,
            doc_id=doc_id,
            question=intents["general_question"],
        )
        ctx = ""
        if cov and cov.get("snippets"):
            ctx = build_context_coverage(
                cov["snippets"],
                items_count=len(cov.get("items") or []) or None,
            )

        if not ctx:
            ctx = best_context(uid, doc_id, intents["general_question"], max_chars=6000)
        if not ctx:
            hits = retrieve(uid, doc_id, intents["general_question"], top_k=12)
            if hits:
                ctx = build_context(hits)
        if not ctx:
            ctx = _first_chunks_context(uid, doc_id, n=12, max_chars=6000)

        if ctx:
            facts["general_ctx"] = ctx
        if vb:
            facts["verbatim_hits"] = vb
        if cov and cov.get("items"):
            facts["coverage"] = {"items": cov["items"]}
            facts["general_subitems"] = [
                {"id": i + 1, "ask": s} if isinstance(s, str) else s
                for i, s in enumerate(cov["items"])
            ]

        # --- [VISION] –≤—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥: —á–∏—Å–ª–∞ –∏–∑ –¥–∏–∞–≥—Ä–∞–º–º/–∫–∞—Ä—Ç–∏–Ω–æ–∫ (–ø–æ–¥–º–µ—à–∏–≤–∞–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç) ---
        try:
            vision_block = ""
            if Cfg.vision_active():
                # 1) –±–µ—Ä—ë–º —Ç–æ–ø-—Ö–∏—Ç—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫
                hits_v = retrieve(uid, doc_id, intents["general_question"], top_k=10) or []

                # 1–∞) –µ—Å–ª–∏ –≤ —Ö–∏—Ç–∞—Ö –µ—Å—Ç—å chart_data (DOCX-–¥–∏–∞–≥—Ä–∞–º–º—ã) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–∞, –±–µ–∑ vision
                chart_lines: list[str] = []
                for h in hits_v:
                    attrs = (h.get("attrs") or {})
                    cd = attrs.get("chart_data")
                    if cd:
                        # –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä: —É–ø–∞–∫—É–µ–º –≤ attrs-json
                        try:
                            cd_list, _, _ = _parse_chart_data(json.dumps({"chart_data": cd}))
                        except Exception:
                            cd_list = None
                        if cd_list:
                            chart_lines.append(_format_chart_values(cd_list))

                if chart_lines:
                    vision_block = "\n".join(chart_lines[:3])
                else:
                    # 2) –∏–Ω–∞—á–µ ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º 1‚Äì3 –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ vision_extract_values
                    img_paths = _pick_images_from_hits(hits_v, limit=getattr(Cfg, "VISION_MAX_IMAGES_PER_REQUEST", 3))
                    if img_paths and vision_extract_values:
                        hint = (hits_v[0].get("text") or "")[:300]
                        res = vision_extract_values(img_paths, caption_hint=hint, lang="ru")
                        rows = (res or {}).get("data") or []
                        if rows:
                            vision_block = "\n".join(
                                ["[Text on image]"] +
                                _pairs_to_bullets(rows).splitlines()
                            )
                        elif FIG_STRICT:
                            # –Ω–µ—Ç –Ω–∞–¥—ë–∂–Ω—ã—Ö —á–∏—Å–µ–ª —Å –∫–∞—Ä—Ç–∏–Ω–∫–∏ ‚Äî —è–≤–Ω–æ –ø–æ–º–µ—á–∞–µ–º
                            vision_block = "[No precise data]"

            if vision_block:
                prev = facts.get("general_ctx") or ""
                glue = ("\n\n" if prev else "")
                facts["general_ctx"] = (prev + glue + vision_block)
        except Exception:
            # –Ω–µ –ª–æ–º–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ vision –¥–∞–ª —Å–±–æ–π
            pass
        # --- [/VISION] ---


    # –ª–æ–≥–∏—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Å—Ä–µ–∑ —Ñ–∞–∫—Ç–æ–≤ (–±–µ–∑ –æ–≥—Ä–æ–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤)
    log_snapshot = dict(facts)
    if "general_ctx" in log_snapshot and isinstance(log_snapshot["general_ctx"], str):
        log_snapshot["general_ctx"] = log_snapshot["general_ctx"][:300] + "‚Ä¶" if len(log_snapshot["general_ctx"]) > 300 else log_snapshot["general_ctx"]
    if "summary_text" in log_snapshot and isinstance(log_snapshot["summary_text"], str):
        log_snapshot["summary_text"] = log_snapshot["summary_text"][:300] + "‚Ä¶" if len(log_snapshot["summary_text"]) > 300 else log_snapshot["summary_text"]
    logging.debug("FACTS: %s", json.dumps(log_snapshot, ensure_ascii=False))
    return facts


def _strip_unwanted_sections(s: str) -> str:
    """–£–¥–∞–ª—è–µ–º —Ä–∞–∑–¥–µ–ª—ã '–ß–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç'/'–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç' –∏ –ø–æ–¥–æ–±–Ω—ã–µ —Ö–≤–æ—Å—Ç—ã."""
    if not s:
        return s
    # –≤—ã—Ä–µ–∑–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –∞–±–∑–∞—Ü(—ã) –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—É—Å—Ç–æ–≥–æ —Ä–∞–∑—Ä—ã–≤–∞
    pat = re.compile(r"(?mis)^\s*(?:—á–µ–≥–æ|—á—Ç–æ)\s+–Ω–µ\s+—Ö–≤–∞—Ç–∞–µ—Ç\s*:.*?(?:\n\s*\n|\Z)")
    s = pat.sub("", s)
    # –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏-–º–µ—Ç–∫–∏
    s = re.sub(r"(?mi)^\s*–Ω–µ\s+—Ö–≤–∞—Ç–∞–µ—Ç\s*:.*$", "", s)
    return s.strip()


# ------------------------------ FULLREAD: –º–æ–¥–µ–ª—å —á–∏—Ç–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª ------------------------------

def _full_document_text(owner_id: int, doc_id: int, *, limit_chars: int | None = None) -> str:
    """–°–∫–ª–µ–∏–≤–∞–µ–º –í–ï–°–¨ —Ç–µ–∫—Å—Ç –∏–∑ chunks (page ASC, id ASC)."""
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT text FROM chunks WHERE owner_id=? AND doc_id=? ORDER BY page ASC, id ASC",
        (owner_id, doc_id),
    )
    rows = cur.fetchall() or []
    con.close()

    parts = []
    total = 0
    for r in rows:
        t = (r["text"] or "").strip()
        if not t:
            continue
        if limit_chars is not None and total + len(t) > limit_chars:
            remaining = max(0, limit_chars - total)
            if remaining > 0:
                parts.append(t[:remaining])
                total += remaining
            break
        parts.append(t)
        total += len(t)
    return "\n\n".join(parts)

def _fullread_try_answer(uid: int, doc_id: int, q_text: str) -> str | None:
    """
    DIRECT: –æ—Ç–¥–∞—ë–º –º–æ–¥–µ–ª–∏ —Ü–µ–ª–∏–∫–æ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–∞–∫ –µ–¥–∏–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None (—É–π–¥—ë–º –≤ –∏–Ω–æ–π —Ä–µ–∂–∏–º).
    """
    if getattr(Cfg, "FULLREAD_MODE", "off") != "direct":
        return None

    _limit = int(getattr(Cfg, "DIRECT_MAX_CHARS", 80000))
    full_text = _full_document_text(uid, doc_id, limit_chars=_limit + 1)
    if not full_text.strip():
        return None

    if len(full_text) > _limit:
        return None

    system_prompt = (
        "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –¢–µ–±–µ –¥–∞–Ω –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç –í–ö–†/–¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
        "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ–≤. –ù–µ –¥–æ–±–∞–≤–ª—è–π —Ä–∞–∑–¥–µ–ª–æ–≤ –≤–∏–¥–∞ "
        "¬´–ß–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç¬ª –∏ –Ω–µ –ø—Ä–æ—Å–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n"
        "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–¥–ø–∏—Å–∏ –∏ –±–ª–∏–∂–∞–π—à–∏–π —Ç–µ–∫—Å—Ç; –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞/–∑–Ω–∞—á–µ–Ω–∏—è.\n"
        "–ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞/—Ç–∞–±–ª–∏—Ü—ã –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ ‚Äî –æ—Ç–≤–µ—Ç—å: ¬´–¥–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ¬ª.\n"
        "–ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç –µ—Å—Ç—å, –Ω–æ –æ–Ω –≤ –ø–ª–æ—Ö–æ–º –∫–∞—á–µ—Å—Ç–≤–µ/–Ω–µ—á–∏—Ç–∞–µ–º ‚Äî –æ—Ç–≤–µ—Ç—å: ¬´–†–∏—Å—É–Ω–æ–∫ –ø–ª–æ—Ö–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–µ –º–æ–≥—É –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å¬ª, "
        "–∏ –¥–æ–±–∞–≤—å –∫—Ä–∞—Ç–∫—É—é –ø–æ–¥–ø–∏—Å—å/–∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞. –¶–∏—Ç–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–æ, –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
    )

    verbosity = _detect_verbosity(q_text)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": f"[–î–æ–∫—É–º–µ–Ω—Ç ‚Äî –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç]\n{full_text}"},
        {"role": "user", "content": f"{q_text}\n\n{_verbosity_addendum(verbosity)}"},
    ]


    if STREAM_ENABLED and chat_with_gpt_stream is not None:
        return ("__STREAM__", json.dumps(messages, ensure_ascii=False))

    try:
        answer = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
        return (answer or "").strip() or None
    except Exception as e:
        logging.exception("fullread direct failed: %s", e)
        return None


def _fullread_collect_sections(uid: int, doc_id: int, *, max_sections: int = 800) -> List[str]:
    """
    –°–µ–∫—Ü–∏–∏ –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞: —Å–æ–±–∏—Ä–∞–µ–º –±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ section_path –≤ –ø–æ—Ä—è–¥–∫–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.
    """
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT section_path, text FROM chunks WHERE owner_id=? AND doc_id=? ORDER BY page ASC, id ASC",
        (uid, doc_id)
    )
    rows = cur.fetchall() or []
    con.close()

    out: List[str] = []
    cur_sec = None
    buf: List[str] = []

    def _flush():
        if buf:
            text = "\n".join([t for t in buf if t.strip()]).strip()
            if text:
                title = f"[{cur_sec}]" if cur_sec else ""
                out.append(f"{title}\n{text}" if title else text)
        buf.clear()


    for r in rows:
        sec = r["section_path"] or ""
        t = (r["text"] or "").strip()
        if not t:
            continue
        if cur_sec is None:
            cur_sec = sec
        if sec != cur_sec:
            _flush()
            cur_sec = sec
        buf.append(t)
        if len(out) >= max_sections:
            break
    _flush()
    return out[:max_sections]


def _group_for_steps(sections: Iterable[str], per_step_chars: int, max_steps: int) -> List[str]:
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–µ–∫—Ü–∏–∏ –≤ –±–∞—Ç—á–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º (–¥–ª—è map-—à–∞–≥–∞)."""
    batches: List[str] = []
    cur: List[str] = []
    cur_len = 0
    for s in sections:
        if cur_len + len(s) + 1 > per_step_chars and cur:
            batches.append("\n\n".join(cur))
            cur, cur_len = [], 0
            if len(batches) >= max_steps:
                break
        cur.append(s)
        cur_len += len(s) + 1
    if cur and len(batches) < max_steps:
        batches.append("\n\n".join(cur))
    return batches[:max_steps]

def _map_extract(uid: int, doc_id: int, question: str, chunk_text: str, *, map_tokens: int) -> str:
    """–û–¥–∏–Ω map-–≤—ã–∑–æ–≤: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã/—Ü–∏—Ç–∞—Ç—ã –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞."""
    sys_map = (
        "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä. –¢–µ–±–µ –¥–∞–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–∏–ø–ª–æ–º–∞ –∏ –≤–æ–ø—Ä–æ—Å. "
        "–ò–∑–≤–ª–µ–∫–∏ –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏ –º–∏–Ω–∏-—Ü–∏—Ç–∞—Ç—ã, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ –≤–æ–ø—Ä–æ—Å—É. "
        "–ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ç–∞–±–ª–∏—Ü—ã ‚Äî –≤–∫–ª—é—á–∞–π –∏—Ö –Ω–∞–∑–≤–∞–Ω–∏—è –∏ 1‚Äì2 –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å —á–∏—Å–ª–∞–º–∏ "
        "(—Å–æ—Ö—Ä–∞–Ω—è–π –ø–æ—Ä—è–¥–æ–∫ –∏ –∑–Ω–∞—á–µ–Ω–∏—è). –§–æ—Ä–º–∞—Ç: –±—É–ª–ª–µ—Ç—ã."
    )
    return chat_with_gpt(
        [
            {"role": "system", "content": sys_map},
            {"role": "assistant", "content": f"[–§—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞]\n{chunk_text}"},
            {"role": "user", "content": f"–í–æ–ø—Ä–æ—Å: {question}\n–°–¥–µ–ª–∞–π –∫–æ—Ä–æ—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É (–±—É–ª–ª–µ—Ç—ã)."},
        ],
        temperature=0.1,
        max_tokens=max(120, int(map_tokens)),
    )

def _iterative_fullread_build_messages(uid: int, doc_id: int, question: str) -> Tuple[Optional[list], Optional[str]]:
    """
    –°–æ–±–∏—Ä–∞–µ–º map-–≤—ã–∂–∏–º–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º reduce-—Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Å—Ç—Ä–∏–º–∞
    –ò–õ–ò –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç (–µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫).
    """
    per_step = int(getattr(Cfg, "FULLREAD_STEP_CHARS", 14000))
    max_steps = int(getattr(Cfg, "FULLREAD_MAX_STEPS", 2))
    map_tokens = int(getattr(Cfg, "DIGEST_TOKENS_PER_SECTION", 300))

    sections = _fullread_collect_sections(uid, doc_id)
    if not sections:
        return None, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç —Å–µ–∫—Ü–∏—è–º–∏."

    batches = _group_for_steps(sections, per_step_chars=per_step, max_steps=max_steps)
    if not batches:
        return None, "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —à–∞–≥–∏ —á—Ç–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞."

    digests: List[str] = []
    for b in batches:
        try:
            digests.append(_map_extract(uid, doc_id, question, b, map_tokens=map_tokens))
        except Exception as e:
            logging.exception("map extract failed: %s", e)
            digests.append(b[:800])

    joined = "\n\n".join([f"[MAP {i+1}]\n{d}" for i, d in enumerate(digests)])
    ctx = joined[: int(getattr(Cfg, "FULLREAD_CONTEXT_CHARS", 9000))]

    sys_reduce = (
        "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –í–ö–†. –ù–∏–∂–µ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞ (map-–≤—ã–∂–∏–º–∫–∏). "
        "–°–æ–±–µ—Ä–∏ –∏–∑ –Ω–∏—Ö —Å–≤—è–∑–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã—Ö —Ü–∏—Ñ—Ä/—Ç–∞–±–ª–∏—Ü –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–π —Ä–∞–∑–¥–µ–ª–æ–≤ "
        "–ø—Ä–æ ¬´—á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç¬ª. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ –∏–º–µ—é—â–∏–º—Å—è –¥–∞–Ω–Ω—ã–º. –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞/—Ç–∞–±–ª–∏—Ü—ã "
        "–Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ ‚Äî —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∫—Ä–∞—Ç–∫–æ: ¬´–¥–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ¬ª. –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç –µ—Å—Ç—å, –Ω–æ –æ–Ω "
        "–Ω–µ—á–∏—Ç–∞–±–µ–ª–µ–Ω, –¥–∞–π: ¬´–†–∏—Å—É–Ω–æ–∫ –ø–ª–æ—Ö–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–µ –º–æ–≥—É –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å¬ª, –∏ –¥–æ–±–∞–≤—å –ø–æ–¥–ø–∏—Å—å/–∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞."
    )

    verbosity = _detect_verbosity(question)
    messages = [
        {"role": "system", "content": sys_reduce},
        {"role": "assistant", "content": f"–°–≤–æ–¥–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{ctx}"},
        {"role": "user", "content": f"{question}\n\n{_verbosity_addendum(verbosity)}"},
    ]
    return messages, None


# ------------------------------ –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ ------------------------------

@dp.message(F.document)
async def handle_doc(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc = m.document

    # 0) FSM: —Ñ–∏–∫—Å–∏—Ä—É–µ–º, —á—Ç–æ –Ω–∞—á–∞–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
    start_downloading(uid)
    await _send(m, Cfg.MSG_ACK_DOWNLOADING)

    # 1) —Å–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
    file = await bot.get_file(doc.file_id)
    stream = await bot.download_file(file.file_path)
    try:
        data = stream.read()
    finally:
        try:
            stream.close()
        except Exception:
            pass


    # 2) —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫ (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞)
    filename = safe_filename(f"{m.from_user.id}_{doc.file_name}")
    path = save_upload(data, filename, Cfg.UPLOAD_DIR)
    await _send(m, Cfg.MSG_ACK_INDEXING)

    # 3) –æ–±—ë—Ä—Ç–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞ –ø–æ–¥ —Å–∏–≥–Ω–∞—Ç—É—Ä—É –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞ (–∑–∞–º—ã–∫–∞–µ–º uid)
    def _indexer_fn(doc_id: int, file_path: str, kind: str) -> dict:
        sections = _parse_by_ext(file_path)
        sections = enrich_sections(sections, doc_kind=os.path.splitext(file_path)[1].lower().strip("."))
        # sanity-check –Ω–∞ ¬´–ø—É—Å—Ç—ã–µ¬ª —Ñ–∞–π–ª—ã
        if sum(len(s.get("text") or "") for s in sections) < 500 and not any(
            s.get("element_type") in ("table", "table_row", "figure") for s in sections
        ):
            raise RuntimeError("–ü–æ—Ö–æ–∂–µ, —Ñ–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç ¬´–∂–∏–≤–æ–≥–æ¬ª —Ç–µ–∫—Å—Ç–∞/—Å—Ç—Ä—É–∫—Ç—É—Ä.")
        # –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è ¬´–∫–∞–∫ —Ä–∞–Ω—å—à–µ¬ª
        delete_document_chunks(doc_id, uid)
        index_document(uid, doc_id, sections)
        invalidate_cache(uid, doc_id)
        update_document_meta(doc_id, layout_profile=_current_embedding_profile())
        return {"sections_count": len(sections)}

    # 4) –∑–∞–ø—É—Å–∫–∞–µ–º –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä (–æ–Ω —Å–∞–º: –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å, INDEXING, READY/IDLE, –≤–µ—Ä—Å–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞)
    try:
        result = ingest_document(
            user_id=uid,
            file_path=path,
            kind=infer_doc_kind(doc.file_name),
            file_uid=getattr(doc, "file_unique_id", None),
            content_sha256=sha256_bytes(data),
            indexer_fn=_indexer_fn,
        )
    except Exception as e:
        logging.exception("ingest failed: %s", e)
        await _send(m, Cfg.MSG_INDEX_FAILED + f" –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏: {e}")
        return

    doc_id = int(result["doc_id"])
    ACTIVE_DOC[uid] = doc_id
    set_user_active_doc(uid, doc_id)

    # NEW: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞—Ç—å (—Å—Ç–∞—Ä—ã–π –ø—É—Ç—å)
    try:
        if fig_index_document is not None:
            FIG_INDEX[doc_id] = fig_index_document(path)
    except Exception as e:
        logging.exception("figures indexing failed: %s", e)


        # NEW: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ï–î–ò–ù–´–ô OOXML-–∏–Ω–¥–µ–∫—Å (–≥–ª–∞–≤—ã/—Ä–∏—Å—É–Ω–∫–∏/—Ç–∞–±–ª–∏—Ü—ã/–∏—Å—Ç–æ—á–Ω–∏–∫–∏) –±–µ–∑ LibreOffice
    # NEW: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ï–î–ò–ù–´–ô OOXML-–∏–Ω–¥–µ–∫—Å (–≥–ª–∞–≤—ã/—Ä–∏—Å—É–Ω–∫–∏/—Ç–∞–±–ª–∏—Ü—ã/–∏—Å—Ç–æ—á–Ω–∏–∫–∏) –±–µ–∑ LibreOffice
    try:
        idx_oox = oox_build_index(path)
        OOXML_INDEX[doc_id] = idx_oox
        #persist –ø–æ–¥ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –ë–î ‚Äî —á—Ç–æ–±—ã _ooxml_get_index —Ä–∞–±–æ—Ç–∞–ª –ø–æ—Å–ª–µ —Ä–µ—Å—Ç–∞—Ä—Ç–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞
        try:
            os.makedirs(os.path.join("runtime", "indexes"), exist_ok=True)
            with open(os.path.join("runtime", "indexes", f"{doc_id}.json"), "w", encoding="utf-8") as f:
                json.dump(idx_oox, f, ensure_ascii=False, indent=2)
        except Exception:
            pass
    except Exception as e:
        logging.exception("ooxml build_index failed: %s", e)


    # 5) READY: —Å–æ–æ–±—â–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º ...
    await _send(
        m,
        (f"–≠—Ç–æ—Ç —Ñ–∞–π–ª —É–∂–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç #{doc_id}. " if result.get("reused") else "") + Cfg.MSG_READY
    )

    caption = (m.caption or "").strip()
    if caption:
        await respond_with_answer(m, uid, doc_id, caption)

    # –∞–≤—Ç–æ-–¥—Ä–µ–Ω–∞–∂ –æ—á–µ—Ä–µ–¥–∏ –æ–∂–∏–¥–∞–Ω–∏—è (–±–µ–∑ –¥—É–±–ª–µ–π –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤)
    try:
        queued = dequeue_all_pending_queries(uid)
        for item in queued:
            q = (item.get("text") or "").strip()
            if not q:
                continue
            # –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –∏–∑ –æ—á–µ—Ä–µ–¥–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø–æ–¥–ø–∏—Å—å—é –∫ —Ñ–∞–π–ª—É ‚Äî –Ω–µ –æ—Ç–≤–µ—á–∞–µ–º –≤—Ç–æ—Ä–æ–π —Ä–∞–∑
            if caption and q.strip() == caption:
                continue
            await respond_with_answer(m, uid, doc_id, q)
            await asyncio.sleep(0)  # –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º —Ü–∏–∫–ª
    except Exception as e:
        logging.exception("drain pending queue failed: %s", e)



# ------------------------------ –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç—á–∏–∫ ------------------------------

async def respond_with_answer(m: types.Message, uid: int, doc_id: int, q_text: str):
    q_text = (q_text or "").strip()
    logging.debug(f"–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {q_text}")
    if not q_text:
        await _send(m, "–í–æ–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π. –ù–∞–ø–∏—à–∏—Ç–µ, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –ø–æ –í–ö–†.")
        return

    viol = safety_check(q_text)
    if viol:
        await _send(m, viol + " –ó–∞–¥–∞–π—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ –í–ö–†.")
        return

    if await _maybe_run_gost(m, uid, doc_id, q_text):
        return


    # –†–ê–ù–û –≤ respond_with_answer, –¥–æ detect_intents:
    if _ALL_FIGS_HINT.search(q_text or ""):
        meta = _list_figures_db(uid, doc_id, limit=999999)
        total = int(meta["count"])
        if total == 0:
            await _send(m, "–í —Ä–∞–±–æ—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞.")
            return
        # –ø–∞—Ä—Ç–∏—è–º–∏ –ø–æ 8‚Äì12 –Ω–æ–º–µ—Ä–æ–≤
        nums = []
        for disp in meta["list"]:
            # –∏–∑ "–†–∏—Å—É–Ω–æ–∫ 2.1 ‚Äî ..." –≤—ã—Ç–∞—â–∏–º "2.1" (–µ—Å–ª–∏ –µ—Å—Ç—å)
            mnum = re.search(r"(?i)\b—Ä–∏—Å—É–Ω–æ–∫\s+([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*)\b", disp)
            if mnum:
                nums.append(mnum.group(1).replace(" ", "").replace(",", "."))
        batch = nums[:8] or nums[:12]
        # –∫–∞—Ä—Ç–æ—á–∫–∏ + —Å–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤–∏–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        cards = []
        try:
            cards = describe_figures_by_numbers(uid, doc_id, batch, sample_chunks=1, use_vision=False, lang="ru") or []
        except Exception:
            cards = []
        await _send_media_from_cards(m, cards)

        # –∑–∞—Ç–µ–º ‚Äî —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ –∫–∞–∂–¥–æ–º—É —Ä–∏—Å—É–Ω–∫—É: prefer vision_analyzer
        lines = []
        if va_analyze_figure and cards:
            for c in cards:
                disp = c.get("display") or f"–†–∏—Å—É–Ω–æ–∫ {c.get('num') or ''}".strip()
                imgs = c.get("images") or []
                hint = (c.get("highlights") or [None])[0]
                if not imgs:
                    continue
                try:
                    res = va_analyze_figure(imgs[0], caption_hint=hint, lang="ru")
                    if isinstance(res, dict):
                        text_block = (res.get("text") or "").strip() or _pairs_to_bullets(res.get("data") or [])
                    else:
                        text_block = (str(res) or "").strip()
                except Exception:
                    text_block = ""
                if not text_block:
                    # —Ñ–æ–ª–±—ç–∫ ‚Äî —Å—Ç–∞—Ä—ã–π summarizer
                    text_block = ""
                if text_block:
                    # —ç—Ç–æ —Ç–µ–∫—Å—Ç –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (OCR/–æ–ø–∏—Å–∞–Ω–∏–µ)
                    lines.append(f"[Text on image] **{disp}**\n\n{text_block}")
                else:
                    # —Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º ‚Äî —è–≤–Ω–æ –≥–æ–≤–æ—Ä–∏–º, —á—Ç–æ —Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç
                    if FIG_STRICT:
                        lines.append(f"[No precise data] **{disp}**")


        suffix = (f"\n\n–ü–æ–∫–∞–∑–∞–Ω–∞ –ø–µ—Ä–≤–∞—è –ø–∞—Ä—Ç–∏—è –∏–∑ {len(batch)} / {total}." if total > len(batch) else "")
        if lines:
            await _send(m, "\n\n".join(lines) + suffix)
        else:
            # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–ª–±—ç–∫, –µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            txt = vision_describe_figures(uid, doc_id, batch)
            await _send(m, (txt or "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø–∏—Å–∞—Ç—å —Ä–∏—Å—É–Ω–∫–∏.") + suffix)
        return


    # NEW: –µ—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω —Ä–∞–∑–¥–µ–ª/–ø—É–Ω–∫—Ç ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –µ–≥–æ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π
    m_area = _SECTION_NUM_RE.search(q_text)
    if m_area:
        try:
            area = (m_area.group(1) or "").replace(" ", "").replace(",", ".")
            LAST_REF.setdefault(uid, {})["area"] = area
        except Exception:
            pass

    # NEW: –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Ä–∞—Å–ø–ª—ã–≤—á–∞—Ç—ã–π ¬´–ø—Ä–æ —ç—Ç–æ—Ç ...¬ª, –ø–æ–¥—Å—Ç–∞–≤–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Ç
    def _expand_with_last_referent(uid: int, text: str) -> str:
        if not _ANAPH_HINT_RE.search(text or ""):
            return text
        last = LAST_REF.get(uid) or {}
        # –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∏—Å—É–Ω–æ–∫
        figs = last.get("figure_nums") or []
        if figs:
            return f"{text} (–∏–º–µ–µ—Ç—Å—è –≤ –≤–∏–¥—É —Ä–∏—Å—É–Ω–æ–∫ {figs[0]})"
        area = (last.get("area") or "").strip()
        if area:
            # –µ—Å–ª–∏ –Ω–µ—Ç —Å–ª–æ–≤–∞ ¬´–ø—É–Ω–∫—Ç/—Ä–∞–∑–¥–µ–ª¬ª, –¥–æ–±–∞–≤–∏–º
            if not re.search(r"(?i)\b(–≥–ª–∞–≤–∞|—Ä–∞–∑–¥–µ–ª|–ø—É–Ω–∫—Ç|–ø–æ–¥—Ä–∞–∑–¥–µ–ª)\b", text):
                return f"{text} (–∏–º–µ–µ—Ç—Å—è –≤ –≤–∏–¥—É –ø—É–Ω–∫—Ç {area})"
            return f"{text} ({area})"
        return text
    q_text = _expand_with_last_referent(uid, q_text)

    # NEW: –±—ã—Å—Ç—Ä—ã–π –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—É—Ç—å –¥–ª—è ¬´–ø–æ—è—Å–Ω–∏ —Ä–∏—Å—É–Ω–æ–∫ 2.1/3.4 ‚Ä¶¬ª
    # --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç—ã –∑–∞—Ä–∞–Ω–µ–µ
    intents = detect_intents(q_text)
    verbosity = _detect_verbosity(q_text)

    # –ß–∏—Å—Ç—ã–π –∑–∞–ø—Ä–æ—Å –ø—Ä–æ —Ä–∏—Å—É–Ω–∫–∏ (–Ω–µ—Ç —Å–µ–∫—Ü–∏–π/—Ç–∞–±–ª–∏—Ü/–æ–±—â–µ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è)
    pure_figs = intents["figures"]["want"] and not (
        intents["tables"]["want"] or intents["sources"]["want"] or
        intents.get("summary") or intents.get("general_question") or
        _SECTION_NUM_RE.search(q_text)
    )

    if intents["figures"]["want"]:
        try:
            await _ensure_modalities_indexed(m, uid, doc_id, intents)  # –µ—Å–ª–∏ figure==0, —Ç–∏—Ö–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç
        except Exception:
            pass

    if pure_figs:
        if await _answer_figure_query(m, uid, doc_id, q_text, verbosity=verbosity):
            return
    else:
        if intents["figures"]["want"]:
            # —Å–Ω–∞—á–∞–ª–∞ –∫—Ä–∞—Ç–∫–æ –æ—Ç–≤–µ—Ç–∏–º –ø–æ —Ä–∏—Å—É–Ω–∫–∞–º, –∑–∞—Ç–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏–º –æ–±—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω
            await _answer_figure_query(m, uid, doc_id, q_text, verbosity=verbosity)



    # NEW: —è–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ¬´–ø–æ –ø—É–Ω–∫—Ç—É/—Ä–∞–∑–¥–µ–ª—É/–≥–ª–∞–≤–µ X.Y¬ª
        # NEW: —è–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ¬´–ø–æ –ø—É–Ω–∫—Ç—É/—Ä–∞–∑–¥–µ–ª—É/–≥–ª–∞–≤–µ X.Y¬ª (—Å –∑–∞—â–∏—Ç–æ–π –æ—Ç ¬´–∑–∞–ª–∏–ø–∞–Ω–∏–π¬ª)
    m_sec = _SECTION_NUM_RE.search(q_text)
    sec = None
    if m_sec:
        raw_sec = (m_sec.group(1) or "").strip()
        raw_sec = re.sub(r"^[A-Za-z–ê-–Ø–∞-—è]\s+(?=\d)", "", raw_sec)
        sec = raw_sec.replace(" ", "").replace(",", ".")

    # –í–°–ï–ì–î–ê –ø–µ—Ä–≤—ã–º –¥–µ–ª–æ–º –ø—Ä–æ–±—É–µ–º —Å—Ç—Ä–æ–≥–∏–π —Å–µ–∫—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ –Ω–æ–º–µ—Ä –Ω–∞–π–¥–µ–Ω
    if sec:
        verbosity = _detect_verbosity(q_text)
        ctx = _section_context(uid, doc_id, sec, max_chars=9000)
        if ctx:
            base_sys = (
                "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –í–ö–†. –ù–∏–∂–µ ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç –¢–û–õ–¨–ö–û –ø–æ –æ–¥–Ω–æ–º—É –ø—É–Ω–∫—Ç—É/–≥–ª–∞–≤–µ –¥–∏–ø–ª–æ–º–∞.\n"
                "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É: –Ω–µ –¥–æ–±–∞–≤–ª—è–π –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ–≤, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã—Ö –ø–æ–ª–æ–∂–µ–Ω–∏–π "
                "–∏ –Ω–µ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–π —Ç–æ, —á–µ–≥–æ –≤ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ –Ω–µ—Ç. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ –Ω–∞–ø–∏—à–∏, "
                "—á—Ç–æ –¥–∞–Ω–Ω—ã—Ö –≤ —ç—Ç–æ–º –ø—É–Ω–∫—Ç–µ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."
            )
            if verbosity == "brief":
                sys_prompt = base_sys + " –ù—É–∂–Ω–∞ –ö–†–ê–¢–ö–ê–Ø –≤—ã–∂–∏–º–∫–∞."
                user_prompt = (
                    f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {q_text}\n\n"
                    f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –ø–æ –ø—É–Ω–∫—Ç—É {sec}. {_verbosity_addendum('brief')}"
                )
            elif verbosity == "detailed":
                sys_prompt = base_sys + " –ù—É–∂–µ–Ω –ü–û–î–†–û–ë–ù–´–ô —Ä–∞–∑–±–æ—Ä."
                user_prompt = (
                    f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {q_text}\n\n"
                    f"–°–¥–µ–ª–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ä–∞–∑–±–æ—Ä –ø–æ –ø—É–Ω–∫—Ç—É {sec}. {_verbosity_addendum('detailed')}"
                )
            else:
                sys_prompt = base_sys + " –û—Ç–≤–µ—Ç—å –ø–æ –¥–µ–ª—É, –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
                user_prompt = (
                    f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {q_text}\n\n"
                    f"–û—Ç–≤–µ—Ç—å –ø–æ –ø—É–Ω–∫—Ç—É {sec}. {_verbosity_addendum('normal')}"
                )

            messages = [
                {"role": "system", "content": sys_prompt},
                {"role": "assistant", "content": f"[–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –ø—É–Ω–∫—Ç—É {sec}]\n{ctx}"},
                {"role": "user", "content": user_prompt},
            ]

            if STREAM_ENABLED and chat_with_gpt_stream is not None:
                try:
                    stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                    await _stream_to_telegram(m, stream)
                    return
                except Exception as e:
                    logging.exception("section summary stream failed: %s", e)
            try:
                ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                if ans:
                    await _send(m, _strip_unwanted_sections(ans))
                    return
            except Exception as e:
                logging.exception("section summary non-stream failed: %s", e)
                # –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è ‚Äî –ø—É—Å—Ç—å –ø–æ–π–¥—ë—Ç –æ–±—ã—á–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –Ω–∏–∂–µ, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ —Å–ª–æ–º–∞–ª–æ—Å—å
        else:
            await _send(m, f"–ü—É–Ω–∫—Ç {sec} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∏–Ω–¥–µ–∫—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞.")
            return

    # –ï—Å–ª–∏ sec –Ω–∞–π–¥–µ–Ω, –Ω–æ –∑–∞–ø—Ä–æ—Å –ù–ï —á–∏—Å—Ç—ã–π ‚Äî –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ –ø—É–Ω–∫—Ç—É,
    # –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—ã—á–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –Ω–∏–∂–µ (RAG / FULLREAD), —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤—Å—ë —Ü–µ–ª–∏–∫–æ–º.




    # ====== FULLREAD: auto ======
    fr_mode = getattr(Cfg, "FULLREAD_MODE", "off")
    if fr_mode == "auto":
        _limit = int(getattr(Cfg, "DIRECT_MAX_CHARS", 80000))
        # –ø—Ä–æ–±—É–µ–º –¥–∞—Ç—å –º–æ–¥–µ–ª–∏ –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –≤–ª–∞–∑–∏—Ç
        full_text = _full_document_text(uid, doc_id, limit_chars=_limit + 1)
        if full_text and len(full_text) <= _limit:
            system_prompt = (
                "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –¢–µ–±–µ –¥–∞–Ω –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç –í–ö–†/–¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
                "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ–≤. –ù–µ –¥–æ–±–∞–≤–ª—è–π —Ä–∞–∑–¥–µ–ª–æ–≤ –≤–∏–¥–∞ "
                "¬´–ß–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç¬ª –∏ –Ω–µ –ø—Ä–æ—Å–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n"
                "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–¥–ø–∏—Å–∏ –∏ –±–ª–∏–∂–∞–π—à–∏–π —Ç–µ–∫—Å—Ç; –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞/–∑–Ω–∞—á–µ–Ω–∏—è.\n"
                "–ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞/—Ç–∞–±–ª–∏—Ü—ã –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ ‚Äî –æ—Ç–≤–µ—Ç—å: ¬´–¥–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ¬ª.\n"
                "–ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç –µ—Å—Ç—å, –Ω–æ –æ–Ω –≤ –ø–ª–æ—Ö–æ–º –∫–∞—á–µ—Å—Ç–≤–µ/–Ω–µ—á–∏—Ç–∞–µ–º ‚Äî –æ—Ç–≤–µ—Ç—å: ¬´–†–∏—Å—É–Ω–æ–∫ –ø–ª–æ—Ö–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–µ –º–æ–≥—É –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å¬ª, "
                "–∏ –¥–æ–±–∞–≤—å –∫—Ä–∞—Ç–∫—É—é –ø–æ–¥–ø–∏—Å—å/–∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞. –¶–∏—Ç–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–æ, –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
            )

            verbosity = _detect_verbosity(q_text)
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "assistant", "content": f"[–î–æ–∫—É–º–µ–Ω—Ç ‚Äî –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç]\n{full_text}"},
                {"role": "user", "content": f"{q_text}\n\n{_verbosity_addendum(verbosity)}"},
            ]

            if STREAM_ENABLED and chat_with_gpt_stream is not None:
                try:
                    stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                    await _stream_to_telegram(m, stream)
                    return
                except Exception as e:
                    logging.exception("auto fullread stream failed: %s", e)
            try:
                ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                if ans:
                    await _send(m, _strip_unwanted_sections(ans))
                    return
            except Exception as e:
                logging.exception("auto fullread non-stream failed: %s", e)
        else:
            # –¥–æ–∫—É–º–µ–Ω—Ç –±–æ–ª—å—à–æ–π ‚Üí –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —á—Ç–µ–Ω–∏–µ (map‚Üíreduce)
            messages, err = _iterative_fullread_build_messages(uid, doc_id, q_text)
            if messages:
                if STREAM_ENABLED and chat_with_gpt_stream is not None:
                    try:
                        stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                        await _stream_to_telegram(m, stream)
                        return
                    except Exception as e:
                        logging.exception("auto iterative stream failed: %s", e)
                try:
                    ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                    if ans:
                        await _send(m, _strip_unwanted_sections(ans))
                        return
                except Exception as e:
                    logging.exception("auto iterative non-stream failed: %s", e)
            elif err:
                await _send(m, err)
                return


    # ====== FULLREAD: direct ======
    if fr_mode == "direct":
        fr = _fullread_try_answer(uid, doc_id, q_text)
        if isinstance(fr, tuple) and fr and fr[0] == "__STREAM__":
            messages = json.loads(fr[1])
            try:
                stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                await _stream_to_telegram(m, stream)
                return
            except Exception as e:
                logging.exception("direct fullread stream failed: %s", e)
                # —Ç–∏—Ö–æ –ø–∞–¥–∞–µ–º –≤ –æ–±—ã—á–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
        elif isinstance(fr, str) and fr:
            await _send(m, _strip_unwanted_sections(fr))
            return
        # –∏–Ω–∞—á–µ ‚Äî RAG –Ω–∏–∂–µ

    # ====== FULLREAD: iterative/digest ======
    if fr_mode in {"iterative", "digest"}:
        messages, err = _iterative_fullread_build_messages(uid, doc_id, q_text)
        if messages:
            if STREAM_ENABLED and chat_with_gpt_stream is not None:
                try:
                    stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                    await _stream_to_telegram(m, stream)
                    return
                except Exception as e:
                    logging.exception("iterative fullread stream failed: %s", e)
            try:
                ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                if ans:
                    await _send(m, _strip_unwanted_sections(ans))
                    return
            except Exception as e:
                logging.exception("iterative fullread non-stream failed: %s", e)
        else:
            if err:
                await _send(m, err)
                return
        # –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ –≤—ã—à–ª–æ ‚Äî –ø—Ä–æ–≤–∞–ª–∏–≤–∞–µ–º—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º –Ω–∏–∂–µ

    # ====== –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º—É–ª—å—Ç–∏-–∏–Ω—Ç–µ–Ω—Ç –ø–∞–π–ø–ª–∞–π–Ω (RAG) ======
    await _ensure_modalities_indexed(m, uid, doc_id, intents)
    facts = _gather_facts(uid, doc_id, intents)


    # ‚Üì –ù–û–í–û–ï: –µ—Å–ª–∏ –µ—Å—Ç—å –ø–ª–∞–Ω –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ ‚Äî –≤–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—É—é –ø–æ–¥–∞—á—É
    discovered_items = None
    if isinstance(facts, dict):
        discovered_items = (facts.get("coverage", {}).get("items")
                            or facts.get("general_subitems"))
    try:
        handled = await _run_multistep_answer(
            m, uid, doc_id, q_text, discovered_items=discovered_items  # –æ—Ç–ø—Ä–∞–≤–∏—Ç A‚ÜíB‚Üí‚Ä¶ –∏ –≤–µ—Ä–Ω—ë—Ç True
        )
        if handled:
            return
    except Exception as e:
        logging.exception("multistep pipeline failed, fallback to normal: %s", e)


        # –æ–±—ã—á–Ω—ã–π –ø—É—Ç—å + —è–≤–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –≤–µ—Ä–±–æ–∑–Ω–æ—Å—Ç–∏
    verbosity = _detect_verbosity(q_text)
    SAFE_RULES = ("–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–º —Ñ–∞–∫—Ç–∞–º –∏ —Ü–∏—Ç–∞—Ç–∞–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. "
                "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏, –±–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞/–∑–Ω–∞—á–µ–Ω–∏—è.")
    enriched_q = f"{SAFE_RULES}\n\n{q_text}\n\n{_verbosity_addendum(verbosity)}"

    # –µ—Å–ª–∏ —Ö–æ—á–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª—è—Ç—å ¬´–ø–æ—Å–ª–µ–¥–Ω–∏–π —É–ø–æ–º—è–Ω—É—Ç—ã–π —Ä–∏—Å—É–Ω–æ–∫¬ª ‚Äî –≤–æ–∑—å–º–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞
    figs_in_q = [_num_norm_fig(n) for n in FIG_NUM_RE.findall(q_text)]
    if figs_in_q:
        LAST_REF.setdefault(uid, {})["figure_nums"] = figs_in_q

    # NEW: –ø—Ä—è–º–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    # (–Ω–µ –ª–æ–º–∞–µ—Ç —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É: –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å/–Ω–µ—Ç –∫–∞—Ä—Ç–∏–Ω–æ–∫ ‚Äî –∏–¥—ë–º –≤ generate_answer)
    try:
        if intents.get("general_question") and getattr(Cfg, "vision_active", lambda: False)():
            # –ø–æ–¥—Ç—è–Ω–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫-—Ö–∏—Ç—ã –∏ –≤—ã–±–µ—Ä–µ–º 1‚Äì3 —Ñ–∞–π–ª–∞-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            hits_v = retrieve(uid, doc_id, intents["general_question"], top_k=10) or []
            img_paths = _pick_images_from_hits(hits_v, limit=getattr(Cfg, "VISION_MAX_IMAGES_PER_REQUEST", 3))
            if img_paths and (chat_with_gpt_stream_multimodal or chat_with_gpt_multimodal):
                # –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                ctx = (facts.get("general_ctx") or "").strip() if isinstance(facts, dict) else ""
                mm_system = (
                    "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –í–ö–†. –£ —Ç–µ–±—è –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å, –∫—Ä–∞—Ç–∫–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è "
                    "(—Ñ–æ—Ç–æ/—Å–∫–∞–Ω—ã/–¥–∏–∞–≥—Ä–∞–º–º—ã) –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –û—Ç–≤–µ—á–∞–π –ø–æ –¥–µ–ª—É, –∏—Å–ø–æ–ª—å–∑—É—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é. "
                    "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∑–Ω–∞—á–µ–Ω–∏—è –∏ –Ω–æ–º–µ—Ä–∞, –ø–∏—à–∏ —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –≤–∏–¥–Ω–æ –∏–ª–∏ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ."
                )
                mm_prompt = (f"{q_text}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{ctx}" if ctx else q_text)

                if STREAM_ENABLED and chat_with_gpt_stream_multimodal is not None:
                    stream = chat_with_gpt_stream_multimodal(
                        mm_prompt,
                        image_paths=img_paths,
                        system=mm_system,
                        temperature=0.2,
                        max_tokens=FINAL_MAX_TOKENS,
                    )
                    await _stream_to_telegram(m, stream)
                    return
                elif chat_with_gpt_multimodal is not None:
                    ans = chat_with_gpt_multimodal(
                        mm_prompt,
                        image_paths=img_paths,
                        system=mm_system,
                        temperature=0.2,
                        max_tokens=FINAL_MAX_TOKENS,
                    )
                    if ans:
                        await _send(m, _strip_unwanted_sections(ans))
                        return
    except Exception as e:
        logging.exception("multimodal answer path failed, falling back: %s", e)

    # —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å RAG ‚Üí –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    if STREAM_ENABLED and generate_answer_stream is not None:
        try:
            stream = generate_answer_stream(enriched_q, facts, language=intents.get("language", "ru"))
            await _stream_to_telegram(m, stream)
            return
        except Exception as e:
            logging.exception("stream answer failed, fallback to non-stream: %s", e)

    reply = generate_answer(enriched_q, facts, language=intents.get("language", "ru"))
    await _send(m, _strip_unwanted_sections(reply))



# ------------------------------ —ç–º–±–µ–¥–¥–∏–Ω–≥-–ø—Ä–æ—Ñ–∏–ª—å ------------------------------

def _current_embedding_profile() -> str:
    dim = probe_embedding_dim(None)
    if dim:
        return f"emb={Cfg.POLZA_EMB}|dim={dim}"
    return f"emb={Cfg.POLZA_EMB}"

def _needs_reindex_by_embeddings(con, doc_id: int) -> bool:
    if not _table_has_columns(con, "documents", ["layout_profile"]):
        return True
    cur = con.cursor()
    cur.execute("SELECT layout_profile FROM documents WHERE id=?", (doc_id,))
    row = cur.fetchone()
    stored = (row["layout_profile"] or "") if row else ""
    if not stored:
        return True
    cur_model = Cfg.POLZA_EMB.strip().lower()
    stored_model = ""
    stored_dim = None
    for part in stored.split("|"):
        part = (part or "").strip().lower()
        if part.startswith("emb="):
            stored_model = part[4:]
        if part.startswith("dim="):
            try:
                stored_dim = int(part[4:])
            except Exception:
                stored_dim = None
    if stored_model and stored_model != cur_model:
        return True
    cur_dim = probe_embedding_dim(None)
    if cur_dim and stored_dim and stored_dim != cur_dim:
        return True
    return False


# ------------------------------ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ------------------------------

@dp.message(F.text & ~F.via_bot & ~F.text.startswith("/"))
async def qa(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid)

    if not doc_id:
        persisted = get_user_active_doc(uid)
        if persisted:
            ACTIVE_DOC[uid] = persisted
            doc_id = persisted

    text = (m.text or "").strip()

    # üëã –†–ê–ù–ù–ò–ô –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –±–µ–∑ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
    if _is_greeting(text):
        greet = getattr(
            Cfg, "MSG_GREET",
            "–ü—Ä–∏–≤–µ—Ç! –Ø —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ —Ç–≤–æ–µ–π –í–ö–†. –ü—Ä–∏—à–ª–∏ —Ñ–∞–π–ª –í–ö–† (.doc/.docx) ‚Äî –∏ —è –ø–æ–º–æ–≥—É –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é."
        )
        await _send(m, greet)
        return

    if not doc_id:
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–ø—Ä–æ—Å, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –ø–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞
        if text:
            enqueue_pending_query(uid, text, meta={"source": "chat", "reason": "no_active_doc"})
        await _send(m, Cfg.MSG_NEED_FILE_QUEUED)
        return

    await respond_with_answer(m, uid, doc_id, text)
