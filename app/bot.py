# app/bot.py
import re
import os
import html
import json
import logging
import asyncio
import time
from typing import Iterable, AsyncIterable, Optional, List, Tuple

from aiogram import Bot, Dispatcher, F, types
from aiogram.filters import Command
from aiogram.exceptions import TelegramBadRequest
from aiogram.enums import ChatAction

# ---------- answer builder: –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å —Å—Ç—Ä–∏–º–æ–≤—É—é –≤–µ—Ä—Å–∏—é, —Ñ–æ–ª–±—ç–∫ –Ω–∞ –Ω–µ—Å—Ç—Ä–∏–º–æ–≤—É—é ----------
try:
    from .answer_builder import generate_answer, generate_answer_stream  # type: ignore
except Exception:
    from .answer_builder import generate_answer  # type: ignore
    generate_answer_stream = None  # —Å—Ç—Ä–∏–º–∞ –Ω–µ—Ç ‚Äî –±—É–¥–µ–º —Ñ–æ–ª–±—ç–∫–∞—Ç—å

from .config import Cfg
from .db import (
    ensure_user, get_conn,
    set_document_indexer_version, get_document_indexer_version,
    CURRENT_INDEXER_VERSION,
    update_document_meta, delete_document_chunks,
    set_user_active_doc, get_user_active_doc,  # ‚¨ÖÔ∏è –ø–µ—Ä—Å–∏—Å—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
)
from .parsing import parse_docx, parse_doc, save_upload
from .indexing import index_document
from .retrieval import (
    retrieve, build_context, invalidate_cache,
    retrieve_coverage, build_context_coverage,
)
from .intents import detect_intents

# ‚Üì –¥–æ–±–∞–≤–∏–ª–∏ –º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç –ø–æ-–ø–æ–¥–ø—É–Ω–∫—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑ ace
try:
    from .ace import plan_subtasks, answer_subpoint, _merge_subanswers as merge_subanswers  # type: ignore
except Exception:
    try:
        # –±—ç–∫–∞–ø: –µ—Å–ª–∏ –≤ ace —Ñ—É–Ω–∫—Ü–∏–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã —Å –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ–º
        from .ace import _plan_subtasks as plan_subtasks, _answer_subpoint as answer_subpoint, _merge_subanswers as merge_subanswers  # type: ignore
    except Exception:
        plan_subtasks = None   # type: ignore
        answer_subpoint = None # type: ignore
        merge_subanswers = None # type: ignore

# ---------- polza client: –ø—Ä–æ–±—É–µ–º —Å—Ç—Ä–∏–º, —Ñ–æ–ª–±—ç–∫ –Ω–∞ –æ–±—ã—á–Ω—ã–π —á–∞—Ç ----------
try:
    from .polza_client import probe_embedding_dim, chat_with_gpt, chat_with_gpt_stream  # type: ignore
except Exception:
    from .polza_client import probe_embedding_dim, chat_with_gpt  # type: ignore
    chat_with_gpt_stream = None

# –ù–û–í–û–ï: –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –ø—Ä–∏—ë–º–∞/–æ–±–æ–≥–∞—â–µ–Ω–∏—è (OCR —Ç–∞–±–ª–∏—Ü-–∫–∞—Ä—Ç–∏–Ω–æ–∫, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–µ–ª)
from .ingest_orchestrator import enrich_sections
# –ù–û–í–û–ï: –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ç–∞–±–ª–∏—Ü
from .analytics import analyze_table_by_num

# —É—Ç–∏–ª–∏—Ç—ã
from .utils import safe_filename, sha256_bytes, split_for_telegram, infer_doc_kind

# –≥–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: —Å–µ–º–∞–Ω—Ç–∏–∫–∞ + FTS/LIKE
from .lexsearch import best_context

# —Å—Ä–∞–∑—É –ø–æ–¥ —Ç–µ–∫—É—â–∏–º–∏ import‚Äô–∞–º–∏
from .paywall_stub import setup_paywall

# –≥–¥–µ —É –≤–∞—Å —Å–æ–∑–¥–∞—é—Ç—Å—è –æ–±—ä–µ–∫—Ç—ã –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞:
bot = Bot(Cfg.TG_TOKEN)
dp = Dispatcher()
# –¥–æ–±–∞–≤—å—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É (–æ–¥–∏–Ω —Ä–∞–∑):
setup_paywall(dp, bot)


# --------------------- –ü–ê–†–ê–ú–ï–¢–†–´ –°–¢–†–ò–ú–ò–ù–ì–ê (—Å –¥–µ—Ñ–æ–ª—Ç–∞–º–∏) ---------------------

STREAM_ENABLED: bool = getattr(Cfg, "STREAM_ENABLED", True)
STREAM_EDIT_INTERVAL_MS: int = getattr(Cfg, "STREAM_EDIT_INTERVAL_MS", 900)  # –∫–∞–∫ —á–∞—Å—Ç–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ
STREAM_MIN_CHARS: int = getattr(Cfg, "STREAM_MIN_CHARS", 120)               # –º–∏–Ω. –ø—Ä–∏—Ä–∞—â–µ–Ω–∏–µ –º–µ–∂–¥—É –∞–ø–¥–µ–π—Ç–∞–º–∏
STREAM_MODE: str = getattr(Cfg, "STREAM_MODE", "edit")                       # "edit" | "multi"
TG_MAX_CHARS: int = getattr(Cfg, "TG_MAX_CHARS", 3900)

# ‚Üì –ù–æ–≤–æ–µ: —É–ø—Ä–∞–≤–ª—è–µ–º ¬´–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π¬ª –¥–∞–∂–µ –∫–æ–≥–¥–∞ –Ω–µ —É–ø–∏—Ä–∞–µ–º—Å—è –≤ 4096
TG_SPLIT_TARGET: int = getattr(Cfg, "TG_SPLIT_TARGET", 1600)   # —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏
TG_SPLIT_MAX_PARTS: int = getattr(Cfg, "TG_SPLIT_MAX_PARTS", 3)  # –Ω–µ –±–æ–ª—å—à–µ 3 —Å–æ–æ–±—â–µ–Ω–∏–π
_SPLIT_ANCHOR_RE = re.compile(
    r"(?m)^(?:### .+|## .+|\*\*[^\n]+?\*\*|\d+[).] .+|- .+)$"
)  # –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã (–∑–∞–≥–æ–ª–æ–≤–∫–∏/—Å–ø–∏—Å–∫–∏)
STREAM_HEAD_START_MS: int = getattr(Cfg, "STREAM_HEAD_START_MS", 250)        # –ø–µ—Ä–≤—ã–π –∞–ø–¥–µ–π—Ç –±—ã—Å—Ç—Ä–µ–µ
FINAL_MAX_TOKENS: int = getattr(Cfg, "FINAL_MAX_TOKENS", 1600)
TYPE_INDICATION_EVERY_MS: int = getattr(Cfg, "TYPE_INDICATION_EVERY_MS", 2000)

# ‚Üì –Ω–æ–≤–æ–µ: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–π –ø–æ–¥–∞—á–µ–π
MULTI_STEP_SEND_ENABLED: bool = getattr(Cfg, "MULTI_STEP_SEND_ENABLED", True)
MULTI_STEP_MIN_ITEMS: int = getattr(Cfg, "MULTI_STEP_MIN_ITEMS", 2)
MULTI_STEP_MAX_ITEMS: int = getattr(Cfg, "MULTI_STEP_MAX_ITEMS", 8)
MULTI_STEP_FINAL_MERGE: bool = getattr(Cfg, "MULTI_STEP_FINAL_MERGE", True)
MULTI_STEP_PAUSE_MS: int = getattr(Cfg, "MULTI_STEP_PAUSE_MS", 120)  # –º/—É –±–ª–æ–∫–∞–º–∏
MULTI_PASS_SCORE: int = getattr(Cfg, "MULTI_PASS_SCORE", 85)         # –ø–æ—Ä–æ–≥ –∫—Ä–∏—Ç–∏–∫–∞ –≤ ace

# --------------------- —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ ---------------------

# Markdown ‚Üí HTML (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ-–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ: **bold**, __bold__, *italic*, _italic_, `code`)
# --------------------- —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ ---------------------

# Markdown ‚Üí HTML (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ-–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ: –∑–∞–≥–æ–ª–æ–≤–∫–∏, **bold**, *italic*, `code`)
_MD_H_RE       = re.compile(r"(?m)^\s{0,3}#{1,6}\s+(.+?)\s*$")
_MD_BOLD_RE    = re.compile(r"\*\*(.+?)\*\*", re.DOTALL)
_MD_BOLD2_RE   = re.compile(r"__(.+?)__", re.DOTALL)
_MD_ITALIC_RE  = re.compile(r"(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)", re.DOTALL)
_MD_ITALIC2_RE = re.compile(r"(?<!_)_(?!_)(.+?)(?<!_)_(?!_)", re.DOTALL)
_MD_CODE_RE    = re.compile(r"`([^`]+)`")

def _to_html(text: str) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º HTML –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π Markdown –≤ —Ç–≥-HTML."""
    if not text:
        return ""
    # 1) —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º –≤—Å—ë
    txt = html.escape(text)

    # 2) –∫–æ–¥–æ–≤—ã–µ —Å–ø–∞–Ω—ã –ø–µ—Ä–≤—ã–º–∏
    txt = _MD_CODE_RE.sub(r"<code>\1</code>", txt)

    # 3) –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤–∏–¥–∞ '# ...' ‚Üí <b>...</b>
    txt = _MD_H_RE.sub(r"<b>\1</b>", txt)

    # 4) –∂–∏—Ä–Ω—ã–π –∏ –∫—É—Ä—Å–∏–≤
    txt = _MD_BOLD_RE.sub(r"<b>\1</b>", txt)
    txt = _MD_BOLD2_RE.sub(r"<b>\1</b>", txt)
    txt = _MD_ITALIC_RE.sub(r"<i>\1</i>", txt)
    txt = _MD_ITALIC2_RE.sub(r"<i>\1</i>", txt)

    # 5) –∑–∞—á–∏—Å—Ç–∫–∞ ¬´–≤–∏—Å—è—á–∏—Ö¬ª **
    txt = re.sub(r"(?<!\*)\*\*(?!\*)", "", txt)
    return txt


# -------- –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ --------
_GREET_RE = re.compile(
    r"(?i)\b(–ø—Ä–∏–≤–µ—Ç|–∑–¥—Ä–∞–≤—Å—Ç–≤|–¥–æ–±—Ä—ã–π\s*(–¥–µ–Ω—å|–≤–µ—á–µ—Ä|—É—Ç—Ä–æ)|hello|hi|hey|—Ö–∞–π|—Å–∞–ª—é—Ç|–∫—É)\b"
)

def _is_greeting(text: str) -> bool:
    t = (text or "").strip()
    if not t:
        return False
    # –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ —Ñ—Ä–∞–∑—ã, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
    return bool(_GREET_RE.search(t))


def _split_multipart(text: str,
                     *,
                     target: int = TG_SPLIT_TARGET,
                     max_parts: int = TG_SPLIT_MAX_PARTS,
                     hard: int = TG_MAX_CHARS) -> list[str]:
    """
    –î—Ä–æ–±–∏–º –æ—Ç–≤–µ—Ç –Ω–∞ 2‚Äì3 –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è:
    - —Å—Ç—Ä–µ–º–∏–º—Å—è –∫ target —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —á–∞—Å—Ç—å;
    - —Ä–µ–∂–µ–º –ø–æ —è–∫–æ—Ä—è–º (###/—Å–ø–∏—Å–∫–∏/–Ω—É–º–µ—Ä–∞—Ü–∏—è), –µ—Å–ª–∏ –µ—Å—Ç—å;
    - –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ–º hard (–ª–∏–º–∏—Ç Telegram).
    """
    s = text or ""
    if not s:
        return []
    if len(s) <= target:
        return [s]

    parts: list[str] = []
    rest = s

    for _ in range(max_parts - 1):
        if len(rest) <= target:
            break
        # –∏—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é ¬´–∫—Ä–∞—Å–∏–≤—É—é¬ª –≥—Ä–∞–Ω–∏—Ü—É –¥–æ target
        cut = -1
        for m in _SPLIT_ANCHOR_RE.finditer(rest[: min(len(rest), hard)]):
            if m.start() < target:
                cut = m.start()
        if cut <= 0:
            cut = _smart_cut_point(rest, min(hard, target))
        parts.append(rest[:cut].rstrip())
        rest = rest[cut:].lstrip()

    # –æ—Å—Ç–∞—Ç–æ–∫ –∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ, —Å–≤–µ—Ä—Ö–∂—ë—Å—Ç–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ hard
    while rest:
        parts.append(rest[:hard])
        rest = rest[hard:]

    return parts

async def _send(m: types.Message, text: str):
    """–ë–µ—Ä–µ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞—Å—Ç—è–º–∏ –≤ HTML-—Ä–µ–∂–∏–º–µ (–Ω–µ—Å—Ç—Ä–∏–º–æ–≤—ã–π —Ñ–æ–ª–±—ç–∫)."""
    for chunk in _split_multipart(text or ""):
        await m.answer(_to_html(chunk), parse_mode="HTML", disable_web_page_preview=True)

# --------------------- STREAM: –≤—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ---------------------

def _now_ms() -> int:
    return int(time.time() * 1000)

async def _typing_loop(chat_id: int, stop_event: asyncio.Event):
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä 'typing', –ø–æ–∫–∞ –Ω–µ –æ—Å—Ç–∞–Ω–æ–≤–∏–º."""
    try:
        while not stop_event.is_set():
            await bot.send_chat_action(chat_id, ChatAction.TYPING)
            await asyncio.wait_for(stop_event.wait(), timeout=TYPE_INDICATION_EVERY_MS / 1000)
    except asyncio.TimeoutError:
        pass
    except Exception:
        pass

def _ensure_iterable(stream_obj) -> Iterable[str]:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ (a)—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏—Ç–µ—Ä–∞—Ç–æ—Ä —Å—Ç—Ä–æ–∫; –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞ –ø—Ä–∏–ª–µ—Ç–µ–ª–∞ –∫–æ—Ä—É—Ç–∏–Ω–∞."""
    import inspect

    # –ï—Å–ª–∏ —ç—Ç–æ –∫–æ—Ä—É—Ç–∏–Ω–∞ ‚Äî –æ–±–µ—Ä–Ω—ë–º –≤ async-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∞—á–∞–ª–∞ –µ—ë await-–∏—Ç,
    # –∞ –ø–æ—Ç–æ–º —É–∂–µ –∏—Ç–µ—Ä–∏—Ä—É–µ—Ç—Å—è –ø–æ —Ä–µ–∞–ª—å–Ω–æ–º—É —Å—Ç—Ä–∏–º—É.
    if inspect.iscoroutine(stream_obj):
        async def _await_then_iter():
            real = await stream_obj
            if hasattr(real, "__aiter__"):
                async for chunk in real:
                    yield chunk
            else:
                for chunk in real:
                    yield chunk
        return _await_then_iter()

    if hasattr(stream_obj, "__aiter__"):
        async def _drain_to_queue(q: asyncio.Queue):
            try:
                async for chunk in stream_obj:  # type: ignore
                    await q.put(chunk or "")
            except Exception:
                pass
            finally:
                await q.put(None)

        queue: asyncio.Queue = asyncio.Queue()

        async def _producer():
            await _drain_to_queue(queue)

        asyncio.create_task(_producer())

        async def _async_iter():
            while True:
                item = await queue.get()
                if item is None:
                    break
                yield item
        return _async_iter()

    return stream_obj

async def _iterate_chunks(stream_obj) -> AsyncIterable[str]:
    """–ï–¥–∏–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —á–∞–Ω–∫–æ–≤ (—É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∏ —Å sync-, –∏ —Å async-–∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞–º–∏)."""
    if hasattr(stream_obj, "__aiter__"):
        async for ch in stream_obj:
            if ch:
                yield str(ch)
        return
    for ch in stream_obj:
        if ch:
            yield str(ch)

def _smart_cut_point(s: str, limit: int) -> int:
    """–ò—â–µ–º ¬´–∫—Ä–∞—Å–∏–≤–æ–µ¬ª –º–µ—Å—Ç–æ —Ä–∞–∑—Ä–µ–∑–∞ <= limit (–ø–æ –ø–µ—Ä–µ–Ω–æ—Å—É/—Ç–æ—á–∫–µ/–ø—Ä–æ–±–µ–ª—É)."""
    if len(s) <= limit:
        return len(s)
    cut = s.rfind("\n", 0, limit)
    if cut == -1:
        cut = s.rfind(". ", 0, limit)
    if cut == -1:
        cut = s.rfind(" ", 0, limit)
    if cut == -1:
        cut = limit
    return max(1, cut)

async def _stream_to_telegram(m: types.Message, stream, head_text: str = "‚åõÔ∏è –ü–µ—á–∞—Ç–∞—é –æ—Ç–≤–µ—Ç‚Ä¶") -> None:
    current_text = ""
    sent_parts = 0
    initial = await m.answer(_to_html(head_text), parse_mode="HTML", disable_web_page_preview=True)
    last_edit_at = _now_ms() - STREAM_HEAD_START_MS
    stop_typer = asyncio.Event()
    typer_task = asyncio.create_task(_typing_loop(m.chat.id, stop_event=stop_typer))

    # üîß –Ω–æ–≤–æ–µ: –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –≤ multi –±–æ–ª—å—à–µ –Ω–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º initial
    freeze_initial = False

    try:
        async for delta in _iterate_chunks(_ensure_iterable(stream)):
            current_text += delta

            # 3.a) –º—É–ª—å—Ç–∏-—Ä–µ–∂–∏–º: —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–æ—Ä—Ü–∏—è–º–∏
            if STREAM_MODE == "multi" and sent_parts < TG_SPLIT_MAX_PARTS - 1 and len(current_text) >= TG_SPLIT_TARGET:
                cut = -1
                for mm in _SPLIT_ANCHOR_RE.finditer(current_text[: min(len(current_text), TG_MAX_CHARS)]):
                    if mm.start() < TG_SPLIT_TARGET:
                        cut = mm.start()
                if cut <= 0:
                    cut = _smart_cut_point(current_text, min(TG_MAX_CHARS, TG_SPLIT_TARGET))

                part = current_text[:cut].rstrip()
                try:
                    if sent_parts == 0:
                        await initial.edit_text(_to_html(part), parse_mode="HTML", disable_web_page_preview=True)
                        freeze_initial = True  # <- –±–æ–ª—å—à–µ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º initial
                    else:
                        await m.answer(_to_html(part), parse_mode="HTML", disable_web_page_preview=True)
                except TelegramBadRequest:
                    await m.answer(_to_html(part), parse_mode="HTML", disable_web_page_preview=True)

                sent_parts += 1
                current_text = current_text[cut:].lstrip()
                last_edit_at = _now_ms()
                continue

            # 3.b) –∑–∞—â–∏—Ç–∞ –æ—Ç –ª–∏–º–∏—Ç–∞
            if len(current_text) >= TG_MAX_CHARS:
                cut = _smart_cut_point(current_text, TG_MAX_CHARS)
                final_part = current_text[:cut]

                if STREAM_MODE == "multi" and (freeze_initial or sent_parts > 0):
                    # üîß –≤ multi –Ω–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º initial –ø–æ—Å–ª–µ 1-–π —á–∞—Å—Ç–∏
                    await m.answer(_to_html(final_part), parse_mode="HTML", disable_web_page_preview=True)
                else:
                    try:
                        await initial.edit_text(_to_html(final_part), parse_mode="HTML", disable_web_page_preview=True)
                    except TelegramBadRequest:
                        await m.answer(_to_html(final_part), parse_mode="HTML", disable_web_page_preview=True)

                current_text = current_text[cut:].lstrip()
                # üîß –Ω–æ–≤—ã–π –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –≤ edit-—Ä–µ–∂–∏–º–µ
                if STREAM_MODE == "edit":
                    initial = await m.answer(_to_html("‚Ä¶"), parse_mode="HTML", disable_web_page_preview=True)
                last_edit_at = _now_ms()
                continue

            # 3.c) –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∫–∏ ‚Äî üîß –¢–û–õ–¨–ö–û –≤ —Ä–µ–∂–∏–º–µ edit
            now = _now_ms()
            if STREAM_MODE == "edit" and (now - last_edit_at) >= STREAM_EDIT_INTERVAL_MS and len(current_text) >= STREAM_MIN_CHARS:
                try:
                    await initial.edit_text(_to_html(current_text), parse_mode="HTML", disable_web_page_preview=True)
                    last_edit_at = now
                except TelegramBadRequest:
                    pass

        # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ö–≤–æ—Å—Ç
        if current_text:
            try:
                if STREAM_MODE == "multi" and sent_parts > 0:
                    await m.answer(_to_html(current_text), parse_mode="HTML", disable_web_page_preview=True)
                else:
                    await initial.edit_text(_to_html(current_text), parse_mode="HTML", disable_web_page_preview=True)
            except TelegramBadRequest:
                await m.answer(_to_html(current_text), parse_mode="HTML", disable_web_page_preview=True)

    finally:
        stop_typer.set()
        try:
            await typer_task
        except Exception:
            pass


async def _run_multistep_answer(
    m: types.Message,
    uid: int,
    doc_id: int,
    q_text: str,
    *,
    discovered_items: list[dict] | None = None,
) -> bool:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º: –ø–ª–∞–Ω ‚Üí –ø–æ –∫–∞–∂–¥–æ–º—É –ø–æ–¥–ø—É–Ω–∫—Ç—É –æ—Ç–¥–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç ‚Üí (–æ–ø—Ü.) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π merge.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –ø—É—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –Ω–∏—á–µ–≥–æ –¥–∞–ª—å—à–µ –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ.
    """
    if not MULTI_STEP_SEND_ENABLED:
        return False
    if not (plan_subtasks and answer_subpoint and merge_subanswers):
        # –Ω–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ ace ‚Äî –≤—ã—Ö–æ–¥–∏–º
        return False

    # –ø–ª–∞–Ω –∏–∑ coverage –∏–ª–∏ —Å—Ç—Ä–æ–∏–º –ø–ª–∞–Ω–µ—Äo–º
            # –ø–ª–∞–Ω –∏–∑ coverage –∏–ª–∏ —Å—Ç—Ä–æ–∏–º –ø–ª–∞–Ω–µ—Ä–æ–º
        items = (discovered_items or [])
    if not items:
        try:
            items = plan_subtasks(q_text) or []
        except Exception:
            items = []

    # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∏ dict, –∏ str
    norm_items: list[dict] = []
    for idx, it in enumerate(items, start=1):
        if isinstance(it, str):
            norm_items.append({"id": idx, "ask": it.strip()})
        elif isinstance(it, dict):
            ask = (it.get("ask") or it.get("text") or it.get("q") or "").strip()
            if ask:
                norm_items.append({"id": it.get("id") or idx, "ask": ask})
    items = [it for it in norm_items if (it.get("ask") or "").strip()]
    if len(items) < MULTI_STEP_MIN_ITEMS:
        return False

    # –æ—Ç—Å–µ—á—ë–º —Ö–≤–æ—Å—Ç –ø–æ –ª–∏–º–∏—Ç—É
    items = items[:MULTI_STEP_MAX_ITEMS]

    # –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–æ–Ω—Å
    preview = "\n".join([f"{i+1}) {(it['ask'] or '').strip()}" for i, it in enumerate(items)])
    await _send(m, f"–í–æ–ø—Ä–æ—Å –º–Ω–æ–≥–æ—á–∞—Å—Ç–Ω—ã–π. –û—Ç–≤–µ—á–∞—é –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º ({len(items)} —à—Ç.):\n\n{preview}")

    subanswers: list[str] = []

    # coverage-aware —Ä–∞–∑–¥–∞—á–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: —Ä–∞–∑–ª–æ–∂–∏–º –≤—ã–∂–∏–º–∫–∏ –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º
    cov = None
    try:
        cov = retrieve_coverage(owner_id=uid, doc_id=doc_id, question=q_text)
    except Exception:
        cov = None
    cov_snips = (cov or {}).get("snippets") or []
    cov_map = (cov or {}).get("by_item") or {}  # { "1": [—á–∞–Ω–∫–∏], "2": [—á–∞–Ω–∫–∏], ... }

    # –ø–æ –æ—á–µ—Ä–µ–¥–∏: A ‚Üí send, B ‚Üí send, ...
    for i, it in enumerate(items, start=1):
        ask = (it.get("ask") or "").strip()
        # –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞
                # –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞
        ctx_text = ""
        try:
            # –µ—Å–ª–∏ –µ—Å—Ç—å coverage-–±–∞–∫–µ—Ç ‚Äî —Å–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä—è–º–æ –∏–∑ —á–∞–Ω–∫–æ–≤ –ø–æ–¥–ø—É–Ω–∫—Ç–∞
            bucket = cov_map.get(str(it.get("id") or i)) or []
            if bucket:
                ctx_text = build_context_coverage(bucket, items_count=1)
        except Exception:
            ctx_text = ""
        # 2) —Ñ–æ–ª–±—ç–∫–∏
        if not ctx_text:
            ctx_text = best_context(uid, doc_id, ask, max_chars=6000) or ""
        if not ctx_text:
            hits = retrieve(uid, doc_id, ask, top_k=8)
            if hits:
                ctx_text = build_context(hits)
        if not ctx_text:
            ctx_text = _first_chunks_context(uid, doc_id, n=12, max_chars=6000)

        # –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç—É (–∫–∞—Å—Ç–æ–º–Ω–∞—è –ø–æ–¥—Å–∫–∞–∑–∫–∞ –≤ ace + –∫—Ä–∏—Ç–∏–∫–∞/–ø—Ä–∞–≤–∫–∞)
        try:
            part = answer_subpoint(ask, ctx_text, MULTI_PASS_SCORE).strip()
        except Exception as e:
            logging.exception("answer_subpoint failed: %s", e)
            part = ""

        # –æ—Ç–ø—Ä–∞–≤–∫–∞ –±–ª–æ–∫–∞
        header = f"<b>{i}. {html.escape(ask)}</b>\n\n"
        await _send(m, header + (part or "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –ø–æ —ç—Ç–æ–º—É –ø–æ–¥–ø—É–Ω–∫—Ç—É."))
        subanswers.append(f"{header}{part}")

        # –º–∏–∫—Ä–æ–ø–∞—É–∑a, —á—Ç–æ–±—ã –Ω–µ —É–ø–µ—Ä–µ—Ç—å—Å—è –≤ rate/—á–∞—Ç—ã
        await asyncio.sleep(MULTI_STEP_PAUSE_MS / 1000)

    # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –±–ª–æ–∫
    if MULTI_STEP_FINAL_MERGE:
        try:
            merged = merge_subanswers(q_text, items, subanswers).strip()
            if merged:
                await _send(m, "<b>–ò—Ç–æ–≥–æ–≤—ã–π —Å–≤–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç</b>\n\n" + merged)
        except Exception as e:
            logging.exception("merge_subanswers failed: %s", e)

    return True

# summarizer (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç)
try:
    from .summarizer import is_summary_intent, overview_context  # –º–æ–≥—É—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å ‚Äî –µ—Å—Ç—å —Ñ–æ–ª–±—ç–∫–∏ –Ω–∏–∂–µ
except Exception:
    def is_summary_intent(text: str) -> bool:
        return bool(re.search(r"\b(—Å—É—Ç—å|–∫—Ä–∞—Ç–∫–æ|–æ—Å–Ω–æ–≤–Ω|–≥–ª–∞–≤–Ω|summary|overview|–∏—Ç–æ–≥|–≤—ã–≤–æ–¥)\w*\b",
                              text or "", re.IGNORECASE))

    def overview_context(owner_id: int, doc_id: int, max_chars: int = 6000) -> str:
        con = get_conn()
        cur = con.cursor()
        cur.execute(
            """
            SELECT page, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
              AND (text LIKE '[–ó–∞–≥–æ–ª–æ–≤–æ–∫]%%'
                   OR text LIKE '%%–¶–µ–ª—å%%'
                   OR text LIKE '%%–ó–∞–¥–∞—á%%'
                   OR text LIKE '%%–í–≤–µ–¥–µ–Ω%%'
                   OR text LIKE '%%–ó–∞–∫–ª—é—á–µ–Ω%%'
                   OR text LIKE '%%–í—ã–≤–æ–¥%%')
            ORDER BY page ASC, id ASC
            LIMIT 14
            """,
            (owner_id, doc_id),
        )
        rows = cur.fetchall()
        con.close()
        if not rows:
            return ""
        parts, total = [], 0
        for r in rows:
            block = f"{(r['text'] or '').strip()}"
            if total + len(block) > max_chars:
                break
            parts.append(block)
            total += len(block)
        return "\n\n".join(parts)

# vision-–æ–ø–∏—Å–∞–Ω–∏–µ —Ä–∏—Å—É–Ω–∫–æ–≤ (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –æ—Ç–≤–µ—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ñ–æ–ª–±—ç–∫–æ–º)
try:
    from .summarizer import describe_figures as vision_describe_figures
except Exception:
    def vision_describe_figures(owner_id: int, doc_id: int, numbers: list[str]) -> str:
        if not numbers:
            return "–ù–µ —É–∫–∞–∑–∞–Ω—ã –Ω–æ–º–µ—Ä–∞ —Ä–∏—Å—É–Ω–∫–æ–≤."
        return "–û–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (vision-–º–æ–¥—É–ª—å –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω)."

# –ì–û–°–¢-–≤–∞–ª–∏–¥–∞—Ç–æ—Ä (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç)
try:
    from .validators_gost import validate_gost, render_report
except Exception:
    validate_gost = None
    render_report = None


# ¬´–ê–∫—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç¬ª –≤ –ø–∞–º—è—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
ACTIVE_DOC: dict[int, int] = {}  # user_id -> doc_id


# ------------------------ –ì–∞—Ä–¥—Ä–µ–π–ª—ã ------------------------

_BANNED_PATTERNS = [
    r"jail ?break|system\s*prompt|developer\s*mode|dan\b|ignore (all|previous) (rules|instructions)",
    r"\b–≤–∑–ª–æ–º|—Ö–∞–∫–∏?|–∫–µ–π–≥–µ–Ω|–∫—Ä—è–∫|—Å–æ—Ü–∏–∞–ª—å–Ω(–∞—è|—ã–µ) –∏–Ω–∂–µ–Ω–µ—Ä–∏—è",
    r"\b–≤–∏—Ä—É—Å|–≤—Ä–µ–¥–æ–Ω–æ—Å|—ç–∫—Å–ø–ª–æ–π—Ç|–±–æ—Ç–Ω–µ—Ç|ddos\b",
    r"\b–æ—Ä—É–∂–∏|–≤–∑—Ä—ã–≤—á–∞—Ç|–±–æ–º–±|–Ω–∞—Ä–∫–æ—Ç|–ø–æ—Ä–Ω–æ|—ç—Ä–æ—Ç–∏–∫|18\+",
    r"\b–ø–∞—Å–ø–æ—Ä—Ç|—Å–Ω–∏–ª—Å|–∏–Ω–Ω\b.*(—Å–≥–µ–Ω–µ—Ä|–ø–æ–¥–¥–µ–ª)",
    r"\b–æ–±–æ–π(–¥|—Ç–∏)\b.*–∞–Ω—Ç–∏–ø–ª–∞–≥|–∞–Ω—Ç–∏–ø–ª–∞–≥–∏–∞—Ç|antiplagiat",
    r"sql.?–∏–Ω—ä–µ–∫|–∏–Ω—ä–µ–∫—Ü–∏(—è|–∏) sql",
]

def safety_check(text: str) -> str | None:
    t = (text or "").lower()
    for p in _BANNED_PATTERNS:
        if re.search(p, t, flags=re.IGNORECASE):
            return ("–ó–∞–ø—Ä–æ—Å –Ω–∞—Ä—É—à–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ "
                    "(–≤–∑–ª–æ–º/–≤—Ä–µ–¥–æ–Ω–æ—Å/–æ–±—Ö–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π/NSFW/–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ).")
    return None

_ALLOWED_HINT_WORDS = [
    "–≤–∫—Ä", "–¥–∏–ø–ª–æ–º", "–∫—É—Ä—Å–æ–≤", "–º–µ—Ç–æ–¥–æ–ª–æ–≥", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–ª–∏—Ç–æ–±–∑–æ—Ä",
    "–≥–∏–ø–æ—Ç–µ–∑", "—Ü–µ–ª—å", "–∑–∞–¥–∞—á", "–≤–≤–µ–¥–µ–Ω–∏–µ", "–∑–∞–∫–ª—é—á–µ–Ω", "–æ–±–∑–æ—Ä",
    "–æ—Ñ–æ—Ä–º–ª–µ–Ω", "–≥–æ—Å—Ç", "—Ç–∞–±–ª–∏—Ü", "—Ä–∏—Å—É–Ω–∫", "–∞–Ω—Ç–∏–ø–ª–∞–≥", "–ø–ª–∞–≥–∏–∞—Ç",
    "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü", "–∑–∞—â–∏—Ç—É", "–æ–ø—Ä–æ—Å", "–∞–Ω–∫–µ—Ç–∞", "–º–µ—Ç–æ–¥—ã", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫",
]

def topical_check(text: str) -> str | None:
    """
    –ú—è–≥–∫–æ–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–õ–¨–ö–û –∫–∞–∫ –ø–æ–¥—Å–∫–∞–∑–∫—É,
    –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    """
    t = (text or "").lower()
    if not any(w in t for w in _ALLOWED_HINT_WORDS):
        return ("–ü–æ–¥—Å–∫–∞–∑–∫–∞: —Å–∏–ª—å–Ω–µ–µ –≤—Å–µ–≥–æ –æ—Ç–≤–µ—á–∞—é –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –í–ö–† (–≥–ª–∞–≤—ã, —Ç–∞–±–ª–∏—Ü—ã, —Ä–∏—Å—É–Ω–∫–∏, –≤—ã–≤–æ–¥—ã). "
                "–ï—Å–ª–∏ –ø—Ä–∏—à–ª—ë—Ç–µ —Ñ–∞–π–ª –¥–∏–ø–ª–æ–º–∞ ‚Äî —Å–º–æ–≥—É –æ–±—ä—è—Å–Ω—è—Ç—å –ø—Ä—è–º–æ –ø–æ –≤–∞—à–µ–º—É —Ç–µ–∫—Å—Ç—É.")
    return None


# --------------------- –ë–î / —É—Ç–∏–ª–∏—Ç—ã ---------------------

def _table_has_columns(con, table: str, cols: list[str]) -> bool:
    cur = con.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    have = {row[1] for row in cur.fetchall()}
    return all(c in have for c in cols)

def _find_existing_doc(con, owner_id: int, sha256: str | None, file_uid: str | None):
    """–ü–æ–∏—Å–∫ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ sha/file_unique_id (–µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å)."""
    if not _table_has_columns(con, "documents", ["content_sha256", "file_uid"]):
        return None
    cur = con.cursor()
    if sha256 and file_uid:
        cur.execute(
            "SELECT id FROM documents WHERE owner_id=? AND (content_sha256=? OR file_uid=?) "
            "ORDER BY id DESC LIMIT 1",
            (owner_id, sha256, file_uid),
        )
    elif sha256:
        cur.execute(
            "SELECT id FROM documents WHERE owner_id=? AND content_sha256=? "
            "ORDER BY id DESC LIMIT 1",
            (owner_id, sha256),
        )
    elif file_uid:
        cur.execute(
            "SELECT id FROM documents WHERE owner_id=? AND file_uid=? "
            "ORDER BY id DESC LIMIT 1",
            (owner_id, file_uid),
        )
    row = cur.fetchone()
    return (row["id"] if row else None)

def _insert_document(con, owner_id: int, kind: str, path: str,
                     sha256: str | None, file_uid: str | None) -> int:
    cur = con.cursor()
    if _table_has_columns(con, "documents", ["content_sha256", "file_uid"]):
        cur.execute(
            "INSERT INTO documents(owner_id, kind, path, content_sha256, file_uid) VALUES(?,?,?,?,?)",
            (owner_id, kind, path, sha256, file_uid),
        )
    else:
        cur.execute(
            "INSERT INTO documents(owner_id, kind, path) VALUES(?,?,?)",
            (owner_id, kind, path),
        )
    doc_id = cur.lastrowid
    con.commit()
    return doc_id


# --------------------- –¢–∞–±–ª–∏—Ü—ã: –ø–∞—Ä—Å–∏–Ω–≥/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ---------------------

_TABLE_ANY = re.compile(r"\b—Ç–∞–±–ª–∏—Ü\w*|\b—Ç–∞–±–ª\.\b|\b—Ç–∞–±–ª–∏—Ü–∞\w*|(?:^|\s)table(s)?\b", re.IGNORECASE)
# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º: 2.1, 3, A.1, –ê.1, –ü1.2
_TABLE_TITLE_RE = re.compile(r"(?i)\b—Ç–∞–±–ª–∏—Ü–∞\s+(\d+(?:[.,]\d+)*|[a-z–∞-—è]\.?\s*\d+(?:[.,]\d+)*)\b(?:\s*[‚Äî\-‚Äì]\s*(.+))?")
_COUNT_HINT = re.compile(r"\b—Å–∫–æ–ª—å–∫–æ\b|how many", re.IGNORECASE)
_WHICH_HINT = re.compile(r"\b–∫–∞–∫–∏(–µ|—Ö)\b|\b—Å–ø–∏—Å–æ–∫\b|\b–ø–µ—Ä–µ—á–∏—Å–ª\w*\b|\b–Ω–∞–∑–æ–≤\w*\b", re.IGNORECASE)

def _plural_tables(n: int) -> str:
    n_abs = abs(n) % 100
    n1 = n_abs % 10
    if 11 <= n_abs <= 14:
        return "—Ç–∞–±–ª–∏—Ü"
    if n1 == 1:
        return "—Ç–∞–±–ª–∏—Ü–∞"
    if 2 <= n1 <= 4:
        return "—Ç–∞–±–ª–∏—Ü—ã"
    return "—Ç–∞–±–ª–∏—Ü"

def _strip_table_prefix(s: str) -> str:
    return re.sub(r"^\[\s*—Ç–∞–±–ª–∏—Ü–∞\s*\]\s*", "", s or "", flags=re.IGNORECASE)

def _last_segment(name: str) -> str:
    s = (name or "").strip()
    if "/" in s:
        s = s.split("/")[-1].strip()
    s = _strip_table_prefix(s)
    s = re.sub(r"\s*[-‚Äì‚Äî]\s*", " ‚Äî ", s)
    s = re.sub(r"\s+", " ", s).strip(" .")
    return s

def _parse_table_title(text: str) -> tuple[str | None, str | None]:
    t = (text or "").strip()
    m = _TABLE_TITLE_RE.search(t)
    if not m:
        return (None, None)
    num = (m.group(1) or "").strip() or None
    title = (m.group(2) or "").strip() or None
    return (num, title)

def _shorten(s: str, limit: int = 120) -> str:
    s = (s or "").strip()
    if len(s) <= limit:
        return s
    return s[:limit - 1].rstrip() + "‚Ä¶"


# -------- –¢–∞–±–ª–∏—Ü—ã: –ø–æ–¥—Å—á—ë—Ç –∏ —Å–ø–∏—Å–æ–∫ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –ë–î) --------

def _distinct_table_basenames(uid: int, doc_id: int) -> list[str]:
    """
    –°–æ–±–∏—Ä–∞–µ–º ¬´–±–∞–∑–æ–≤—ã–µ¬ª –∏–º–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü (section_path –±–µ–∑ —Ö–≤–æ—Å—Ç–∞ ' [row ‚Ä¶]').
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ —Å –Ω–æ–≤—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ (table_row) –∏ —Å–æ —Å—Ç–∞—Ä—ã–º–∏.
    """
    con = get_conn()
    cur = con.cursor()

    # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–ø–µ—Ä–µ—Ç—å—Å—è –Ω–∞ —Ç–∏–ø—ã
    if _table_has_columns(con, "chunks", ["element_type"]):
        cur.execute(
            """
            SELECT DISTINCT
                CASE
                    WHEN instr(section_path, ' [row ')>0
                        THEN substr(section_path, 1, instr(section_path,' [row ')-1)
                    ELSE section_path
                END AS base_name
            FROM chunks
            WHERE doc_id=? AND owner_id=? AND element_type IN ('table','table_row')
            """,
            (doc_id, uid),
        )
    else:
        # –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å ‚Äî —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        cur.execute(
            """
            SELECT DISTINCT
                CASE
                    WHEN instr(section_path, ' [row ')>0
                        THEN substr(section_path, 1, instr(section_path,' [row ')-1)
                    ELSE section_path
                END AS base_name
            FROM chunks
            WHERE doc_id=? AND owner_id=? AND (
                  lower(section_path) LIKE '%—Ç–∞–±–ª–∏—Ü–∞%'
               OR lower(text)        LIKE '%—Ç–∞–±–ª–∏—Ü–∞%'
               OR section_path LIKE '–¢–∞–±–ª–∏—Ü–∞ %' COLLATE NOCASE
               OR text        LIKE '[–¢–∞–±–ª–∏—Ü–∞]%' COLLATE NOCASE
               OR lower(section_path) LIKE '%table %'
               OR lower(text)        LIKE '%table %'
            )
            """,
            (doc_id, uid),
        )

    base_items = [r["base_name"] for r in cur.fetchall() if r["base_name"]]
    con.close()
    base_items = sorted(set(base_items), key=lambda s: s.lower())
    return base_items

def _count_tables(uid: int, doc_id: int) -> int:
    return len(_distinct_table_basenames(uid, doc_id))

def _compose_display_from_attrs(attrs_json: str | None, base: str, first_row_text: str | None) -> str:
    """
    –ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è:
      1) –µ—Å—Ç—å caption_num ‚Üí '–¢–∞–±–ª–∏—Ü–∞ N ‚Äî tail/header/firstrow' (–≤—Å–µ–≥–¥–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å).
      2) –Ω–µ—Ç –Ω–æ–º–µ—Ä–∞ ‚Üí –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ: caption_tail/ header_preview/ first_row.
      3) —Ñ–æ–ª–±—ç–∫: –ø–∞—Ä—Å–∏–º –Ω–æ–º–µ—Ä –∏ —Ö–≤–æ—Å—Ç –∏–∑ base ('–¢–∞–±–ª–∏—Ü–∞ N ‚Äî ...') –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å –Ω–æ–º–µ—Ä–æ–º.
      4) –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã—à–ª–æ ‚Äî –±–µ—Ä—ë–º –∫–æ—Ä–æ—Ç–∫–∏–π base –±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤.
    """
    num = None
    tail = None
    header_preview = None
    if attrs_json:
        try:
            a = json.loads(attrs_json or "{}")
            num = a.get("caption_num") or a.get("label")
            tail = a.get("caption_tail") or a.get("title")
            header_preview = a.get("header_preview")
        except Exception:
            pass

    if num:
        num = str(num).replace(",", ".").strip()
        tail_like = (tail or header_preview or first_row_text or "").strip()
        return f"–¢–∞–±–ª–∏—Ü–∞ {num}" + (f" ‚Äî {_shorten(tail_like, 160)}" if tail_like else "")

    # –±–µ–∑ –Ω–æ–º–µ—Ä–∞ –≤ attrs ‚Äî –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∏–∑ base –∏ –ø–æ–∫–∞–∑–∞—Ç—å –° –Ω–æ–º–µ—Ä–æ–º
    num_b, title_b = _parse_table_title(_last_segment(base))
    if num_b:
        text_tail = title_b or first_row_text or header_preview
        return f"–¢–∞–±–ª–∏—Ü–∞ {num_b}" + (f" ‚Äî {_shorten(text_tail, 160)}" if text_tail else "")

    # –±–µ–∑ –Ω–æ–º–µ—Ä–∞ ‚Äî —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ
    if tail:
        return _shorten(str(tail), 160)
    if header_preview:
        return _shorten(str(header_preview), 160)
    if first_row_text:
        return _shorten(first_row_text, 160)

    s = _last_segment(base)
    s = re.sub(r"(?i)^\s*—Ç–∞–±–ª–∏—Ü–∞\s+\d+(?:\.\d+)*\s*", "", s).strip(" ‚Äî‚Äì-")
    return _shorten(s or "–¢–∞–±–ª–∏—Ü–∞", 160)


# ------------------------------ –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ------------------------------

_SOURCES_HINT = re.compile(
    r"\b(–∏—Å—Ç–æ—á–Ω–∏–∫(?:–∏|–æ–≤)?|—Å–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã|—Å–ø–∏—Å–æ–∫\s+–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤|–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ\w*|references?|bibliograph\w*)\b",
    re.IGNORECASE
)
_REF_LINE_RE = re.compile(r"^\s*(?:\[\d+\]|\d+[.)])\s+.+", re.MULTILINE)

def _count_sources(uid: int, doc_id: int) -> int:
    """
    –ü–æ–¥—Å—á—ë—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
      1) –µ—Å–ª–∏ –≤ –ë–î –µ—Å—Ç—å element_type='reference' ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ;
      2) –∏–Ω–∞—á–µ —Å–æ–±–∏—Ä–∞–µ–º –ª—é–±—ã–µ –Ω–µ–ø—É—Å—Ç—ã–µ –∞–±–∑–∞—Ü—ã –≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–π ¬´–∏—Å—Ç–æ—á–Ω–∏–∫–∏/–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞/‚Ä¶¬ª
         (–±–µ–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–ª–∞—Å—å —Å –Ω–æ–º–µ—Ä–∞).
    """
    con = get_conn()
    cur = con.cursor()
    has_type = _table_has_columns(con, "chunks", ["element_type"])

    total = 0
    if has_type:
        cur.execute(
            "SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=? AND element_type='reference'",
            (uid, doc_id),
        )
        row = cur.fetchone()
        total = int(row["c"] or 0)

    if total == 0:
        items = set()
        cur.execute(
            """
            SELECT element_type, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
            ORDER BY page ASC, id ASC
            """,
            (uid, doc_id),
        )
        raw_rows = cur.fetchall()
        for r in raw_rows:
            sec = (r["section_path"] or "").lower()
            if not any(k in sec for k in ("–∏—Å—Ç–æ—á–Ω–∏–∫", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ", "reference", "bibliograph")):
                continue
            et = (r["element_type"] or "").lower()
            if et in ("heading", "table", "figure", "table_row"):
                continue
            t = (r["text"] or "").strip()
            if not t:
                continue
            k = re.sub(r"\s+", " ", t).strip().rstrip(".").lower()
            if len(k) >= 5:
                items.add(k)
        total = len(items)

    con.close()
    return total


# --------- –±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç: –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏ ---------
_PRACTICAL_Q = re.compile(r"(–µ—Å—Ç—å –ª–∏|–Ω–∞–ª–∏—á–∏–µ|–ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ª–∏|–∏–º–µ–µ—Ç—Å—è –ª–∏).{0,40}–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫", re.IGNORECASE)

def _has_practical_part(uid: int, doc_id: int) -> bool:
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        """
        SELECT 1
        FROM chunks
        WHERE owner_id=? AND doc_id=? AND (
            lower(section_path) LIKE '%–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫%' OR
            lower(text)         LIKE '%–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫%'
        )
        LIMIT 1
        """,
        (uid, doc_id),
    )
    row = cur.fetchone()
    con.close()
    return row is not None


# ------------- –ì–û–°–¢-–∏–Ω—Ç–µ–Ω—Ç –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ -------------

_GOST_HINT = re.compile(r"\b(–≥–æ—Å—Ç|–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏|—à—Ä–∏—Ñ—Ç|–º–µ–∂—Å—Ç—Ä–æ—á|–∫–µ–≥–ª|–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏|–ø–æ–ª—è|–æ—Ñ–æ—Ä–º–∏—Ç—å)\w*\b", re.IGNORECASE)

async def _maybe_run_gost(m: types.Message, uid: int, doc_id: int, text: str) -> bool:
    """–ï—Å–ª–∏ –ø–æ—Ö–æ–∂–µ, —á—Ç–æ –ø—Ä–æ—Å—è—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –ì–û–°–¢. –í–æ–∑–≤—Ä–∞—â–∞–µ–º True, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç–∏–ª–∏."""
    if not validate_gost or not render_report:
        return False
    if not _GOST_HINT.search(text or ""):
        return False

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    if not row:
        return False

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
    except Exception:
        return False

    report = validate_gost(sections)
    text_rep = render_report(report, max_issues=25)
    await _send(m, text_rep)
    return True

# ------------------------------ helpers ------------------------------

def _parse_by_ext(path: str) -> list[dict]:
    fname = (os.path.basename(path) or "").lower()
    if fname.endswith(".docx"):
        return parse_docx(path)
    if fname.endswith(".doc"):
        return parse_doc(path)
    raise RuntimeError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é —Ç–æ–ª—å–∫–æ .doc –∏ .docx.")

def _first_chunks_context(owner_id: int, doc_id: int, n: int = 10, max_chars: int = 6000) -> str:
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT page, section_path, text FROM chunks "
        "WHERE owner_id=? AND doc_id=? "
        "ORDER BY page ASC, id ASC LIMIT ?",
        (owner_id, doc_id, n)
    )
    rows = cur.fetchall()
    con.close()
    if not rows:
        return ""
    parts, total = [], 0
    for r in rows:
        block = f"{(r['text'] or '').strip()}"
        if total + len(block) > max_chars:
            break
        parts.append(block)
        total += len(block)
    return "\n\n".join(parts)

# ---------- verbatim fallback –ø–æ —Ü–∏—Ç–∞—Ç–µ (—à–∏–Ω–≥–ª—ã + LIKE/NOCASE) ----------

def _normalize_for_like(s: str) -> str:
    s = (s or "")
    s = s.replace("\u00A0", " ")  # NBSP -> –ø—Ä–æ–±–µ–ª
    s = s.replace("¬´", '"').replace("¬ª", '"').replace("‚Äú", '"').replace("‚Äù", '"')
    s = s.replace("‚Äô", "'").replace("‚Äò", "'")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _make_shingles(s: str, min_len: int = 30, max_len: int = 90, step: int = 25) -> list[str]:
    s = _normalize_for_like(s)
    if not s:
        return []
    if len(s) <= max_len:
        return [s]
    out = []
    i = 0
    while i < len(s):
        chunk = s[i:i + max_len]
        if len(chunk) >= min_len:
            out.append(chunk)
        i += step
    return out[:6]

def verbatim_find(owner_id: int, doc_id: int, q_text: str, max_hits: int = 3) -> list[dict]:
    shingles = _make_shingles(q_text)
    if not shingles:
        return []
    con = get_conn()
    cur = con.cursor()
    hits: list[dict] = []
    for sh in shingles:
        pattern = f"%{sh}%"
        cur.execute(
            """
            SELECT page, section_path, text FROM chunks
            WHERE owner_id=? AND doc_id=? AND
                  replace(text, char(160), ' ') LIKE ? COLLATE NOCASE
            ORDER BY page ASC, id ASC
            LIMIT ?
            """,
            (owner_id, doc_id, pattern, max_hits - len(hits)),
        )
        for r in cur.fetchall():
            t = (r["text"] or "")
            t_norm = _normalize_for_like(t)
            pos = t_norm.lower().find(_normalize_for_like(sh).lower())
            if pos >= 0:
                s = max(pos - 120, 0)
                e = min(pos + len(sh) + 120, len(t_norm))
                hits.append({
                    "page": r["page"],
                    "section_path": r["section_path"],
                    "snippet": t_norm[s:e].strip(),
                })
            if len(hits) >= max_hits:
                con.close()
                return hits
    con.close()
    return hits


# ------------------------------ /start ------------------------------

@dp.message(Command("start"))
async def start(m: types.Message):
    ensure_user(str(m.from_user.id))
    await _send(m,
        "–ü—Ä–∏–≤–µ—Ç! –Ø —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ —Ç–≤–æ–µ–π –í–ö–†. –ü—Ä–∏—à–ª–∏ .doc/.docx ‚Äî —è –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä—É—é –∏ –±—É–¥—É –æ–±—ä—è—Å–Ω—è—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: –≥–ª–∞–≤—ã –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, —Å–º—ã—Å–ª —Ç–∞–±–ª–∏—Ü/—Ä–∏—Å—É–Ω–∫–æ–≤, –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –∫ –∑–∞—â–∏—Ç–µ. –ú–æ–∂–µ—à—å –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç—å –≤–æ–ø—Ä–æ—Å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –Ω–∞–ø–∏—Å–∞—Ç—å –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º."
    )

# ------------------------------ /diag ------------------------------

@dp.message(Command("diag"))
async def cmd_diag(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid) or get_user_active_doc(uid)
    if not doc_id:
        await _send(m, "–ê–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ—Ç. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª (.doc/.docx) —Å–Ω–∞—á–∞–ª–∞.")
        return

    # –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ë–î
    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    path = row["path"] if row else "‚Äî"

    cur.execute("SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=?", (uid, doc_id))
    chunks_cnt = int(cur.fetchone()["c"])

    con.close()

    tables_cnt = _count_tables(uid, doc_id)
    figures_cnt = _list_figures_db(uid, doc_id, limit=999999)["count"]
    sources_cnt = _count_sources(uid, doc_id)
    indexer_ver = get_document_indexer_version(doc_id) or 0

    txt = (
        f"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ #{doc_id}\n"
        f"‚Äî –ü—É—Ç—å: {path}\n"
        f"‚Äî –ß–∞–Ω–∫–æ–≤: {chunks_cnt}\n"
        f"‚Äî –¢–∞–±–ª–∏—Ü: {tables_cnt}\n"
        f"‚Äî –†–∏—Å—É–Ω–∫–æ–≤: {figures_cnt}\n"
        f"‚Äî –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {sources_cnt}\n"
        f"‚Äî –í–µ—Ä—Å–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: {indexer_ver} (—Ç–µ–∫—É—â–∞—è {CURRENT_INDEXER_VERSION})\n"
    )
    await _send(m, txt)


# ------------------------------ /reindex ------------------------------

@dp.message(Command("reindex"))
async def cmd_reindex(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid) or get_user_active_doc(uid)
    if not doc_id:
        await _send(m, "–ê–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ—Ç. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª —Å–Ω–∞—á–∞–ª–∞.")
        return

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()

    if not row:
        await _send(m, "–ù–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–Ω–æ–≤–æ.")
        return

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
        # –æ–±–æ–≥–∞—â–∞–µ–º —Å–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–æ–º
        sections = enrich_sections(sections, doc_kind=os.path.splitext(path)[1].lower().strip("."))
        delete_document_chunks(doc_id, uid)
        index_document(uid, doc_id, sections)
        invalidate_cache(uid, doc_id)
        set_document_indexer_version(doc_id, CURRENT_INDEXER_VERSION)
        update_document_meta(doc_id, layout_profile=_current_embedding_profile())
        await _send(m, f"–î–æ–∫—É–º–µ–Ω—Ç #{doc_id} –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω.")
    except Exception as e:
        logging.exception("reindex failed: %s", e)
        await _send(m, f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}")


# ---------- –†–∏—Å—É–Ω–∫–∏: –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–ª–æ–∫–∞–ª—å–Ω—ã–µ, –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç retrieval.py) ----------

_FIG_TITLE_RE = re.compile(
    r"(?i)\b(—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|figure|fig\.?)\s*(?:‚Ññ\s*)?(\d+(?:[.,]\d+)*)\b(?:\s*[‚Äî\-‚Äì:\u2013\u2014]\s*(.+))?"
)

def _compose_figure_display(attrs_json: str | None, section_path: str, title_text: str | None) -> str:
    """–î–µ–ª–∞–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∏—Å—É–Ω–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º."""
    num = None
    tail = None
    if attrs_json:
        try:
            a = json.loads(attrs_json or "{}")
            num = (a.get("caption_num") or a.get("label") or "").strip()
            tail = (a.get("caption_tail") or a.get("title") or "").strip()
        except Exception:
            pass

    if not num or not num.strip():
        cand = title_text or section_path or ""
        m = _FIG_TITLE_RE.search(cand)
        if m:
            num = (m.group(2) or "").replace(",", ".").strip()
            if not tail:
                tail = (m.group(3) or "").strip()

    if num:
        return f"–†–∏—Å—É–Ω–æ–∫ {num}" + (f" ‚Äî {_shorten(tail, 160)}" if tail else "")
    base = title_text or _last_segment(section_path or "")
    base = re.sub(r"(?i)^\s*(—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|figure|fig\.?)\s*", "", base).strip(" ‚Äî‚Äì-")
    return _shorten(base or "–†–∏—Å—É–Ω–æ–∫", 160)

def _list_figures_db(uid: int, doc_id: int, limit: int = 25) -> dict:
    """–°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ –ë–î (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏)."""
    con = get_conn()
    cur = con.cursor()
    has_type = _table_has_columns(con, "chunks", ["element_type", "attrs"])

    if has_type:
        cur.execute(
            "SELECT DISTINCT section_path, attrs, text FROM chunks "
            "WHERE owner_id=? AND doc_id=? AND element_type='figure' "
            "ORDER BY id ASC",
            (uid, doc_id),
        )
    else:
        cur.execute(
            "SELECT DISTINCT section_path, attrs, text FROM chunks "
            "WHERE owner_id=? AND doc_id=? AND (text LIKE '[–†–∏—Å—É–Ω–æ–∫]%' OR lower(section_path) LIKE '%—Ä–∏—Å—É–Ω–æ–∫%') "
            "ORDER BY id ASC",
            (uid, doc_id),
        )
    rows = cur.fetchall() or []
    con.close()

    items: list[str] = []
    for r in rows:
        section_path = r["section_path"] or ""
        attrs_json = r["attrs"] if "attrs" in r.keys() else None
        txt = r["text"] or None
        disp = _compose_figure_display(attrs_json, section_path, txt)
        items.append(disp)

    seen = set()
    uniq = []
    for it in items:
        k = it.strip().lower()
        if k and k not in seen:
            seen.add(k)
            uniq.append(it)

    total = len(uniq)
    return {
        "count": total,
        "list": uniq[:limit],
        "more": max(0, total - limit),
    }


# -------------------------- –°–ê–ú–û–í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ò–ù–î–ï–ö–°–ê --------------------------

def _count_et(con, uid: int, doc_id: int, et: str) -> int:
    cur = con.cursor()
    if _table_has_columns(con, "chunks", ["element_type"]):
        cur.execute(
            "SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=? AND element_type=?",
            (uid, doc_id, et),
        )
        row = cur.fetchone()
        return int(row["c"] or 0)
    return 0

def _need_self_heal(uid: int, doc_id: int, need_refs: bool, need_figs: bool) -> tuple[bool, int, int]:
    con = get_conn()
    rc = _count_et(con, uid, doc_id, "reference") if need_refs else 1
    fc = _count_et(con, uid, doc_id, "figure") if need_figs else 1
    con.close()
    return (rc == 0 or fc == 0, rc, fc)

def _reindex_with_sections(uid: int, doc_id: int, sections: list[dict]) -> None:
    delete_document_chunks(doc_id, uid)
    index_document(uid, doc_id, sections)
    invalidate_cache(uid, doc_id)
    set_document_indexer_version(doc_id, CURRENT_INDEXER_VERSION)
    update_document_meta(doc_id, layout_profile=_current_embedding_profile())

async def _ensure_modalities_indexed(m: types.Message, uid: int, doc_id: int, intents: dict):
    """–ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å—Ç–∞—Ä—ã–π –∏ –Ω–µ—Ç reference/figure ‚Äî —Ç–∏—Ö–æ –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–º –Ω–æ–≤—ã–º –ø–∞—Ä—Å–µ—Ä–æ–º –∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º."""
    need_refs = bool(intents.get("sources", {}).get("want"))
    need_figs = bool(intents.get("figures", {}).get("want"))
    if not (need_refs or need_figs):
        return

    should, have_refs, have_figs = _need_self_heal(uid, doc_id, need_refs, need_figs)
    if not should:
        return

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    if not row:
        return

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
        sections = enrich_sections(sections, doc_kind=os.path.splitext(path)[1].lower().strip("."))
    except Exception as e:
        logging.exception("re-parse/enrich failed: %s", e)
        return

    new_refs = sum(1 for s in sections if (s.get("element_type") == "reference"))
    new_figs = sum(1 for s in sections if (s.get("element_type") == "figure"))

    do_reindex = False
    if need_refs and have_refs == 0 and new_refs > 0:
        do_reindex = True
    if need_figs and have_figs == 0 and new_figs > 0:
        do_reindex = True

    if do_reindex:
        try:
            _reindex_with_sections(uid, doc_id, sections)
            await _send(m, "–û–±–Ω–æ–≤–∏–ª –∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞: –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ —Ä–∏—Å—É–Ω–∫–∏/–∏—Å—Ç–æ—á–Ω–∏–∫–∏.")
        except Exception as e:
            logging.exception("self-heal reindex failed: %s", e)


# -------------------------- –°–±–æ—Ä —Ñ–∞–∫—Ç–æ–≤ --------------------------

def _gather_facts(uid: int, doc_id: int, intents: dict) -> dict:
    """
    –°–æ–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –ë–î/–∏–Ω–¥–µ–∫—Å–∞, –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.
    """
    facts: dict[str, object] = {"doc_id": doc_id, "owner_id": uid}
    # —Ñ–ª–∞–≥ ¬´—Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–∞ –∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ¬ª
    facts["exact_numbers"] = bool(intents.get("exact_numbers"))

    # ----- –¢–∞–±–ª–∏—Ü—ã -----
    if intents["tables"]["want"]:
        total_tables = _count_tables(uid, doc_id)
        basenames = _distinct_table_basenames(uid, doc_id)

        con = get_conn()
        cur = con.cursor()
        items: list[str] = []
        for base in basenames:
            cur.execute(
                """
                SELECT attrs FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                  AND section_path LIKE ? || ' [row %'
                ORDER BY id ASC LIMIT 1
                """,
                (uid, doc_id, base),
            )
            r = cur.fetchone()
            attrs_json = r["attrs"] if r else None

            cur.execute(
                """
                SELECT text FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                  AND section_path LIKE ? || ' [row %'
                ORDER BY id ASC LIMIT 2
                """,
                (uid, doc_id, base),
            )
            rows = cur.fetchall() or []
            first_row_text = None
            for rr in rows:
                cand = (rr["text"] or "").split("\n")[0]
                cand = " ‚Äî ".join([c.strip() for c in cand.split(" | ") if c.strip()])
                if cand:
                    first_row_text = cand
                    break

            title = _compose_display_from_attrs(attrs_json, base, first_row_text)
            title = _strip_table_prefix(title)
            items.append(title)

        con.close()
        facts["tables"] = {
            "count": total_tables,
            "list": items[:intents["tables"]["limit"]],
            "more": max(0, len(items) - intents["tables"]["limit"]),
            "describe": [],
        }

        # –ê–≤—Ç–æ-–æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –æ–±—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã
        desc_cards = []
        if not intents["tables"].get("describe"):
            # –≤–æ–∑—å–º—ë–º –ø–µ—Ä–≤—ã–µ 3‚Äì5 —Ç–∞–±–ª–∏—Ü –∏–∑ —Å–ø–∏—Å–∫–∞
            bases = _distinct_table_basenames(uid, doc_id)[:min(5, intents["tables"]["limit"])]
            con = get_conn()
            cur = con.cursor()
            for base in bases:
                # attrs + –ø–µ—Ä–≤—ã–µ 1‚Äì2 —Å—Ç—Ä–æ–∫–∏
                cur.execute("""
                    SELECT page, section_path, attrs FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                    AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 1
                """, (uid, doc_id, base, base))
                row = cur.fetchone()
                if not row:
                    continue

                cur.execute("""
                    SELECT text FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                    AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 2
                """, (uid, doc_id, row["section_path"], row["section_path"]))
                rows = cur.fetchall() or []
                highlights = []
                for r in rows:
                    first = (r["text"] or "").split("\n")[0]
                    if first:
                        highlights.append(" ‚Äî ".join([c.strip() for c in first.split(" | ") if c.strip()]))

                attrs_json = row["attrs"] if row else None
                display = _compose_display_from_attrs(attrs_json, row["section_path"], highlights[0] if highlights else None)
                display = _strip_table_prefix(display)

                # –ø–æ–ø—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å –Ω–æ–º–µ—Ä –¥–ª—è stats
                num, _ = _parse_table_title(display)
                stats = None
                if num:
                    try:
                        stats = analyze_table_by_num(uid, doc_id, num, max_series=6)
                    except Exception:
                        stats = None

                desc_cards.append({
                    "num": num,
                    "display": display,
                    "where": {"page": row["page"], "section_path": row["section_path"]},
                    "highlights": highlights,
                    "stats": stats,
                })
            con.close()

        # –∑–∞–ø–∏—à–µ–º –¥–∞–∂–µ –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç–æ–π ‚Äî –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–∞ —ç—Ç–æ —É—á—Ç—ë—Ç
        facts["tables"]["describe"] = desc_cards
        # describe –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –Ω–æ–º–µ—Ä–∞–º + —Ç–æ—á–Ω—ã–µ —Ä–∞—Å—á–µ—Ç—ã
        desc_cards = []
        if intents["tables"]["describe"]:
            con = get_conn()
            cur = con.cursor()
            for num in intents["tables"]["describe"]:

                like1 = f'%\"caption_num\": \"{num}\"%'
                like2 = f'%\"label\": \"{num}\"%'
                cur.execute(
                    """
                    SELECT page, section_path, attrs FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                      AND (attrs LIKE ? OR attrs LIKE ?)
                    ORDER BY id ASC LIMIT 1
                    """,
                    (uid, doc_id, like1, like2),
                )
                row = cur.fetchone()

                if not row:
                    cur.execute(
                        """
                        SELECT page, section_path, attrs FROM chunks
                        WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                          AND section_path LIKE ? COLLATE NOCASE
                        ORDER BY id ASC LIMIT 1
                        """,
                        (uid, doc_id, f'%–¢–∞–±–ª–∏—Ü–∞ {num}%'),
                    )
                    row = cur.fetchone()

                if not row:
                    continue

                attrs_json = row["attrs"] if row else None
                # 1‚Äì2 –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ highlights
                cur.execute(
                    """
                    SELECT text FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                      AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 2
                    """,
                    (uid, doc_id, row["section_path"], row["section_path"]),
                )
                rows = cur.fetchall()
                highlights = []
                for r in rows or []:
                    first_line = (r["text"] or "").split("\n")[0]
                    if first_line:
                        highlights.append(" ‚Äî ".join([c.strip() for c in first_line.split(" | ") if c.strip()]))

                base = row["section_path"]
                first_row_text = highlights[0] if highlights else None
                display = _compose_display_from_attrs(attrs_json, base, first_row_text)
                display = _strip_table_prefix(display)

                # –ù–û–í–û–ï: —Ç–æ—á–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ —Ç–∞–±–ª–∏—Ü–µ
                stats = None
                try:
                    stats = analyze_table_by_num(uid, doc_id, num, max_series=6)
                except Exception:
                    stats = None

                desc_cards.append({
                    "num": num,
                    "display": display,
                    "where": {"page": row["page"], "section_path": row["section_path"]},
                    "highlights": highlights,
                    "stats": stats,
                })
            con.close()

        facts["tables"]["describe"] = desc_cards

    # ----- –†–∏—Å—É–Ω–∫–∏ -----
    if intents["figures"]["want"]:
        lst = _list_figures_db(uid, doc_id, limit=intents["figures"]["limit"])
        figs_block = {
            "count": int(lst.get("count") or 0),
            "list": list(lst.get("list") or []),
            "more": int(lst.get("more") or 0),
            "describe_lines": [],
        }

        if intents["figures"]["describe"]:
            try:
                desc_text = vision_describe_figures(uid, doc_id, intents["figures"]["describe"])
                lines = [ln.strip() for ln in (desc_text or "").splitlines() if ln.strip()]
                figs_block["describe_lines"] = lines[:25]
            except Exception as e:
                figs_block["describe_lines"] = [f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤: {e}"]

        facts["figures"] = figs_block

    # ----- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ -----
    if intents["sources"]["want"]:
        con = get_conn()
        cur = con.cursor()
        has_type = _table_has_columns(con, "chunks", ["element_type", "attrs"])
        items: list[str] = []

        if has_type:
            cur.execute(
                "SELECT text FROM chunks WHERE owner_id=? AND doc_id=? AND element_type='reference' ORDER BY id ASC",
                (uid, doc_id),
            )
            items = [(r["text"] or "").strip() for r in cur.fetchall()]

        if not any(items):
            cur.execute(
                """
                SELECT element_type, section_path, text
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                ORDER BY page ASC, id ASC
                """,
                (uid, doc_id),
            )
            raw = []
            for r in cur.fetchall():
                sec = (r["section_path"] or "").lower()
                if not any(k in sec for k in ("–∏—Å—Ç–æ—á–Ω–∏–∫", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ", "reference", "bibliograph")):
                    continue
                et = (r["element_type"] or "").lower()
                if et in ("heading", "table", "figure", "table_row"):
                    continue
                t = (r["text"] or "").strip()
                if t:
                    raw.append(t)

            seen = set()
            items = []
            for t in raw:
                k = re.sub(r"\s+", " ", t).strip().rstrip(".").lower()
                if len(k) < 5 or k in seen:
                    continue
                seen.add(k)
                items.append(t)

        con.close()

        facts["sources"] = {
            "count": len(items),
            "list": items[:intents["sources"]["limit"]],
            "more": max(0, len(items) - intents["sources"]["limit"]),
        }

    # ----- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å -----
    if intents.get("practical"):
        facts["practical_present"] = _has_practical_part(uid, doc_id)

    # ----- Summary -----
    if intents.get("summary"):
        s = overview_context(uid, doc_id, max_chars=6000) or _first_chunks_context(uid, doc_id, n=12, max_chars=6000)
        if s:
            facts["summary_text"] = s

    # ----- –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç / —Ü–∏—Ç–∞—Ç—ã -----
    # app/bot.py (_gather_facts: –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç / —Ü–∏—Ç–∞—Ç—ã)
    if intents.get("general_question"):
        vb = verbatim_find(uid, doc_id, intents["general_question"], max_hits=3)

        # –ù–û–í–û–ï: coverage-aware –≤—ã–±–æ—Ä–∫–∞ –ø–æ–¥ –º–Ω–æ–≥–æ–ø—É–Ω–∫—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å
        cov = retrieve_coverage(
            owner_id=uid,
            doc_id=doc_id,
            question=intents["general_question"],
            # per_item_k/prelim_factor/backfill_k ‚Äî —Å —Ä–∞–∑—É–º–Ω—ã–º–∏ –¥–µ—Ñ–æ–ª—Ç–∞–º–∏ –∏–∑ retrieval.py
        )
        ctx = ""
        if cov and cov.get("snippets"):
            ctx = build_context_coverage(
                cov["snippets"],
                items_count=len(cov.get("items") or []) or None,
                # base_chars/per_item_bonus/hard_limit ‚Äî –¥–µ—Ñ–æ–ª—Ç—ã –∏–∑ retrieval.py
            )

        # –§–æ–ª–±—ç–∫-—Å—Ç—É–ø–µ–Ω–∏, –µ—Å–ª–∏ coverage-–∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–±—Ä–∞–ª—Å—è
        if not ctx:
            ctx = best_context(uid, doc_id, intents["general_question"], max_chars=6000)
        if not ctx:
            hits = retrieve(uid, doc_id, intents["general_question"], top_k=12)  # –±—ã–ª–æ 8 ‚Üí —á—É—Ç—å —à–∏—Ä–µ
            if hits:
                ctx = build_context(hits)
        if not ctx:
            ctx = _first_chunks_context(uid, doc_id, n=12, max_chars=6000)

        if ctx:
            facts["general_ctx"] = ctx
        if vb:
            facts["verbatim_hits"] = vb
        # –ø–µ—Ä–µ–¥–∞—ë–º –ø–æ–¥–ø—É–Ω–∫—Ç—ã –∏ –≤ coverage.items (–¥–ª—è [Items]), –∏ –≤ general_subitems (–¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–π –ø–æ–¥–∞—á–∏)
        if cov and cov.get("items"):
            facts["coverage"] = {"items": cov["items"]}
            # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º general_subitems –ø–æ–¥ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π —Ä–µ–∂–∏–º (id+ask)
            facts["general_subitems"] = [
                {"id": i + 1, "ask": s} if isinstance(s, str) else s
                for i, s in enumerate(cov["items"])
            ]

    # –ª–æ–≥–∏—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Å—Ä–µ–∑ —Ñ–∞–∫—Ç–æ–≤ (–±–µ–∑ –æ–≥—Ä–æ–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤)
    log_snapshot = dict(facts)
    if "general_ctx" in log_snapshot and isinstance(log_snapshot["general_ctx"], str):
        log_snapshot["general_ctx"] = log_snapshot["general_ctx"][:300] + "‚Ä¶" if len(log_snapshot["general_ctx"]) > 300 else log_snapshot["general_ctx"]
    if "summary_text" in log_snapshot and isinstance(log_snapshot["summary_text"], str):
        log_snapshot["summary_text"] = log_snapshot["summary_text"][:300] + "‚Ä¶" if len(log_snapshot["summary_text"]) > 300 else log_snapshot["summary_text"]
    logging.debug("FACTS: %s", json.dumps(log_snapshot, ensure_ascii=False))
    return facts


# ------------------------------ FULLREAD: –º–æ–¥–µ–ª—å —á–∏—Ç–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª ------------------------------

def _full_document_text(owner_id: int, doc_id: int, *, limit_chars: int | None = None) -> str:
    """–°–∫–ª–µ–∏–≤–∞–µ–º –í–ï–°–¨ —Ç–µ–∫—Å—Ç –∏–∑ chunks (page ASC, id ASC)."""
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT text FROM chunks WHERE owner_id=? AND doc_id=? ORDER BY page ASC, id ASC",
        (owner_id, doc_id),
    )
    rows = cur.fetchall() or []
    con.close()

    parts = []
    total = 0
    for r in rows:
        t = (r["text"] or "").strip()
        if not t:
            continue
        if limit_chars is not None and total + len(t) > limit_chars:
            remaining = max(0, limit_chars - total)
            if remaining > 0:
                parts.append(t[:remaining])
                total += remaining
            break
        parts.append(t)
        total += len(t)
    return "\n\n".join(parts)

def _fullread_try_answer(uid: int, doc_id: int, q_text: str) -> str | None:
    """
    DIRECT: –æ—Ç–¥–∞—ë–º –º–æ–¥–µ–ª–∏ —Ü–µ–ª–∏–∫–æ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–∞–∫ –µ–¥–∏–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None (—É–π–¥—ë–º –≤ –∏–Ω–æ–π —Ä–µ–∂–∏–º).
    """
    if (Cfg.FULLREAD_MODE or "off") != "direct":
        return None

    full_text = _full_document_text(uid, doc_id, limit_chars=Cfg.DIRECT_MAX_CHARS + 1)
    if not full_text.strip():
        return None

    if len(full_text) > Cfg.DIRECT_MAX_CHARS:
        return None

    system_prompt = (
        "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –¢–µ–±–µ –¥–∞–Ω –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç –í–ö–†/–¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
        "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ–≤. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.\n"
        "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–¥–ø–∏—Å–∏ –∏ —Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º; –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞/–∑–Ω–∞—á–µ–Ω–∏—è.\n"
        "–¶–∏—Ç–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": f"[–î–æ–∫—É–º–µ–Ω—Ç ‚Äî –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç]\n{full_text}"},
        {"role": "user", "content": q_text},
    ]

    if STREAM_ENABLED and chat_with_gpt_stream is not None:
        return ("__STREAM__", json.dumps(messages, ensure_ascii=False))

    try:
        answer = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
        return (answer or "").strip() or None
    except Exception as e:
        logging.exception("fullread direct failed: %s", e)
        return None

def _fullread_collect_sections(uid: int, doc_id: int, *, max_sections: int = 800) -> List[str]:
    """
    –°–µ–∫—Ü–∏–∏ –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞: —Å–æ–±–∏—Ä–∞–µ–º –±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ section_path –≤ –ø–æ—Ä—è–¥–∫–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.
    """
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT section_path, text FROM chunks WHERE owner_id=? AND doc_id=? ORDER BY page ASC, id ASC",
        (uid, doc_id)
    )
    rows = cur.fetchall() or []
    con.close()

    out: List[str] = []
    cur_sec = None
    buf: List[str] = []

    def _flush():
        if buf:
            text = "\n".join([t for t in buf if t.strip()])
            if text.strip():
                out.append(text.strip())
        buf.clear()

    for r in rows:
        sec = r["section_path"] or ""
        t = (r["text"] or "").strip()
        if not t:
            continue
        if cur_sec is None:
            cur_sec = sec
        if sec != cur_sec:
            _flush()
            cur_sec = sec
        buf.append(t)
        if len(out) >= max_sections:
            break
    _flush()
    return out[:max_sections]

def _group_for_steps(sections: Iterable[str], per_step_chars: int, max_steps: int) -> List[str]:
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–µ–∫—Ü–∏–∏ –≤ –±–∞—Ç—á–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º (–¥–ª—è map-—à–∞–≥–∞)."""
    batches: List[str] = []
    cur: List[str] = []
    cur_len = 0
    for s in sections:
        if cur_len + len(s) + 1 > per_step_chars and cur:
            batches.append("\n\n".join(cur))
            cur, cur_len = [], 0
            if len(batches) >= max_steps:
                break
        cur.append(s)
        cur_len += len(s) + 1
    if cur and len(batches) < max_steps:
        batches.append("\n\n".join(cur))
    return batches[:max_steps]

def _map_extract(uid: int, doc_id: int, question: str, chunk_text: str, *, map_tokens: int) -> str:
    """–û–¥–∏–Ω map-–≤—ã–∑–æ–≤: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã/—Ü–∏—Ç–∞—Ç—ã –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞."""
    sys_map = (
        "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä. –¢–µ–±–µ –¥–∞–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–∏–ø–ª–æ–º–∞ –∏ –≤–æ–ø—Ä–æ—Å. "
        "–ò–∑–≤–ª–µ–∫–∏ –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏ –º–∏–Ω–∏-—Ü–∏—Ç–∞—Ç—ã, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ –≤–æ–ø—Ä–æ—Å—É. "
        "–ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ç–∞–±–ª–∏—Ü—ã ‚Äî –≤–∫–ª—é—á–∞–π –∏—Ö –Ω–∞–∑–≤–∞–Ω–∏—è –∏ 1‚Äì2 –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å —á–∏—Å–ª–∞–º–∏ "
        "(—Å–æ—Ö—Ä–∞–Ω—è–π –ø–æ—Ä—è–¥–æ–∫ –∏ –∑–Ω–∞—á–µ–Ω–∏—è). –§–æ—Ä–º–∞—Ç: –±—É–ª–ª–µ—Ç—ã."
    )
    return chat_with_gpt(
        [
            {"role": "system", "content": sys_map},
            {"role": "assistant", "content": f"[–§—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞]\n{chunk_text}"},
            {"role": "user", "content": f"–í–æ–ø—Ä–æ—Å: {question}\n–°–¥–µ–ª–∞–π –∫–æ—Ä–æ—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É (–±—É–ª–ª–µ—Ç—ã)."},
        ],
        temperature=0.1,
        max_tokens=max(120, int(map_tokens)),
    )

def _iterative_fullread_build_messages(uid: int, doc_id: int, question: str) -> Tuple[Optional[list], Optional[str]]:
    """
    –°–æ–±–∏—Ä–∞–µ–º map-–≤—ã–∂–∏–º–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º reduce-—Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Å—Ç—Ä–∏–º–∞
    –ò–õ–ò –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç (–µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫).
    """
    per_step = int(getattr(Cfg, "FULLREAD_STEP_CHARS", 14000))
    max_steps = int(getattr(Cfg, "FULLREAD_MAX_STEPS", 2))
    map_tokens = int(getattr(Cfg, "DIGEST_TOKENS_PER_SECTION", 300))
    reduce_tokens = int(getattr(Cfg, "FINAL_MAX_TOKENS", 900))

    sections = _fullread_collect_sections(uid, doc_id)
    if not sections:
        return None, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç —Å–µ–∫—Ü–∏—è–º–∏."

    batches = _group_for_steps(sections, per_step_chars=per_step, max_steps=max_steps)
    if not batches:
        return None, "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —à–∞–≥–∏ —á—Ç–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞."

    digests: List[str] = []
    for b in batches:
        try:
            digests.append(_map_extract(uid, doc_id, question, b, map_tokens=map_tokens))
        except Exception as e:
            logging.exception("map extract failed: %s", e)
            digests.append(b[:800])

    joined = "\n\n".join([f"[MAP {i+1}]\n{d}" for i, d in enumerate(digests)])
    ctx = joined[: int(getattr(Cfg, "FULLREAD_CONTEXT_CHARS", 9000))]

    sys_reduce = (
        "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –í–ö–†. –ù–∏–∂–µ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞ (map-–≤—ã–∂–∏–º–∫–∏). "
        "–°–æ–±–µ—Ä–∏ –∏–∑ –Ω–∏—Ö —Å–≤—è–∑–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã—Ö —Ü–∏—Ñ—Ä/—Ç–∞–±–ª–∏—Ü. "
        "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç ‚Äî –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π –ø–µ—Ä–µ—á–∏—Å–ª–∏, —á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç."
    )
    messages = [
        {"role": "system", "content": sys_reduce},
        {"role": "assistant", "content": f"–°–≤–æ–¥–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{ctx}"},
        {"role": "user", "content": question},
    ]
    return messages, None

# ------------------------------ –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ ------------------------------

@dp.message(F.document)
async def handle_doc(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc = m.document

    # 1) —Å–∫–∞—á–∏–≤–∞–µ–º
    file = await bot.get_file(doc.file_id)
    stream = await bot.download_file(file.file_path)
    data = stream.read()
    stream.close()

    # 2) –¥–µ–¥—É–ø –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É + file_unique_id
    sha256 = sha256_bytes(data)
    file_uid = getattr(doc, "file_unique_id", None)

    con = get_conn()
    existing_id = _find_existing_doc(con, uid, sha256, file_uid)

    if existing_id:
        existing_ver = get_document_indexer_version(existing_id) or 0

        need_reindex = False
        try:
            if _needs_reindex_by_embeddings(con, existing_id):
                need_reindex = True
        except Exception:
            need_reindex = True

        if existing_ver < CURRENT_INDEXER_VERSION:
            need_reindex = True

        if need_reindex:
            filename = safe_filename(f"{m.from_user.id}_{doc.file_name}")
            path = save_upload(data, filename, Cfg.UPLOAD_DIR)
            update_document_meta(existing_id, path=path, content_sha256=sha256, file_uid=file_uid)
            con.close()

            try:
                sections = _parse_by_ext(path)
                sections = enrich_sections(sections, doc_kind=os.path.splitext(path)[1].lower().strip("."))
                if sum(len(s.get("text") or "") for s in sections) < 500 and not any(
                    s.get("element_type") in ("table", "table_row", "figure") for s in sections
                ):
                    await _send(m, "–ü–æ—Ö–æ–∂–µ, —Ñ–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞/—Å—Ç—Ä—É–∫—Ç—É—Ä. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω .docx —Å ¬´–∂–∏–≤—ã–º–∏¬ª —Ç–∞–±–ª–∏—Ü–∞–º–∏. –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü—ã –±—ã–ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ ‚Äî —è –∏—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")
                    return

                delete_document_chunks(existing_id, uid)
                index_document(uid, existing_id, sections)
                invalidate_cache(uid, existing_id)

                set_document_indexer_version(existing_id, CURRENT_INDEXER_VERSION)
                update_document_meta(existing_id, layout_profile=_current_embedding_profile())

            except Exception as e:
                await _send(m, f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç #{existing_id}: {e}")
                return

            ACTIVE_DOC[uid] = existing_id
            set_user_active_doc(uid, existing_id)
            caption = (m.caption or "").strip()
            await _send(m, f"–î–æ–∫—É–º–µ–Ω—Ç #{existing_id} –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω. –ì–æ—Ç–æ–≤ –æ—Ç–≤–µ—á–∞—Ç—å.")
            if caption:
                await respond_with_answer(m, uid, existing_id, caption)
            return

        con.close()
        ACTIVE_DOC[uid] = existing_id
        set_user_active_doc(uid, existing_id)
        caption = (m.caption or "").strip()
        await _send(m, f"–≠—Ç–æ—Ç —Ñ–∞–π–ª —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ä–∞–Ω–µ–µ –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç #{existing_id}. –ò—Å–ø–æ–ª—å–∑—É—é –µ–≥–æ.")
        if caption:
            await respond_with_answer(m, uid, existing_id, caption)
        return

    # 3) —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    filename = safe_filename(f"{m.from_user.id}_{doc.file_name}")
    path = save_upload(data, filename, Cfg.UPLOAD_DIR)

    # 4) –ø–∞—Ä—Å–∏–º –∏ –û–ë–û–ì–ê–©–ê–ï–ú
    try:
        sections = _parse_by_ext(path)
        sections = enrich_sections(sections, doc_kind=os.path.splitext(path)[1].lower().strip("."))
    except Exception as e:
        await _send(m, f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")
        return

    # 5) –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä—ë–º–∞ ‚Äî —É–∂–µ –ø–æ—Å–ª–µ enrich
    if sum(len(s.get("text") or "") for s in sections) < 500 and not any(
        s.get("element_type") in ("table", "table_row", "figure") for s in sections
    ):
        await _send(m, "–ü–æ—Ö–æ–∂–µ, —Ñ–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞/—Å—Ç—Ä—É–∫—Ç—É—Ä. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π DOC/DOCX; —Ç–∞–±–ª–∏—Ü—ã-–∫–∞—Ä—Ç–∏–Ω–∫–∏ —è —Ä–∞—Å–ø–æ–∑–Ω–∞—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")
        return

    # 6) –¥–æ–∫—É–º–µ–Ω—Ç ‚Üí –ë–î –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    kind = infer_doc_kind(doc.file_name)
    doc_id = _insert_document(con, uid, kind, path, sha256, file_uid)
    con.close()

    index_document(uid, doc_id, sections)
    invalidate_cache(uid, doc_id)
    set_document_indexer_version(doc_id, CURRENT_INDEXER_VERSION)
    update_document_meta(doc_id, layout_profile=_current_embedding_profile())

    ACTIVE_DOC[uid] = doc_id
    set_user_active_doc(uid, doc_id)

    caption = (m.caption or "").strip()
    if caption:
        await _send(m, f"–î–æ–∫—É–º–µ–Ω—Ç #{doc_id} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω. –û—Ç–≤–µ—á–∞—é –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–∑ –ø–æ–¥–ø–∏—Å–∏‚Ä¶")
        await respond_with_answer(m, uid, doc_id, caption)
    else:
        await _send(m, f"–ì–æ—Ç–æ–≤–æ. –î–æ–∫—É–º–µ–Ω—Ç #{doc_id} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω. –ú–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ä–∞–±–æ—Ç–µ.")


# ------------------------------ –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç—á–∏–∫ ------------------------------

async def respond_with_answer(m: types.Message, uid: int, doc_id: int, q_text: str):
    q_text = (q_text or "").strip()
    logging.debug(f"–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {q_text}")
    if not q_text:
        await _send(m, "–í–æ–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π. –ù–∞–ø–∏—à–∏—Ç–µ, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –ø–æ –í–ö–†.")
        return

    viol = safety_check(q_text)
    if viol:
        await _send(m, viol + " –ó–∞–¥–∞–π—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ –í–ö–†.")
        return

    if await _maybe_run_gost(m, uid, doc_id, q_text):
        return

    # ====== FULLREAD: auto ======
    mode = (Cfg.FULLREAD_MODE or "off")
    if mode == "auto":
        # –ø—Ä–æ–±—É–µ–º –¥–∞—Ç—å –º–æ–¥–µ–ª–∏ –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –≤–ª–∞–∑–∏—Ç
        full_text = _full_document_text(uid, doc_id, limit_chars=Cfg.DIRECT_MAX_CHARS + 1)
        if full_text and len(full_text) <= Cfg.DIRECT_MAX_CHARS:
            system_prompt = (
                "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –¢–µ–±–µ –¥–∞–Ω –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç –í–ö–†/–¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
                "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ–≤. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.\n"
                "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–¥–ø–∏—Å–∏ –∏ —Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º; –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞/–∑–Ω–∞—á–µ–Ω–∏—è.\n"
                "–¶–∏—Ç–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
            )
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "assistant", "content": f"[–î–æ–∫—É–º–µ–Ω—Ç ‚Äî –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç]\n{full_text}"},
                {"role": "user", "content": q_text},
            ]
            if STREAM_ENABLED and chat_with_gpt_stream is not None:
                try:
                    stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                    await _stream_to_telegram(m, stream)
                    return
                except Exception as e:
                    logging.exception("auto fullread stream failed: %s", e)
            try:
                ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                if ans:
                    await _send(m, ans)
                    return
            except Exception as e:
                logging.exception("auto fullread non-stream failed: %s", e)
        else:
            # –¥–æ–∫—É–º–µ–Ω—Ç –±–æ–ª—å—à–æ–π ‚Üí –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —á—Ç–µ–Ω–∏–µ (map‚Üíreduce)
            messages, err = _iterative_fullread_build_messages(uid, doc_id, q_text)
            if messages:
                if STREAM_ENABLED and chat_with_gpt_stream is not None:
                    try:
                        stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                        await _stream_to_telegram(m, stream)
                        return
                    except Exception as e:
                        logging.exception("auto iterative stream failed: %s", e)
                try:
                    ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                    if ans:
                        await _send(m, ans)
                        return
                except Exception as e:
                    logging.exception("auto iterative non-stream failed: %s", e)
            elif err:
                await _send(m, err)
                return


    # ====== FULLREAD: direct ======
    if (Cfg.FULLREAD_MODE or "off") == "direct":
        fr = _fullread_try_answer(uid, doc_id, q_text)
        if isinstance(fr, tuple) and fr and fr[0] == "__STREAM__":
            messages = json.loads(fr[1])
            try:
                stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                await _stream_to_telegram(m, stream)
                return
            except Exception as e:
                logging.exception("direct fullread stream failed: %s", e)
                # —Ç–∏—Ö–æ –ø–∞–¥–∞–µ–º –≤ –æ–±—ã—á–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
        elif isinstance(fr, str) and fr:
            await _send(m, fr)
            return
        # –∏–Ω–∞—á–µ ‚Äî RAG –Ω–∏–∂–µ

    # ====== FULLREAD: iterative/digest ======
    if (Cfg.FULLREAD_MODE or "off") in {"iterative", "digest"}:
        messages, err = _iterative_fullread_build_messages(uid, doc_id, q_text)
        if messages:
            if STREAM_ENABLED and chat_with_gpt_stream is not None:
                try:
                    stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                    await _stream_to_telegram(m, stream)
                    return
                except Exception as e:
                    logging.exception("iterative fullread stream failed: %s", e)
            try:
                ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                if ans:
                    await _send(m, ans)
                    return
            except Exception as e:
                logging.exception("iterative fullread non-stream failed: %s", e)
        else:
            if err:
                await _send(m, err)
                return
        # –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ –≤—ã—à–ª–æ ‚Äî –ø—Ä–æ–≤–∞–ª–∏–≤–∞–µ–º—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º –Ω–∏–∂–µ

    # ====== –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º—É–ª—å—Ç–∏-–∏–Ω—Ç–µ–Ω—Ç –ø–∞–π–ø–ª–∞–π–Ω (RAG) ======
    intents = detect_intents(q_text)
    await _ensure_modalities_indexed(m, uid, doc_id, intents)
    facts = _gather_facts(uid, doc_id, intents)

    # ‚Üì –ù–û–í–û–ï: –µ—Å–ª–∏ –µ—Å—Ç—å –ø–ª–∞–Ω –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ ‚Äî –≤–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—É—é –ø–æ–¥–∞—á—É
    discovered_items = None
    if isinstance(facts, dict):
        discovered_items = (facts.get("coverage", {}).get("items")
                            or facts.get("general_subitems"))
    try:
        handled = await _run_multistep_answer(
            m, uid, doc_id, q_text, discovered_items=discovered_items  # –æ—Ç–ø—Ä–∞–≤–∏—Ç A‚ÜíB‚Üí‚Ä¶ –∏ –≤–µ—Ä–Ω—ë—Ç True
        )
        if handled:
            return
    except Exception as e:
        logging.exception("multistep pipeline failed, fallback to normal: %s", e)

    # –æ–±—ã—á–Ω—ã–π –ø—É—Ç—å
    # app/bot.py
    if STREAM_ENABLED and generate_answer_stream is not None:
        try:
            stream = generate_answer_stream(q_text, facts, language=intents.get("language", "ru"))
            await _stream_to_telegram(m, stream)
            return

        except Exception as e:
            logging.exception("stream answer failed, fallback to non-stream: %s", e)

    reply = generate_answer(q_text, facts, language=intents.get("language", "ru"))
    await _send(m, reply)


# ------------------------------ —ç–º–±–µ–¥–¥–∏–Ω–≥-–ø—Ä–æ—Ñ–∏–ª—å ------------------------------

def _current_embedding_profile() -> str:
    dim = probe_embedding_dim(None)
    if dim:
        return f"emb={Cfg.POLZA_EMB}|dim={dim}"
    return f"emb={Cfg.POLZA_EMB}"

def _needs_reindex_by_embeddings(con, doc_id: int) -> bool:
    if not _table_has_columns(con, "documents", ["layout_profile"]):
        return True
    cur = con.cursor()
    cur.execute("SELECT layout_profile FROM documents WHERE id=?", (doc_id,))
    row = cur.fetchone()
    stored = (row["layout_profile"] or "") if row else ""
    if not stored:
        return True
    cur_model = Cfg.POLZA_EMB.strip().lower()
    stored_model = ""
    stored_dim = None
    for part in stored.split("|"):
        part = (part or "").strip().lower()
        if part.startswith("emb="):
            stored_model = part[4:]
        if part.startswith("dim="):
            try:
                stored_dim = int(part[4:])
            except Exception:
                stored_dim = None
    if stored_model and stored_model != cur_model:
        return True
    cur_dim = probe_embedding_dim(None)
    if cur_dim and stored_dim and stored_dim != cur_dim:
        return True
    return False


# ------------------------------ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ------------------------------

@dp.message(F.text & ~F.via_bot & ~F.text.startswith("/"))
async def qa(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid)

    # –ù–û–í–û–ï: –µ—Å–ª–∏ –≤ –ø–∞–º—è—Ç–∏ –Ω–µ—Ç ‚Äî –ø–æ–¥–Ω–∏–º–µ–º –∏–∑ –ë–î (—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Ä–µ—Å—Ç–∞—Ä—Ç–∞–º –ø—Ä–æ—Ü–µ—Å—Å–∞)
    if not doc_id:
        persisted = get_user_active_doc(uid)
        if persisted:
            ACTIVE_DOC[uid] = persisted
            doc_id = persisted

    text = (m.text or "").strip()

    # –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º: –±–µ–∑ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –æ—Ç–≤–µ—á–∞–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é
    if not doc_id:
        if _is_greeting(text):
            await start(m)
        else:
            await _send(m, "–°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª (.doc/.docx). –ë–µ–∑ –Ω–µ–≥–æ —è –Ω–µ –æ—Ç–≤–µ—á–∞—é –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é.")
        return

    await respond_with_answer(m, uid, doc_id, text)
