# app/bot.py
import re
import os
import html
import json
import logging
from aiogram import Bot, Dispatcher, F, types
from aiogram.filters import Command

from .config import Cfg
from .db import (
    ensure_user, get_conn,
    set_document_indexer_version, get_document_indexer_version,
    CURRENT_INDEXER_VERSION,
    update_document_meta, delete_document_chunks,
)
from .parsing import parse_docx, parse_pdf, parse_doc, save_upload
from .indexing import index_document
from .retrieval import (
    retrieve, build_context, invalidate_cache,
    _mk_table_pattern, _mk_figure_pattern, keyword_find,  # –æ—Å—Ç–∞–≤–∏–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
)
from .ace import ace_once, agent_no_context
from .polza_client import probe_embedding_dim, chat_with_gpt  # ‚¨ÖÔ∏è –¥–æ–±–∞–≤–∏–ª–∏ chat_with_gpt

# —É—Ç–∏–ª–∏—Ç—ã
from .utils import safe_filename, sha256_bytes, split_for_telegram, infer_doc_kind

# –≥–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: —Å–µ–º–∞–Ω—Ç–∏–∫–∞ + FTS/LIKE
from .lexsearch import best_context

# —Å—Ä–∞–∑—É –ø–æ–¥ —Ç–µ–∫—É—â–∏–º–∏ import‚Äô–∞–º–∏
from .paywall_stub import setup_paywall

# –≥–¥–µ —É –≤–∞—Å —Å–æ–∑–¥–∞—é—Ç—Å—è –æ–±—ä–µ–∫—Ç—ã –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞:
bot = Bot(Cfg.TG_TOKEN)
dp = Dispatcher()
# –¥–æ–±–∞–≤—å—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É (–æ–¥–∏–Ω —Ä–∞–∑):
setup_paywall(dp, bot)



# --------------------- —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ ---------------------

_BOLD_RE = re.compile(r"\*\*(.+?)\*\*", re.DOTALL)

def _to_html(text: str) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º **bold** –≤ <b>...</b> –∏ —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º HTML."""
    if not text:
        return ""
    text = _BOLD_RE.sub(r"<b>\1</b>", text)
    text = html.escape(text)
    return text.replace("&lt;b&gt;", "<b>").replace("&lt;/b&gt;", "</b>")

async def _send(m: types.Message, text: str):
    """–ë–µ—Ä–µ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞—Å—Ç—è–º–∏ –≤ HTML-—Ä–µ–∂–∏–º–µ."""
    for chunk in split_for_telegram(text or "", 3900):
        await m.answer(_to_html(chunk), parse_mode="HTML", disable_web_page_preview=True)

# summarizer (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç)
try:
    from .summarizer import is_summary_intent, overview_context  # –º–æ–≥—É—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å ‚Äî –µ—Å—Ç—å —Ñ–æ–ª–±—ç–∫–∏ –Ω–∏–∂–µ
except Exception:
    def is_summary_intent(text: str) -> bool:
        return bool(re.search(r"\b(—Å—É—Ç—å|–∫—Ä–∞—Ç–∫–æ|–æ—Å–Ω–æ–≤–Ω|–≥–ª–∞–≤–Ω|summary|overview|–∏—Ç–æ–≥|–≤—ã–≤–æ–¥)\w*\b",
                              text or "", re.IGNORECASE))

    def overview_context(owner_id: int, doc_id: int, max_chars: int = 6000) -> str:
        con = get_conn()
        cur = con.cursor()
        cur.execute(
            """
            SELECT page, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=? 
              AND (text LIKE '[–ó–∞–≥–æ–ª–æ–≤–æ–∫]%%'
                   OR text LIKE '%%–¶–µ–ª—å%%'
                   OR text LIKE '%%–ó–∞–¥–∞—á%%'
                   OR text LIKE '%%–í–≤–µ–¥–µ–Ω%%'
                   OR text LIKE '%%–ó–∞–∫–ª—é—á–µ–Ω%%'
                   OR text LIKE '%%–í—ã–≤–æ–¥%%')
            ORDER BY page ASC, id ASC
            LIMIT 14
            """,
            (owner_id, doc_id),
        )
        rows = cur.fetchall()
        con.close()
        if not rows:
            return ""
        parts, total = [], 0
        for r in rows:
            block = f"{(r['text'] or '').strip()}"
            if total + len(block) > max_chars:
                break
            parts.append(block)
            total += len(block)
        return "\n\n".join(parts)

# vision-–æ–ø–∏—Å–∞–Ω–∏–µ —Ä–∏—Å—É–Ω–∫–æ–≤ (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –æ—Ç–≤–µ—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ñ–æ–ª–±—ç–∫–æ–º)
try:
    from .summarizer import describe_figures as vision_describe_figures
except Exception:
    def vision_describe_figures(owner_id: int, doc_id: int, numbers: list[str]) -> str:
        if not numbers:
            return "–ù–µ —É–∫–∞–∑–∞–Ω—ã –Ω–æ–º–µ—Ä–∞ —Ä–∏—Å—É–Ω–∫–æ–≤."
        return "–û–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (vision-–º–æ–¥—É–ª—å –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω)."

# –ì–û–°–¢-–≤–∞–ª–∏–¥–∞—Ç–æ—Ä (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç)
try:
    from .validators_gost import validate_gost, render_report
except Exception:
    validate_gost = None
    render_report = None


# ¬´–ê–∫—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç¬ª –≤ –ø–∞–º—è—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
ACTIVE_DOC: dict[int, int] = {}  # user_id -> doc_id


# ------------------------ –ì–∞—Ä–¥—Ä–µ–π–ª—ã ------------------------

_BANNED_PATTERNS = [
    r"jail ?break|system\s*prompt|developer\s*mode|dan\b|ignore (all|previous) (rules|instructions)",
    r"\b–≤–∑–ª–æ–º|—Ö–∞–∫–∏?|–∫–µ–π–≥–µ–Ω|–∫—Ä—è–∫|—Å–æ—Ü–∏–∞–ª—å–Ω(–∞—è|—ã–µ) –∏–Ω–∂–µ–Ω–µ—Ä–∏—è",
    r"\b–≤–∏—Ä—É—Å|–≤—Ä–µ–¥–æ–Ω–æ—Å|—ç–∫—Å–ø–ª–æ–π—Ç|–±–æ—Ç–Ω–µ—Ç|ddos\b",
    r"\b–æ—Ä—É–∂–∏|–≤–∑—Ä—ã–≤—á–∞—Ç|–±–æ–º–±|–Ω–∞—Ä–∫–æ—Ç|–ø–æ—Ä–Ω–æ|—ç—Ä–æ—Ç–∏–∫|18\+",
    r"\b–ø–∞—Å–ø–æ—Ä—Ç|—Å–Ω–∏–ª—Å|–∏–Ω–Ω\b.*(—Å–≥–µ–Ω–µ—Ä|–ø–æ–¥–¥–µ–ª)",
    r"\b–æ–±–æ–π(–¥|—Ç–∏)\b.*–∞–Ω—Ç–∏–ø–ª–∞–≥|–∞–Ω—Ç–∏–ø–ª–∞–≥–∏–∞—Ç|antiplagiat",
    r"sql.?–∏–Ω—ä–µ–∫|–∏–Ω—ä–µ–∫—Ü–∏(—è|–∏) sql",
]

def safety_check(text: str) -> str | None:
    t = (text or "").lower()
    for p in _BANNED_PATTERNS:
        if re.search(p, t, flags=re.IGNORECASE):
            return ("–ó–∞–ø—Ä–æ—Å –Ω–∞—Ä—É—à–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ "
                    "(–≤–∑–ª–æ–º/–≤—Ä–µ–¥–æ–Ω–æ—Å/–æ–±—Ö–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π/NSFW/–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ).")
    return None

_ALLOWED_HINT_WORDS = [
    "–≤–∫—Ä", "–¥–∏–ø–ª–æ–º", "–∫—É—Ä—Å–æ–≤", "–º–µ—Ç–æ–¥–æ–ª–æ–≥", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–ª–∏—Ç–æ–±–∑–æ—Ä",
    "–≥–∏–ø–æ—Ç–µ–∑", "—Ü–µ–ª—å", "–∑–∞–¥–∞—á", "–≤–≤–µ–¥–µ–Ω–∏–µ", "–∑–∞–∫–ª—é—á–µ–Ω", "–æ–±–∑–æ—Ä",
    "–æ—Ñ–æ—Ä–º–ª–µ–Ω", "–≥–æ—Å—Ç", "—Ç–∞–±–ª–∏—Ü", "—Ä–∏—Å—É–Ω–∫", "–∞–Ω—Ç–∏–ø–ª–∞–≥", "–ø–ª–∞–≥–∏–∞—Ç",
    "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü", "–∑–∞—â–∏—Ç—É", "–æ–ø—Ä–æ—Å", "–∞–Ω–∫–µ—Ç–∞", "–º–µ—Ç–æ–¥—ã", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫",
]

def topical_check(text: str) -> str | None:
    """
    –ú—è–≥–∫–æ–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–õ–¨–ö–û –∫–∞–∫ –ø–æ–¥—Å–∫–∞–∑–∫—É,
    –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    """
    t = (text or "").lower()
    if not any(w in t for w in _ALLOWED_HINT_WORDS):
        return ("–ü–æ–¥—Å–∫–∞–∑–∫–∞: —è —Å–∏–ª—å–Ω–µ–µ –æ—Ç–≤–µ—á–∞—é –ø–æ —Ç–µ–º–µ –í–ö–† (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è, –ì–û–°–¢, "
                "–ª–∏—Ç–æ–±–∑–æ—Ä, –∞–Ω—Ç–∏–ø–ª–∞–≥–∏–∞—Ç). –ï—Å–ª–∏ –ø—Ä–∏—à–ª—ë—Ç–µ —Ñ–∞–π–ª –¥–∏–ø–ª–æ–º–∞ ‚Äî —Å–º–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é.")
    return None


# --------------------- –ë–î / —É—Ç–∏–ª–∏—Ç—ã ---------------------

def _table_has_columns(con, table: str, cols: list[str]) -> bool:
    cur = con.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    have = {row[1] for row in cur.fetchall()}
    return all(c in have for c in cols)

def _find_existing_doc(con, owner_id: int, sha256: str | None, file_uid: str | None):
    """–ü–æ–∏—Å–∫ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ sha/file_unique_id (–µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å)."""
    if not _table_has_columns(con, "documents", ["content_sha256", "file_uid"]):
        return None
    cur = con.cursor()
    if sha256 and file_uid:
        cur.execute(
            "SELECT id FROM documents WHERE owner_id=? AND (content_sha256=? OR file_uid=?) "
            "ORDER BY id DESC LIMIT 1",
            (owner_id, sha256, file_uid),
        )
    elif sha256:
        cur.execute(
            "SELECT id FROM documents WHERE owner_id=? AND content_sha256=? "
            "ORDER BY id DESC LIMIT 1",
            (owner_id, sha256),
        )
    elif file_uid:
        cur.execute(
            "SELECT id FROM documents WHERE owner_id=? AND file_uid=? "
            "ORDER BY id DESC LIMIT 1",
            (owner_id, file_uid),
        )
    row = cur.fetchone()
    return (row["id"] if row else None)

def _insert_document(con, owner_id: int, kind: str, path: str,
                     sha256: str | None, file_uid: str | None) -> int:
    cur = con.cursor()
    if _table_has_columns(con, "documents", ["content_sha256", "file_uid"]):
        cur.execute(
            "INSERT INTO documents(owner_id, kind, path, content_sha256, file_uid) VALUES(?,?,?,?,?)",
            (owner_id, kind, path, sha256, file_uid),
        )
    else:
        cur.execute(
            "INSERT INTO documents(owner_id, kind, path) VALUES(?,?,?)",
            (owner_id, kind, path),
        )
    doc_id = cur.lastrowid
    con.commit()
    return doc_id


# --------------------- –¢–∞–±–ª–∏—Ü—ã: –ø–∞—Ä—Å–∏–Ω–≥/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ---------------------

_TABLE_ANY = re.compile(r"\b—Ç–∞–±–ª–∏—Ü\w*|\b—Ç–∞–±–ª\.\b|\b—Ç–∞–±–ª–∏—Ü–∞\w*|(?:^|\s)table(s)?\b", re.IGNORECASE)
# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º: 2.1, 3, A.1, –ê.1, –ü1.2
_TABLE_TITLE_RE = re.compile(r"(?i)\b—Ç–∞–±–ª–∏—Ü–∞\s+(\d+(?:[.,]\d+)*|[a-z–∞-—è]\.?\s*\d+(?:[.,]\d+)*)\b(?:\s*[‚Äî\-‚Äì]\s*(.+))?")
_COUNT_HINT = re.compile(r"\b—Å–∫–æ–ª—å–∫–æ\b|how many", re.IGNORECASE)
_WHICH_HINT = re.compile(r"\b–∫–∞–∫–∏(–µ|—Ö)\b|\b—Å–ø–∏—Å–æ–∫\b|\b–ø–µ—Ä–µ—á–∏—Å–ª\w*\b|\b–Ω–∞–∑–æ–≤\w*\b", re.IGNORECASE)

def _plural_tables(n: int) -> str:
    n_abs = abs(n) % 100
    n1 = n_abs % 10
    if 11 <= n_abs <= 14:
        return "—Ç–∞–±–ª–∏—Ü"
    if n1 == 1:
        return "—Ç–∞–±–ª–∏—Ü–∞"
    if 2 <= n1 <= 4:
        return "—Ç–∞–±–ª–∏—Ü—ã"
    return "—Ç–∞–±–ª–∏—Ü"

def _strip_table_prefix(s: str) -> str:
    return re.sub(r"^\[\s*—Ç–∞–±–ª–∏—Ü–∞\s*\]\s*", "", s or "", flags=re.IGNORECASE)

def _last_segment(name: str) -> str:
    s = (name or "").strip()
    if "/" in s:
        s = s.split("/")[-1].strip()
    s = _strip_table_prefix(s)
    s = re.sub(r"\s*[-‚Äì‚Äî]\s*", " ‚Äî ", s)
    s = re.sub(r"\s+", " ", s).strip(" .")
    return s

def _parse_table_title(text: str) -> tuple[str | None, str | None]:
    t = (text or "").strip()
    m = _TABLE_TITLE_RE.search(t)
    if not m:
        return (None, None)
    num = (m.group(1) or "").strip() or None
    title = (m.group(2) or "").strip() or None
    return (num, title)

def _shorten(s: str, limit: int = 120) -> str:
    s = (s or "").strip()
    if len(s) <= limit:
        return s
    return s[:limit - 1].rstrip() + "‚Ä¶"


# -------- –¢–∞–±–ª–∏—Ü—ã: –ø–æ–¥—Å—á—ë—Ç –∏ —Å–ø–∏—Å–æ–∫ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –ë–î) --------

def _distinct_table_basenames(uid: int, doc_id: int) -> list[str]:
    """
    –°–æ–±–∏—Ä–∞–µ–º ¬´–±–∞–∑–æ–≤—ã–µ¬ª –∏–º–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü (section_path –±–µ–∑ —Ö–≤–æ—Å—Ç–∞ ' [row ‚Ä¶]').
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ —Å –Ω–æ–≤—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ (table_row) –∏ —Å–æ —Å—Ç–∞—Ä—ã–º–∏.
    """
    con = get_conn()
    cur = con.cursor()

    # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–ø–µ—Ä–µ—Ç—å—Å—è –Ω–∞ —Ç–∏–ø—ã
    if _table_has_columns(con, "chunks", ["element_type"]):
        cur.execute(
            """
            SELECT DISTINCT
                CASE
                    WHEN instr(section_path, ' [row ')>0
                        THEN substr(section_path, 1, instr(section_path,' [row ')-1)
                    ELSE section_path
                END AS base_name
            FROM chunks
            WHERE doc_id=? AND owner_id=? AND element_type IN ('table','table_row')
            """,
            (doc_id, uid),
        )
    else:
        # –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å ‚Äî —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        cur.execute(
            """
            SELECT DISTINCT
                CASE
                    WHEN instr(section_path, ' [row ')>0
                        THEN substr(section_path, 1, instr(section_path,' [row ')-1)
                    ELSE section_path
                END AS base_name
            FROM chunks
            WHERE doc_id=? AND owner_id=? AND (
                  lower(section_path) LIKE '%—Ç–∞–±–ª–∏—Ü–∞%'
               OR lower(text)        LIKE '%—Ç–∞–±–ª–∏—Ü–∞%'
               OR section_path LIKE '–¢–∞–±–ª–∏—Ü–∞ %' COLLATE NOCASE
               OR text        LIKE '[–¢–∞–±–ª–∏—Ü–∞]%' COLLATE NOCASE
               OR lower(section_path) LIKE '%table %'
               OR lower(text)        LIKE '%table %'
            )
            """,
            (doc_id, uid),
        )

    base_items = [r["base_name"] for r in cur.fetchall() if r["base_name"]]
    con.close()
    base_items = sorted(set(base_items), key=lambda s: s.lower())
    return base_items

def _count_tables(uid: int, doc_id: int) -> int:
    return len(_distinct_table_basenames(uid, doc_id))

def _compose_display_from_attrs(attrs_json: str | None, base: str, first_row_text: str | None) -> str:
    """
    –ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è:
      1) –µ—Å—Ç—å caption_num ‚Üí '–¢–∞–±–ª–∏—Ü–∞ N ‚Äî tail/header/firstrow' (–≤—Å–µ–≥–¥–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å).
      2) –Ω–µ—Ç –Ω–æ–º–µ—Ä–∞ ‚Üí –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ: caption_tail/ header_preview/ first_row.
      3) —Ñ–æ–ª–±—ç–∫: –ø–∞—Ä—Å–∏–º –Ω–æ–º–µ—Ä –∏ —Ö–≤–æ—Å—Ç –∏–∑ base ('–¢–∞–±–ª–∏—Ü–∞ N ‚Äî ...') –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å –Ω–æ–º–µ—Ä–æ–º.
      4) –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã—à–ª–æ ‚Äî –±–µ—Ä—ë–º –∫–æ—Ä–æ—Ç–∫–∏–π base –±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤.
    """
    num = None
    tail = None
    header_preview = None
    if attrs_json:
        try:
            a = json.loads(attrs_json or "{}")
            num = a.get("caption_num") or a.get("label")
            tail = a.get("caption_tail") or a.get("title")
            header_preview = a.get("header_preview")
        except Exception:
            pass

    if num:
        num = str(num).replace(",", ".").strip()
        tail_like = (tail or header_preview or first_row_text or "").strip()
        return f"–¢–∞–±–ª–∏—Ü–∞ {num}" + (f" ‚Äî {_shorten(tail_like, 160)}" if tail_like else "")

    # –±–µ–∑ –Ω–æ–º–µ—Ä–∞ –≤ attrs ‚Äî –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∏–∑ base –∏ –ø–æ–∫–∞–∑–∞—Ç—å –° –Ω–æ–º–µ—Ä–æ–º
    num_b, title_b = _parse_table_title(_last_segment(base))
    if num_b:
        text_tail = title_b or first_row_text or header_preview
        return f"–¢–∞–±–ª–∏—Ü–∞ {num_b}" + (f" ‚Äî {_shorten(text_tail, 160)}" if text_tail else "")

    # –±–µ–∑ –Ω–æ–º–µ—Ä–∞ ‚Äî —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ
    if tail:
        return _shorten(str(tail), 160)
    if header_preview:
        return _shorten(str(header_preview), 160)
    if first_row_text:
        return _shorten(first_row_text, 160)

    s = _last_segment(base)
    s = re.sub(r"(?i)^\s*—Ç–∞–±–ª–∏—Ü–∞\s+\d+(?:\.\d+)*\s*", "", s).strip(" ‚Äî‚Äì-")
    return _shorten(s or "–¢–∞–±–ª–∏—Ü–∞", 160)


# ------------------------------ –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ------------------------------

_SOURCES_HINT = re.compile(
    r"\b(–∏—Å—Ç–æ—á–Ω–∏–∫(?:–∏|–æ–≤)?|—Å–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã|—Å–ø–∏—Å–æ–∫\s+–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤|–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ\w*|references?|bibliograph\w*)\b",
    re.IGNORECASE
)
_REF_LINE_RE = re.compile(r"^\s*(?:\[\d+\]|\d+[.)])\s+.+", re.MULTILINE)

def _count_sources(uid: int, doc_id: int) -> int:
    """
    –ü–æ–¥—Å—á—ë—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
      1) –µ—Å–ª–∏ –≤ –ë–î –µ—Å—Ç—å element_type='reference' ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ;
      2) –∏–Ω–∞—á–µ —Å–æ–±–∏—Ä–∞–µ–º –ª—é–±—ã–µ –Ω–µ–ø—É—Å—Ç—ã–µ –∞–±–∑–∞—Ü—ã –≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–π ¬´–∏—Å—Ç–æ—á–Ω–∏–∫–∏/–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞/‚Ä¶¬ª
         (–±–µ–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–ª–∞—Å—å —Å –Ω–æ–º–µ—Ä–∞).
    """
    con = get_conn()
    cur = con.cursor()
    has_type = _table_has_columns(con, "chunks", ["element_type"])

    total = 0
    if has_type:
        cur.execute(
            "SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=? AND element_type='reference'",
            (uid, doc_id),
        )
        row = cur.fetchone()
        total = int(row["c"] or 0)

    if total == 0:
        items = set()
        cur.execute(
            """
            SELECT element_type, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
            ORDER BY page ASC, id ASC
            """,
            (uid, doc_id),
        )
        for r in cur.fetchall():
            sec = (r["section_path"] or "").lower()
            if not any(k in sec for k in ("–∏—Å—Ç–æ—á–Ω–∏–∫", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ", "reference", "bibliograph")):
                continue
            et = (r["element_type"] or "").lower()
            if et in ("heading", "table", "figure", "table_row"):
                continue
            t = (r["text"] or "").strip()
            if not t:
                continue
            k = re.sub(r"\s+", " ", t).strip().rstrip(".").lower()
            if len(k) >= 5:
                items.add(k)
        total = len(items)

    con.close()
    return total


# --------- –±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç: –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏ ---------
_PRACTICAL_Q = re.compile(r"(–µ—Å—Ç—å –ª–∏|–Ω–∞–ª–∏—á–∏–µ|–ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ª–∏|–∏–º–µ–µ—Ç—Å—è –ª–∏).{0,40}–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫", re.IGNORECASE)

def _has_practical_part(uid: int, doc_id: int) -> bool:
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        """
        SELECT 1
        FROM chunks
        WHERE owner_id=? AND doc_id=? AND (
            lower(section_path) LIKE '%–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫%' OR
            lower(text)         LIKE '%–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫%'
        )
        LIMIT 1
        """,
        (uid, doc_id),
    )
    row = cur.fetchone()
    con.close()
    return row is not None


# ------------- –ì–û–°–¢-–∏–Ω—Ç–µ–Ω—Ç –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ ------------- 

_GOST_HINT = re.compile(r"\b(–≥–æ—Å—Ç|–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏|—à—Ä–∏—Ñ—Ç|–º–µ–∂—Å—Ç—Ä–æ—á|–∫–µ–≥–ª|–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏|–ø–æ–ª—è|–æ—Ñ–æ—Ä–º–∏—Ç—å)\w*\b", re.IGNORECASE)

async def _maybe_run_gost(m: types.Message, uid: int, doc_id: int, text: str) -> bool:
    """–ï—Å–ª–∏ –ø–æ—Ö–æ–∂–µ, —á—Ç–æ –ø—Ä–æ—Å—è—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –ì–û–°–¢. –í–æ–∑–≤—Ä–∞—â–∞–µ–º True, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç–∏–ª–∏."""
    if not validate_gost or not render_report:
        return False
    if not _GOST_HINT.search(text or ""):
        return False

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    if not row:
        return False

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
    except Exception:
        return False

    report = validate_gost(sections)
    text_rep = render_report(report, max_issues=25)
    await _send(m, text_rep)
    return True

# ------------------------------ helpers ------------------------------

def _parse_by_ext(path: str) -> list[dict]:
    fname = (os.path.basename(path) or "").lower()
    if fname.endswith(".docx"):
        return parse_docx(path)
    if fname.endswith(".doc"):
        return parse_doc(path)
    if fname.endswith(".pdf"):
        return parse_pdf(path)
    raise RuntimeError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é .doc, .docx –∏ .pdf.")

def _first_chunks_context(owner_id: int, doc_id: int, n: int = 10, max_chars: int = 6000) -> str:
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT page, section_path, text FROM chunks "
        "WHERE owner_id=? AND doc_id=? "
        "ORDER BY page ASC, id ASC LIMIT ?",
        (owner_id, doc_id, n)
    )
    rows = cur.fetchall()
    con.close()
    if not rows:
        return ""
    parts, total = [], 0
    for r in rows:
        block = f"{(r['text'] or '').strip()}"
        if total + len(block) > max_chars:
            break
        parts.append(block)
        total += len(block)
    return "\n\n".join(parts)

# ---------- verbatim fallback –ø–æ —Ü–∏—Ç–∞—Ç–µ (—à–∏–Ω–≥–ª—ã + LIKE/NOCASE) ----------

def _normalize_for_like(s: str) -> str:
    s = (s or "")
    s = s.replace("\u00A0", " ")  # NBSP -> –ø—Ä–æ–±–µ–ª
    s = s.replace("¬´", '"').replace("¬ª", '"').replace("‚Äú", '"').replace("‚Äù", '"')
    s = s.replace("‚Äô", "'").replace("‚Äò", "'")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _make_shingles(s: str, min_len: int = 30, max_len: int = 90, step: int = 25) -> list[str]:
    s = _normalize_for_like(s)
    if not s:
        return []
    if len(s) <= max_len:
        return [s]
    out = []
    i = 0
    while i < len(s):
        chunk = s[i:i + max_len]
        if len(chunk) >= min_len:
            out.append(chunk)
        i += step
    return out[:6]

def verbatim_find(owner_id: int, doc_id: int, q_text: str, max_hits: int = 3) -> list[dict]:
    shingles = _make_shingles(q_text)
    if not shingles:
        return []
    con = get_conn()
    cur = con.cursor()
    hits: list[dict] = []
    for sh in shingles:
        pattern = f"%{sh}%"
        cur.execute(
            """
            SELECT page, section_path, text FROM chunks
            WHERE owner_id=? AND doc_id=? AND
                  replace(text, char(160), ' ') LIKE ? COLLATE NOCASE
            ORDER BY page ASC, id ASC
            LIMIT ?
            """,
            (owner_id, doc_id, pattern, max_hits - len(hits)),
        )
        for r in cur.fetchall():
            t = (r["text"] or "")
            t_norm = _normalize_for_like(t)
            pos = t_norm.lower().find(_normalize_for_like(sh).lower())
            if pos >= 0:
                s = max(pos - 120, 0)
                e = min(pos + len(sh) + 120, len(t_norm))
                hits.append({
                    "page": r["page"],
                    "section_path": r["section_path"],
                    "snippet": t_norm[s:e].strip(),
                })
            if len(hits) >= max_hits:
                con.close()
                return hits
    con.close()
    return hits


# ------------------------------ /start ------------------------------

@dp.message(Command("start"))
async def start(m: types.Message):
    ensure_user(str(m.from_user.id))
    await _send(m,
        "–ü—Ä–∏–≤–µ—Ç! –Ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –í–ö–†. –ü—Ä–∏—à–ª–∏ —Ñ–∞–π–ª –¥–∏–ø–ª–æ–º–∞ (.doc/.docx/) ‚Äî —è –µ–≥–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä—É—é –∏ –±—É–¥—É –æ—Ç–≤–µ—á–∞—Ç—å –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é.\n"
        "–ú–æ–∂–Ω–æ –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç—å –ø–æ–¥–ø–∏—Å—å-–≤–æ–ø—Ä–æ—Å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ–∑–∂–µ –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏.\n"
        "üí≥ –û–ø–ª–∞—Ç–∞ –∏ —Å—Ç–∞—Ç—É—Å:\n"
        "‚Ä¢ /buy ‚Äî –æ–ø–ª–∞—Ç–∏—Ç—å –ø–æ–¥–ø–∏—Å–∫—É\n"
        "‚Ä¢ /status ‚Äî —Å—Ç–∞—Ç—É—Å –ø–æ–¥–ø–∏—Å–∫–∏/–ø—Ä–æ–±–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞\n\n"
    )


# ======================= –ù–û–í–ê–Ø –ú–£–õ–¨–¢–ò-–ò–ù–¢–ï–ù–¢ –õ–û–ì–ò–ö–ê =======================

# –ü–æ–Ω–∏–º–∞–µ–º: 2.1, 3, A.1, –ê.1, –ü1.2 –∏ —Ç.–ø. (–¥–ª—è —Ç–∞–±–ª–∏—Ü)
_NUM_IN_TEXT = re.compile(r"(?i)\b—Ç–∞–±–ª(?:–∏—Ü–∞)?\.?\s*([a-z–∞-—è]\.?[\s-]?\d+(?:[.,]\d+)*|\d+(?:[.,]\d+)*)\b")
# –ü–æ–Ω–∏–º–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø—Ä–æ —Ä–∏—Å—É–Ω–∫–∏/–∫–∞—Ä—Ç–∏–Ω–∫–∏/–¥–∏–∞–≥—Ä–∞–º–º—ã/–≥—Ä–∞—Ñ–∏–∫–∏/—Å—Ö–µ–º—ã –∏ —Ç.–ø.
_FIG_ANY = re.compile(
    r"\b(—Ä–∏—Å—É–Ω\w*|—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|figure|fig\.?|–∫–∞—Ä—Ç–∏–Ω\w*|–∏–∑–æ–±—Ä–∞–∂–µ–Ω\w*|–¥–∏–∞–≥—Ä–∞–º–º\w*|–≥—Ä–∞—Ñ–∏–∫\w*|—Å—Ö–µ–º\w*|–∏–ª–ª—é—Å—Ç—Ä–∞—Ü\w*)\b",
    re.IGNORECASE
)
# –ü–æ–Ω–∏–º–∞–µ–º –Ω–æ–º–µ—Ä–∞ —Ä–∏—Å—É–Ω–∫–æ–≤: ¬´—Ä–∏—Å. 2.3¬ª, ¬´—Ä–∏—Å—É–Ω–æ–∫ 4¬ª, ¬´figure 1.2¬ª, ¬´fig. 3¬ª
_FIG_NUM_IN_TEXT = re.compile(r"(?i)\b(?:—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|figure|fig\.?|–∫–∞—Ä—Ç–∏–Ω\w*)\s*(?:‚Ññ\s*)?(\d+(?:[.,]\d+)*)\b")

def _detect_intents(text: str) -> dict:
    """
    –ò–∑ –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç –±—ã–ª –æ–¥–∏–Ω, –Ω–æ –ø–æ–ª–Ω—ã–π.
    """
    t = (text or "").strip()
    intents = {
        "language": "—Ä—É",
        "tables": {"want": False, "count": False, "list": False, "describe": [], "limit": 25},
        "sources": {"want": False, "count": False, "list": False, "limit": 25},
        "figures": {"want": False, "count": False, "list": False, "describe": [], "limit": 25},  # ‚¨ÖÔ∏è –ù–û–í–û–ï
        "summary": bool(is_summary_intent(t)),
        "practical": bool(_PRACTICAL_Q.search(t or "")),
        "general_question": None,
    }

    # –Ø–∑—ã–∫ (–æ—á–µ–Ω—å –≥—Ä—É–±–æ)
    if re.search(r"[a-z]{3,}", t) and not re.search(r"[–∞-—è]{3,}", t, re.IGNORECASE):
        intents["language"] = "en"

    # –¢–∞–±–ª–∏—Ü—ã
    if _TABLE_ANY.search(t):
        intents["tables"]["want"] = True
        if _COUNT_HINT.search(t):
            intents["tables"]["count"] = True
        if _WHICH_HINT.search(t) or re.search(r"\b(–∫–∞–∫–∏–µ —Ç–∞–±–ª–∏—Ü|—Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü)\b", t, re.IGNORECASE):
            intents["tables"]["list"] = True
        nums = [n.replace(",", ".").replace(" ", "") for n in _NUM_IN_TEXT.findall(t)]
        if nums:
            intents["tables"]["describe"] = sorted(set(nums), key=lambda x: [int(p) if p.isdigit() else p for p in re.split(r"[.]", x)])

    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
    if _SOURCES_HINT.search(t):
        intents["sources"]["want"] = True
        if _COUNT_HINT.search(t):
            intents["sources"]["count"] = True
        if _WHICH_HINT.search(t) or "—Å–ø–∏—Å–æ–∫" in t.lower():
            intents["sources"]["list"] = True

    # –†–∏—Å—É–Ω–∫–∏
    if _FIG_ANY.search(t):
        intents["figures"]["want"] = True
        if _COUNT_HINT.search(t):
            intents["figures"]["count"] = True
        if _WHICH_HINT.search(t) or re.search(r"\b(–∫–∞–∫–∏–µ —Ä–∏—Å—É–Ω–∫|—Å–ø–∏—Å–æ–∫ —Ä–∏—Å—É–Ω–∫)\w*\b", t, re.IGNORECASE):
            intents["figures"]["list"] = True
        nums_f = [n.replace(",", ".").strip() for n in _FIG_NUM_IN_TEXT.findall(t)]
        if nums_f:
            def _key(v: str):
                return [int(p) if p.isdigit() else p for p in v.split(".")]
            intents["figures"]["describe"] = sorted(set(nums_f), key=_key)

    # –û—Å—Ç–∞—Ç–æ–∫ –∫–∞–∫ –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å
    intents["general_question"] = t

    logging.debug("INTENTS: %s", json.dumps(intents, ensure_ascii=False))
    return intents


# ---------- –†–∏—Å—É–Ω–∫–∏: –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–ª–æ–∫–∞–ª—å–Ω—ã–µ, –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç retrieval.py) ----------

_FIG_TITLE_RE = re.compile(
    r"(?i)\b(—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|figure|fig\.?)\s*(?:‚Ññ\s*)?(\d+(?:[.,]\d+)*)\b(?:\s*[‚Äî\-‚Äì:\u2013\u2014]\s*(.+))?"
)

def _compose_figure_display(attrs_json: str | None, section_path: str, title_text: str | None) -> str:
    """–î–µ–ª–∞–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∏—Å—É–Ω–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º."""
    num = None
    tail = None
    if attrs_json:
        try:
            a = json.loads(attrs_json or "{}")
            num = (a.get("caption_num") or a.get("label") or "").strip()
            tail = (a.get("caption_tail") or a.get("title") or "").strip()
        except Exception:
            pass

    if not num or not num.strip():
        # –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞/–ø—É—Ç–∏ —Å–µ–∫—Ü–∏–∏
        cand = title_text or section_path or ""
        m = _FIG_TITLE_RE.search(cand)
        if m:
            num = (m.group(2) or "").replace(",", ".").strip()
            if not tail:
                tail = (m.group(3) or "").strip()

    if num:
        return f"–†–∏—Å—É–Ω–æ–∫ {num}" + (f" ‚Äî {_shorten(tail, 160)}" if tail else "")
    # –±–µ–∑ –Ω–æ–º–µ—Ä–∞ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π —Ö–≤–æ—Å—Ç/–Ω–∞–∑–≤–∞–Ω–∏–µ
    base = title_text or _last_segment(section_path or "")
    base = re.sub(r"(?i)^\s*(—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|figure|fig\.?)\s*", "", base).strip(" ‚Äî‚Äì-")
    return _shorten(base or "–†–∏—Å—É–Ω–æ–∫", 160)

def _list_figures_db(uid: int, doc_id: int, limit: int = 25) -> dict:
    """–°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ –ë–î (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏)."""
    con = get_conn()
    cur = con.cursor()
    has_type = _table_has_columns(con, "chunks", ["element_type", "attrs"])

    if has_type:
        cur.execute(
            "SELECT DISTINCT section_path, attrs, text FROM chunks "
            "WHERE owner_id=? AND doc_id=? AND element_type='figure' "
            "ORDER BY id ASC",
            (uid, doc_id),
        )
    else:
        cur.execute(
            "SELECT DISTINCT section_path, attrs, text FROM chunks "
            "WHERE owner_id=? AND doc_id=? AND (text LIKE '[–†–∏—Å—É–Ω–æ–∫]%' OR lower(section_path) LIKE '%—Ä–∏—Å—É–Ω–æ–∫%') "
            "ORDER BY id ASC",
            (uid, doc_id),
        )
    rows = cur.fetchall() or []
    con.close()

    items: list[str] = []
    for r in rows:
        section_path = r["section_path"] or ""
        attrs_json = r["attrs"] if "attrs" in r.keys() else None
        txt = r["text"] or None
        disp = _compose_figure_display(attrs_json, section_path, txt)
        items.append(disp)

    # –¥–µ–¥—É–ø –∏ –æ—Ç—Å–µ—á–∫–∞
    seen = set()
    uniq = []
    for it in items:
        k = it.strip().lower()
        if k and k not in seen:
            seen.add(k)
            uniq.append(it)

    total = len(uniq)
    return {
        "count": total,
        "list": uniq[:limit],
        "more": max(0, total - limit),
    }


# -------------------------- –°–ê–ú–û–í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ò–ù–î–ï–ö–°–ê --------------------------

def _count_et(con, uid: int, doc_id: int, et: str) -> int:
    cur = con.cursor()
    if _table_has_columns(con, "chunks", ["element_type"]):
        cur.execute(
            "SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=? AND element_type=?",
            (uid, doc_id, et),
        )
        row = cur.fetchone()
        return int(row["c"] or 0)
    return 0

def _need_self_heal(uid: int, doc_id: int, need_refs: bool, need_figs: bool) -> tuple[bool, int, int]:
    con = get_conn()
    rc = _count_et(con, uid, doc_id, "reference") if need_refs else 1
    fc = _count_et(con, uid, doc_id, "figure") if need_figs else 1
    con.close()
    return (rc == 0 or fc == 0, rc, fc)

def _reindex_with_sections(uid: int, doc_id: int, sections: list[dict]) -> None:
    delete_document_chunks(doc_id, uid)
    index_document(uid, doc_id, sections)
    invalidate_cache(uid, doc_id)
    set_document_indexer_version(doc_id, CURRENT_INDEXER_VERSION)
    update_document_meta(doc_id, layout_profile=_current_embedding_profile())

async def _ensure_modalities_indexed(m: types.Message, uid: int, doc_id: int, intents: dict):
    """–ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å—Ç–∞—Ä—ã–π –∏ –Ω–µ—Ç reference/figure ‚Äî —Ç–∏—Ö–æ –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–º –Ω–æ–≤—ã–º –ø–∞—Ä—Å–µ—Ä–æ–º –∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º."""
    need_refs = bool(intents.get("sources", {}).get("want"))
    need_figs = bool(intents.get("figures", {}).get("want"))
    if not (need_refs or need_figs):
        return

    should, have_refs, have_figs = _need_self_heal(uid, doc_id, need_refs, need_figs)
    if not should:
        return

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    if not row:
        return

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
    except Exception as e:
        logging.exception("re-parse failed: %s", e)
        return

    # –°–º–æ—Ç—Ä–∏–º, –ø–æ—è–≤–∏–ª–æ—Å—å –ª–∏ —Ç–æ, —á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–ª–æ
    new_refs = sum(1 for s in sections if (s.get("element_type") == "reference"))
    new_figs = sum(1 for s in sections if (s.get("element_type") == "figure"))

    do_reindex = False
    if need_refs and have_refs == 0 and new_refs > 0:
        do_reindex = True
    if need_figs and have_figs == 0 and new_figs > 0:
        do_reindex = True

    if do_reindex:
        try:
            _reindex_with_sections(uid, doc_id, sections)
            await _send(m, "–û–±–Ω–æ–≤–∏–ª –∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞: –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ —Ä–∏—Å—É–Ω–∫–∏/–∏—Å—Ç–æ—á–Ω–∏–∫–∏.")
        except Exception as e:
            logging.exception("self-heal reindex failed: %s", e)


# -------------------------- –°–±–æ—Ä —Ñ–∞–∫—Ç–æ–≤ --------------------------

def _gather_facts(uid: int, doc_id: int, intents: dict) -> dict:
    """
    –°–æ–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –ë–î/–∏–Ω–¥–µ–∫—Å–∞, –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.
    """
    facts: dict[str, object] = {"doc_id": doc_id}

    # ----- –¢–∞–±–ª–∏—Ü—ã -----
    if intents["tables"]["want"]:
        total_tables = _count_tables(uid, doc_id)
        basenames = _distinct_table_basenames(uid, doc_id)

        con = get_conn()
        cur = con.cursor()
        items: list[str] = []
        for base in basenames:
            # attrs –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏ table_row (–≤ –Ω–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–∞—Ö –æ–Ω–∏ —Ç—É—Ç)
            cur.execute(
                """
                SELECT attrs FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                  AND section_path LIKE ? || ' [row %'
                ORDER BY id ASC LIMIT 1
                """,
                (uid, doc_id, base),
            )
            r = cur.fetchone()
            attrs_json = r["attrs"] if r else None

            # –ø–µ—Ä–≤–∞—è/–≤—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî fallback
            cur.execute(
                """
                SELECT text FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                  AND section_path LIKE ? || ' [row %'
                ORDER BY id ASC LIMIT 2
                """,
                (uid, doc_id, base),
            )
            rows = cur.fetchall() or []
            first_row_text = None
            for rr in rows:
                cand = (rr["text"] or "").split("\n")[0]
                cand = " ‚Äî ".join([c.strip() for c in cand.split(" | ") if c.strip()])
                if cand:
                    first_row_text = cand
                    break

            title = _compose_display_from_attrs(attrs_json, base, first_row_text)
            title = _strip_table_prefix(title)
            items.append(title)

        con.close()
        facts["tables"] = {
            "count": total_tables,
            "list": items[:intents["tables"]["limit"]],
            "more": max(0, len(items) - intents["tables"]["limit"]),
            "describe": [],
        }

        # describe –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –Ω–æ–º–µ—Ä–∞–º
        desc_cards = []
        if intents["tables"]["describe"]:
            con = get_conn()
            cur = con.cursor()
            for num in intents["tables"]["describe"]:

                like1 = f'%\"caption_num\": \"{num}\"%'
                like2 = f'%\"label\": \"{num}\"%'
                cur.execute(
                    """
                    SELECT page, section_path, attrs FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                      AND (attrs LIKE ? OR attrs LIKE ?)
                    ORDER BY id ASC LIMIT 1
                    """,
                    (uid, doc_id, like1, like2),
                )
                row = cur.fetchone()

                if not row:
                    cur.execute(
                        """
                        SELECT page, section_path, attrs FROM chunks
                        WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                          AND section_path LIKE ? COLLATE NOCASE
                        ORDER BY id ASC LIMIT 1
                        """,
                        (uid, doc_id, f'%–¢–∞–±–ª–∏—Ü–∞ {num}%'),
                    )
                    row = cur.fetchone()

                if not row:
                    continue

                attrs_json = row["attrs"] if row else None
                # 1‚Äì2 –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ highlights
                cur.execute(
                    """
                    SELECT text FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                      AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 2
                    """,
                    (uid, doc_id, row["section_path"], row["section_path"]),
                )
                rows = cur.fetchall()
                highlights = []
                for r in rows or []:
                    first_line = (r["text"] or "").split("\n")[0]
                    if first_line:
                        highlights.append(" ‚Äî ".join([c.strip() for c in first_line.split(" | ") if c.strip()]))

                base = row["section_path"]
                first_row_text = highlights[0] if highlights else None
                display = _compose_display_from_attrs(attrs_json, base, first_row_text)
                display = _strip_table_prefix(display)

                desc_cards.append({
                    "num": num,
                    "display": display,
                    "where": {"page": row["page"], "section_path": row["section_path"]},
                    "highlights": highlights,
                })
            con.close()

        facts["tables"]["describe"] = desc_cards

    # ----- –†–∏—Å—É–Ω–∫–∏ (–ù–û–í–û–ï) -----
    if intents["figures"]["want"]:
        lst = _list_figures_db(uid, doc_id, limit=intents["figures"]["limit"])
        figs_block = {
            "count": int(lst.get("count") or 0),
            "list": list(lst.get("list") or []),
            "more": int(lst.get("more") or 0),
            "describe_lines": [],
        }

        # –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –Ω–æ–º–µ—Ä–∞–º —á–µ—Ä–µ–∑ vision
        if intents["figures"]["describe"]:
            try:
                desc_text = vision_describe_figures(uid, doc_id, intents["figures"]["describe"])
                lines = [ln.strip() for ln in (desc_text or "").splitlines() if ln.strip()]
                figs_block["describe_lines"] = lines[:25]
            except Exception as e:
                figs_block["describe_lines"] = [f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤: {e}"]

        facts["figures"] = figs_block

    # ----- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ -----
    if intents["sources"]["want"]:
        con = get_conn()
        cur = con.cursor()
        has_type = _table_has_columns(con, "chunks", ["element_type", "attrs"])
        items: list[str] = []

        # 1) –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ reference-—á–∞–Ω–∫–∏
        if has_type:
            cur.execute(
                "SELECT text FROM chunks WHERE owner_id=? AND doc_id=? AND element_type='reference' ORDER BY id ASC",
                (uid, doc_id),
            )
            items = [(r["text"] or "").strip() for r in cur.fetchall()]

        # 2) —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ñ–æ–ª–±—ç–∫ –ø–æ —Å–µ–∫—Ü–∏–∏ (–±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–∏)
        if not any(items):
            cur.execute(
                """
                SELECT element_type, section_path, text
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                ORDER BY page ASC, id ASC
                """,
                (uid, doc_id),
            )
            raw = []
            for r in cur.fetchall():
                sec = (r["section_path"] or "").lower()
                if not any(k in sec for k in ("–∏—Å—Ç–æ—á–Ω–∏–∫", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ", "reference", "bibliograph")):
                    continue
                et = (r["element_type"] or "").lower()
                if et in ("heading", "table", "figure", "table_row"):
                    continue
                t = (r["text"] or "").strip()
                if t:
                    raw.append(t)

            # –¥–µ–¥—É–ø –∏ –ª—ë–≥–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            seen = set()
            items = []
            for t in raw:
                k = re.sub(r"\s+", " ", t).strip().rstrip(".").lower()
                if len(k) < 5 or k in seen:
                    continue
                seen.add(k)
                items.append(t)

        con.close()

        facts["sources"] = {
            "count": len(items),
            "list": items[:intents["sources"]["limit"]],
            "more": max(0, len(items) - intents["sources"]["limit"]),
        }

    # ----- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å -----
    if intents.get("practical"):
        facts["practical_present"] = _has_practical_part(uid, doc_id)

    # ----- Summary -----
    if intents.get("summary"):
        s = overview_context(uid, doc_id, max_chars=6000) or _first_chunks_context(uid, doc_id, n=12, max_chars=6000)
        if s:
            facts["summary_text"] = s

    # ----- –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç / —Ü–∏—Ç–∞—Ç—ã -----
    if intents.get("general_question"):
        vb = verbatim_find(uid, doc_id, intents["general_question"], max_hits=3)
        ctx = best_context(uid, doc_id, intents["general_question"], max_chars=6000)
        if not ctx:
            hits = retrieve(uid, doc_id, intents["general_question"], top_k=8)
            if hits:
                ctx = build_context(hits)
        if not ctx:
            ctx = _first_chunks_context(uid, doc_id, n=10, max_chars=6000)
        if ctx:
            facts["general_ctx"] = ctx
        if vb:
            facts["verbatim_hits"] = vb

    # –ª–æ–≥–∏—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Å—Ä–µ–∑ —Ñ–∞–∫—Ç–æ–≤ (–±–µ–∑ –æ–≥—Ä–æ–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤)
    log_snapshot = dict(facts)
    if "general_ctx" in log_snapshot and isinstance(log_snapshot["general_ctx"], str):
        log_snapshot["general_ctx"] = log_snapshot["general_ctx"][:300] + "‚Ä¶" if len(log_snapshot["general_ctx"]) > 300 else log_snapshot["general_ctx"]
    if "summary_text" in log_snapshot and isinstance(log_snapshot["summary_text"], str):
        log_snapshot["summary_text"] = log_snapshot["summary_text"][:300] + "‚Ä¶" if len(log_snapshot["summary_text"]) > 300 else log_snapshot["summary_text"]
    logging.debug("FACTS: %s", json.dumps(log_snapshot, ensure_ascii=False))
    return facts


_RULES_MD = (
    "1) –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º, –∑–∞–∫—Ä–æ–π –≤—Å–µ –ø–æ–¥–ø—É–Ω–∫—Ç—ã –≤–æ–ø—Ä–æ—Å–∞.\n"
    "2) –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü: –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–º–µ—Ä ‚Üí ¬´–¢–∞–±–ª–∏—Ü–∞ N ‚Äî –ù–∞–∑–≤–∞–Ω–∏–µ¬ª; –µ—Å–ª–∏ –Ω–æ–º–µ—Ä–∞ –Ω–µ—Ç ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ.\n"
    "3) –ù–µ –≤—ã–≤–æ–¥–∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ —Ä–∞–∑–º–µ—Ä—ã (–Ω–∏–∫–∞–∫–∏—Ö [–¢–∞–±–ª–∏—Ü–∞], ¬´—Ä—è–¥ 1¬ª, ¬´(6√ó7)¬ª).\n"
    "4) –í —Å–ø–∏—Å–∫–∞—Ö –ø–æ–∫–∞–∂–∏ –Ω–µ –±–æ–ª–µ–µ 25 —Å—Ç—Ä–æ–∫, –∑–∞—Ç–µ–º ¬´‚Ä¶ –∏ –µ—â—ë M¬ª, –µ—Å–ª–∏ –µ—Å—Ç—å.\n"
    "5) –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã –≤–Ω–µ –±–ª–æ–∫–∞ Facts; –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏ —á–µ—Å—Ç–Ω–æ.\n"
)

def _compose_answer(question: str, facts: dict, lang: str = "ru") -> str:
    """–ì–æ—Ç–æ–≤–∏–º markdown-–∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Å–∏–º –µ—ë –∫—Ä–∞—Å–∏–≤–æ ¬´—Å—à–∏—Ç—å¬ª –æ—Ç–≤–µ—Ç."""
    def md_list(arr: list[str], max_show: int, more: int | None) -> str:
        out = []
        for x in (arr or [])[:max_show]:
            out.append(f"- {x}")
        if more and more > 0:
            out.append(f"‚Ä¶ –∏ –µ—â—ë {more}")
        return "\n".join(out)

    parts = []

    # –¢–∞–±–ª–∏—Ü—ã
    tables = facts.get("tables") or {}
    if tables:
        block = []
        if "count" in tables:
            block.append(f"count: {tables.get('count', 0)}")
        if tables.get("list"):
            block.append("list:\n" + md_list(tables["list"], 25, tables.get("more", 0)))
        if tables.get("describe"):
            cards = []
            for c in tables["describe"]:
                cards.append({
                    "num": c.get("num"),
                    "display": c.get("display"),
                    "where": c.get("where"),
                    "highlights": c.get("highlights", [])[:2],
                })
            block.append("describe:\n" + json.dumps(cards, ensure_ascii=False, indent=2))
        parts.append("- Tables:\n  " + "\n  ".join(block))

    # –†–∏—Å—É–Ω–∫–∏ (–ù–û–í–û–ï)
    figures = facts.get("figures") or {}
    if figures:
        block = []
        block.append(f"count: {figures.get('count', 0)}")
        if figures.get("list"):
            block.append("list:\n" + md_list(figures["list"], 25, figures.get("more", 0)))
        if figures.get("describe_lines"):
            lines = "\n".join([f"- {ln}" for ln in figures["describe_lines"]])
            block.append("describe:\n" + lines)
        parts.append("- Figures:\n  " + "\n  ".join(block))

    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
    sources = facts.get("sources") or {}
    if sources:
        block = []
        block.append(f"count: {sources.get('count', 0)}")
        if sources.get("list"):
            block.append("list:\n" + md_list(sources["list"], 25, sources.get("more", 0)))
        parts.append("- Sources:\n  " + "\n  ".join(block))

    # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å
    if "practical_present" in facts:
        parts.append(f"- PracticalPartPresent: {bool(facts['practical_present'])}")

    # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (–µ—Å–ª–∏ –ø—Ä–æ—Å–∏–ª–∏)
    if "summary_text" in facts:
        parts.append("- Summary:\n  " + (facts["summary_text"][:1200] + ("‚Ä¶" if len(facts["summary_text"]) > 1200 else "")).replace("\n", "\n  "))

    # –í–µ—Ä–±–∞—Ç–∏–º-—Ü–∏—Ç–∞—Ç—ã
    if facts.get("verbatim_hits"):
        hits_md = []
        for h in facts["verbatim_hits"]:
            page = h.get('page')
            sec = (h.get('section_path') or "").strip()
            page_str = (str(page) if page is not None else "?")
            where = f'–≤ —Ä–∞–∑–¥–µ–ª–µ ¬´{sec}¬ª, —Å—Ç—Ä. {page_str}' if sec else f'–Ω–∞ —Å—Ç—Ä. {page_str}'
            hits_md.append(f"- Match {where}: ¬´{h['snippet']}¬ª")
        parts.append("- Citations:\n  " + "\n  ".join(hits_md))

    # –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å)
    if "general_ctx" in facts:
        parts.append("- Context:\n  " + (facts["general_ctx"][:1500] + ("‚Ä¶" if len(facts["general_ctx"]) > 1500 else "")).replace("\n", "\n  "))

    facts_md = "[Facts]\n" + "\n".join(parts) + "\n\n[Rules]\n" + _RULES_MD

    reply = ace_once(question, facts_md)
    return reply


# ------------------------------ FULLREAD: –º–æ–¥–µ–ª—å —á–∏—Ç–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª ------------------------------

def _full_document_text(owner_id: int, doc_id: int, *, limit_chars: int | None = None) -> str:
    """
    –°–∫–ª–µ–∏–≤–∞–µ–º –í–ï–°–¨ —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü—ã chunks, –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (page ASC, id ASC).
    –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω limit_chars ‚Äî –æ–±—Ä–µ–∑–∞–µ–º –ø–æ –ª–∏–º–∏—Ç—É.
    """
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT text FROM chunks WHERE owner_id=? AND doc_id=? ORDER BY page ASC, id ASC",
        (owner_id, doc_id),
    )
    rows = cur.fetchall() or []
    con.close()

    parts = []
    total = 0
    for r in rows:
        t = (r["text"] or "").strip()
        if not t:
            continue
        if limit_chars is not None and total + len(t) > limit_chars:
            remaining = max(0, limit_chars - total)
            if remaining > 0:
                parts.append(t[:remaining])
                total += remaining
            break
        parts.append(t)
        total += len(t)
    return "\n\n".join(parts)

def _fullread_try_answer(uid: int, doc_id: int, q_text: str) -> str | None:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –º–æ–¥–µ–ª–∏ (—Ä–µ–∂–∏–º FULLREAD_MODE=direct).
    –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None (–ø–µ—Ä–µ–π–¥—ë–º –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É RAG-–ø–∞–π–ø–ª–∞–π–Ω—É).
    """
    if (Cfg.FULLREAD_MODE or "off") != "direct":
        return None

    # –ë–µ—Ä—ë–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –≤–µ—Ä—Ö–Ω–∏–º –ª–∏–º–∏—Ç–æ–º (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
    full_text = _full_document_text(uid, doc_id, limit_chars=Cfg.DIRECT_MAX_CHARS + 1)
    if not full_text.strip():
        return None

    # –ï—Å–ª–∏ –Ω–µ –≤–ª–µ–∑–∞–µ–º –≤ –ª–∏–º–∏—Ç ‚Äî —É—Ö–æ–¥–∏–º –≤ –æ–±—ã—á–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
    if len(full_text) > Cfg.DIRECT_MAX_CHARS:
        return None

    system_prompt = (
        "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –¢–µ–±–µ –¥–∞–Ω –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç –í–ö–†/–¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
        "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ–≤. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.\n"
        "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–¥–ø–∏—Å–∏ –∏ —Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º; –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞/–∑–Ω–∞—á–µ–Ω–∏—è.\n"
        "–¶–∏—Ç–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
    )

    # –ü–µ—Ä–µ–¥–∞–¥–∏–º –¥–æ–∫—É–º–µ–Ω—Ç –µ–¥–∏–Ω–æ–π ¬´assistant¬ª-—Ä–µ–ø–ª–∏–∫–æ–π (–∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç),
    # –∑–∞—Ç–µ–º –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ‚Äî —ç—Ç–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∏ –æ–Ω —É–∂–µ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –≤ summarizer.
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": f"[–î–æ–∫—É–º–µ–Ω—Ç ‚Äî –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç]\n{full_text}"},
        {"role": "user", "content": q_text},
    ]

    try:
        answer = chat_with_gpt(messages, temperature=0.2, max_tokens=900)
        return (answer or "").strip() or None
    except Exception as e:
        logging.exception("fullread direct failed: %s", e)
        return None


# ------------------------------ DIAGNOSTICS ------------------------------

def _json_obj(x):
    if not x:
        return {}
    if isinstance(x, dict):
        return x
    try:
        return json.loads(x)
    except Exception:
        return {}

def _diagnostics_text(uid: int, doc_id: int) -> str:
    con = get_conn()
    cur = con.cursor()

    # path + indexer_version
    cur.execute("SELECT path, indexer_version FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    drow = cur.fetchone()
    path = (drow["path"] if drow else None) or "?"
    idx_ver = int((drow["indexer_version"] or 0) if drow else 0)

    # totals
    cur.execute("SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=?", (uid, doc_id))
    total = int(cur.fetchone()["c"] or 0)

    # groups
    cur.execute("""
        SELECT COALESCE(element_type,'NULL') AS et, COUNT(*) AS c
        FROM chunks WHERE owner_id=? AND doc_id=? GROUP BY et ORDER BY c DESC
    """, (uid, doc_id))
    groups = [(r["et"], int(r["c"])) for r in cur.fetchall()]

    # figures sample
    cur.execute("""
        SELECT page, section_path, text, attrs FROM chunks
        WHERE owner_id=? AND doc_id=? AND element_type='figure'
        ORDER BY id ASC LIMIT 5
    """, (uid, doc_id))
    figs = cur.fetchall() or []

    # references sample
    cur.execute("""
        SELECT page, section_path, text, attrs FROM chunks
        WHERE owner_id=? AND doc_id=? AND element_type='reference'
        ORDER BY id ASC LIMIT 5
    """, (uid, doc_id))
    refs = cur.fetchall() or []

    # tables sample (base names)
    cur.execute("""
        SELECT DISTINCT
            CASE WHEN instr(section_path, ' [row ')>0
                 THEN substr(section_path, 1, instr(section_path,' [row ')-1)
            ELSE section_path END AS base_name
        FROM chunks
        WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
        LIMIT 8
    """, (uid, doc_id))
    tbls = [r["base_name"] for r in cur.fetchall() if r["base_name"]]

    con.close()

    lines = []
    lines.append(f"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ #{doc_id}")
    lines.append(f"–ü—É—Ç—å: {path}")
    lines.append(f"–í–µ—Ä—Å–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: {idx_ver} (–∞–∫—Ç—É–∞–ª—å–Ω–∞—è: {CURRENT_INDEXER_VERSION})")
    lines.append(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total}")
    if groups:
        lines.append("–ü–æ element_type:")
        for et, cnt in groups:
            lines.append(f"‚Äî {et}: {cnt}")

    if tbls:
        lines.append("\n–ü—Ä–∏–º–µ—Ä—ã —Ç–∞–±–ª–∏—Ü (base):")
        for t in tbls:
            lines.append(f"‚Ä¢ {t}")

    if figs:
        lines.append("\n–ü—Ä–∏–º–µ—Ä—ã —Ä–∏—Å—É–Ω–∫–æ–≤:")
        for r in figs:
            a = _json_obj(r["attrs"])
            imgs = a.get("images") or []
            tail = a.get("caption_tail") or a.get("title")
            num = a.get("caption_num") or a.get("label")
            lines.append(f"‚Ä¢ {r['section_path']} | num={num} | tail={tail} | images={len(imgs)}")

    if refs:
        lines.append("\n–ü—Ä–∏–º–µ—Ä—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:")
        for r in refs:
            a = _json_obj(r["attrs"])
            idx = a.get("ref_index")
            text = (r["text"] or "").strip()
            if len(text) > 200:
                text = text[:199] + "‚Ä¶"
            lines.append(f"‚Ä¢ [{idx}] {text}")

    return "\n".join(lines)


@dp.message(Command("diag"))
async def cmd_diag(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid)
    if not doc_id:
        await _send(m, "–ê–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ—Ç. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª —Å–Ω–∞—á–∞–ª–∞.")
        return
    txt = _diagnostics_text(uid, doc_id)
    await _send(m, txt)


@dp.message(Command("reindex"))
async def cmd_reindex(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid)
    if not doc_id:
        await _send(m, "–°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª (–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ—Ç).")
        return

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    if not row:
        await _send(m, "–ù–µ –Ω–∞—à—ë–ª –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–æ–∫—É–º–µ–Ω—Ç–∞.")
        return

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
    except Exception as e:
        await _send(m, f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–ø–∞—Ä—Å–∏—Ç—å: {e}")
        return

    try:
        _reindex_with_sections(uid, doc_id, sections)
        await _send(m, "–î–æ–∫—É–º–µ–Ω—Ç –ø–µ—Ä–µ–ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—É—â–∏–º –ø–∞—Ä—Å–µ—Ä–æ–º. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —Ä–∏—Å—É–Ω–∫–∏/–∏—Å—Ç–æ—á–Ω–∏–∫–∏.")
    except Exception as e:
        await _send(m, f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")


@dp.message(Command("doc"))
async def cmd_doc(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid)
    if not doc_id:
        await _send(m, "–ê–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ—Ç.")
        return
    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path, indexer_version FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    p = row["path"] if row else "?"
    v = int((row["indexer_version"] or 0) if row else 0)
    await _send(m, f"–ê–∫—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: #{doc_id}\n–ü—É—Ç—å: {p}\n–í–µ—Ä—Å–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: {v} (—Ç–µ–∫—É—â–∞—è {CURRENT_INDEXER_VERSION})")


# ------------------------------ –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç—á–∏–∫ ------------------------------

async def respond_with_answer(m: types.Message, uid: int, doc_id: int, q_text: str):
    q_text = (q_text or "").strip()
    logging.debug(f"–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {q_text}")
    if not q_text:
        await _send(m, "–í–æ–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π. –ù–∞–ø–∏—à–∏—Ç–µ, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –ø–æ –í–ö–†.")
        return

    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å ‚Äî –≤—Å–µ–≥–¥–∞
    viol = safety_check(q_text)
    if viol:
        await _send(m, viol + " –ó–∞–¥–∞–π—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ –í–ö–†.")
        return

    # –ì–û–°–¢-–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É
    if await _maybe_run_gost(m, uid, doc_id, q_text):
        return

    # ====== NEW: —Ä–µ–∂–∏–º, –≥–¥–µ –º–æ–¥–µ–ª—å —á–∏—Ç–∞–µ—Ç –í–ï–°–¨ —Ñ–∞–π–ª (FULLREAD_MODE=direct) ======
    if (Cfg.FULLREAD_MODE or "off") == "direct":
        fr_answer = _fullread_try_answer(uid, doc_id, q_text)
        if fr_answer:
            await _send(m, fr_answer)
            return
        # –∏–Ω–∞—á–µ —Ç–∏—Ö–æ –ø–∞–¥–∞–µ–º –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RAG/lexsearch

    # –ï–¥–∏–Ω—ã–π –º—É–ª—å—Ç–∏-–∏–Ω—Ç–µ–Ω—Ç –ø–∞–π–ø–ª–∞–π–Ω (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º)
    intents = _detect_intents(q_text)

    # >>> –°–∞–º–æ–∏—Å—Ü–µ–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –ø–æ–¥ –∑–∞–ø—Ä–æ—Å (–µ—Å–ª–∏ —Å—Ç–∞—Ä—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –±–µ–∑ figures/reference)
    await _ensure_modalities_indexed(m, uid, doc_id, intents)

    facts = _gather_facts(uid, doc_id, intents)
    reply = _compose_answer(q_text, facts, lang=intents.get("language", "ru"))
    await _send(m, reply)


# ------------------------------ —ç–º–±–µ–¥–¥–∏–Ω–≥-–ø—Ä–æ—Ñ–∏–ª—å ------------------------------

def _current_embedding_profile() -> str:
    dim = probe_embedding_dim(None)
    if dim:
        return f"emb={Cfg.POLZA_EMB}|dim={dim}"
    return f"emb={Cfg.POLZA_EMB}"

def _needs_reindex_by_embeddings(con, doc_id: int) -> bool:
    if not _table_has_columns(con, "documents", ["layout_profile"]):
        return True
    cur = con.cursor()
    cur.execute("SELECT layout_profile FROM documents WHERE id=?", (doc_id,))
    row = cur.fetchone()
    stored = (row["layout_profile"] or "") if row else ""
    if not stored:
        return True
    cur_model = Cfg.POLZA_EMB.strip().lower()
    stored_model = ""
    stored_dim = None
    for part in stored.split("|"):
        part = (part or "").strip().lower()
        if part.startswith("emb="):
            stored_model = part[4:]
        if part.startswith("dim="):
            try:
                stored_dim = int(part[4:])
            except Exception:
                stored_dim = None
    if stored_model and stored_model != cur_model:
        return True
    cur_dim = probe_embedding_dim(None)
    if cur_dim and stored_dim and stored_dim != cur_dim:
        return True
    return False


# ------------------------------ –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ ------------------------------

@dp.message(F.document)
async def handle_doc(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc = m.document

    # 1) —Å–∫–∞—á–∏–≤–∞–µ–º
    file = await bot.get_file(doc.file_id)
    stream = await bot.download_file(file.file_path)
    data = stream.read()
    stream.close()

    # 2) –¥–µ–¥—É–ø –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É + file_unique_id
    sha256 = sha256_bytes(data)
    file_uid = getattr(doc, "file_unique_id", None)

    con = get_conn()
    existing_id = _find_existing_doc(con, uid, sha256, file_uid)

    if existing_id:
        existing_ver = get_document_indexer_version(existing_id) or 0

        need_reindex = False
        try:
            if _needs_reindex_by_embeddings(con, existing_id):
                need_reindex = True
        except Exception:
            need_reindex = True

        if existing_ver < CURRENT_INDEXER_VERSION:
            need_reindex = True

        if need_reindex:
            filename = safe_filename(f"{m.from_user.id}_{doc.file_name}")
            path = save_upload(data, filename, Cfg.UPLOAD_DIR)
            update_document_meta(existing_id, path=path, content_sha256=sha256, file_uid=file_uid)
            con.close()

            try:
                sections = _parse_by_ext(path)
                if sum(len(s.get("text") or "") for s in sections) < 500:
                    await _send(m, "–ü–æ—Ö–æ–∂–µ, —Ñ–∞–π–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ —ç—Ç–æ —Å–∫–∞–Ω-PDF –±–µ–∑ —Ç–µ–∫—Å—Ç–∞.")
                    return

                delete_document_chunks(existing_id, uid)
                index_document(uid, existing_id, sections)
                invalidate_cache(uid, existing_id)

                set_document_indexer_version(existing_id, CURRENT_INDEXER_VERSION)
                update_document_meta(existing_id, layout_profile=_current_embedding_profile())

            except Exception as e:
                await _send(m, f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç #{existing_id}: {e}")
                return

            ACTIVE_DOC[uid] = existing_id
            caption = (m.caption or "").strip()
            await _send(m, f"–î–æ–∫—É–º–µ–Ω—Ç #{existing_id} –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω. –ì–æ—Ç–æ–≤ –æ—Ç–≤–µ—á–∞—Ç—å.")
            if caption:
                await respond_with_answer(m, uid, existing_id, caption)
            return

        con.close()
        ACTIVE_DOC[uid] = existing_id
        caption = (m.caption or "").strip()
        await _send(m, f"–≠—Ç–æ—Ç —Ñ–∞–π–ª —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ä–∞–Ω–µ–µ –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç #{existing_id}. –ò—Å–ø–æ–ª—å–∑—É—é –µ–≥–æ.")
        if caption:
            await respond_with_answer(m, uid, existing_id, caption)
        return

    # 3) —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    filename = safe_filename(f"{m.from_user.id}_{doc.file_name}")
    path = save_upload(data, filename, Cfg.UPLOAD_DIR)

    # 4) –ø–∞—Ä—Å–∏–º
    try:
        sections = _parse_by_ext(path)
    except Exception as e:
        await _send(m, f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")
        return

    # 5) –ø—É—Å—Ç–æ–π/—Å–∫–∞–Ω
    if sum(len(s.get("text") or "") for s in sections) < 500:
        await _send(m, "–ü–æ—Ö–æ–∂–µ, —Ñ–∞–π–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ —ç—Ç–æ —Å–∫–∞–Ω-PDF –±–µ–∑ —Ç–µ–∫—Å—Ç–∞. "
                       "–ó–∞–≥—Ä—É–∑–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, DOC/DOCX –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π PDF.")
        return

    # 6) –¥–æ–∫—É–º–µ–Ω—Ç ‚Üí –ë–î –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    kind = infer_doc_kind(doc.file_name)
    doc_id = _insert_document(con, uid, kind, path, sha256, file_uid)
    con.close()

    index_document(uid, doc_id, sections)
    invalidate_cache(uid, doc_id)
    set_document_indexer_version(doc_id, CURRENT_INDEXER_VERSION)
    update_document_meta(doc_id, layout_profile=_current_embedding_profile())

    ACTIVE_DOC[uid] = doc_id

    caption = (m.caption or "").strip()
    if caption:
        await _send(m, f"–î–æ–∫—É–º–µ–Ω—Ç #{doc_id} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω. –û—Ç–≤–µ—á–∞—é –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–∑ –ø–æ–¥–ø–∏—Å–∏‚Ä¶")
        await respond_with_answer(m, uid, doc_id, caption)
    else:
        await _send(m, f"–ì–æ—Ç–æ–≤–æ. –î–æ–∫—É–º–µ–Ω—Ç #{doc_id} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω. –ú–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ä–∞–±–æ—Ç–µ.")


# ------------------------------ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ------------------------------

@dp.message(F.text & ~F.via_bot & ~F.text.startswith("/"))
async def qa(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid)
    text = (m.text or "").strip()

    if not doc_id:
        # –ú—è–≥–∫–∞—è –ø–æ–¥—Å–∫–∞–∑–∫–∞, –Ω–æ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        hint = topical_check(text)
        if hint:
            await _send(m, hint)
        reply = agent_no_context(text)
        await _send(m, reply)
        return

    await respond_with_answer(m, uid, doc_id, text)