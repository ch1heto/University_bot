# app/bot.py
import re
import os
import html
import json
import logging
import asyncio
import time
import math
from decimal import Decimal 
logger = logging.getLogger(__name__)
from typing import Iterable, AsyncIterable, Optional, List, Tuple
from .docs_handlers import register_docs_handlers
from aiogram import Bot, Dispatcher, F, types
from aiogram.filters import Command
from aiogram.exceptions import TelegramBadRequest
from aiogram.enums import ChatAction
from aiogram.types import FSInputFile, InputMediaPhoto

from .ooxml_lite import (
    build_index as oox_build_index,
    figure_lookup as oox_fig_lookup,
    table_lookup as oox_tbl_lookup,
)

# ---------- answer builder: –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å —Å—Ç—Ä–∏–º–æ–≤—É—é –≤–µ—Ä—Å–∏—é, —Ñ–æ–ª–±—ç–∫ –Ω–∞ –Ω–µ—Å—Ç—Ä–∏–º–æ–≤—É—é ----------
try:
    from .answer_builder import generate_answer, generate_answer_stream  # type: ignore
except Exception:
    from .answer_builder import generate_answer  # type: ignore
    generate_answer_stream = None  # —Å—Ç—Ä–∏–º–∞ –Ω–µ—Ç ‚Äî –±—É–¥–µ–º —Ñ–æ–ª–±—ç–∫–∞—Ç—å

from .config import Cfg, ProcessingState
from .db import (
    ensure_user, get_conn,
    set_document_indexer_version, get_document_indexer_version,
    CURRENT_INDEXER_VERSION,
    update_document_meta, delete_document_chunks,
    set_user_active_doc, get_user_active_doc,
    # ‚Üì –Ω–æ–≤–æ–µ –¥–ª—è FSM/–æ—á–µ—Ä–µ–¥–∏
    enqueue_pending_query, dequeue_all_pending_queries,
    get_processing_state, start_downloading,
)
from .parsing import parse_docx, parse_doc, save_upload
from .indexing import index_document
from .retrieval import (
    retrieve, build_context, invalidate_cache,
    retrieve_coverage, build_context_coverage,
    describe_figures_by_numbers,
)
from .intents import detect_intents

# ---------- polza client: –ø—Ä–æ–±—É–µ–º —Å—Ç—Ä–∏–º, —Ñ–æ–ª–±—ç–∫ –Ω–∞ –æ–±—ã—á–Ω—ã–π —á–∞—Ç ----------
try:
    from .polza_client import (
        probe_embedding_dim,
        chat_with_gpt,
        chat_with_gpt_stream,
        vision_extract_values,
        vision_extract_table_values,      # ‚Üê –ù–û–í–û–ï: —Å–ø–µ—Ü-—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–∞–±–ª–∏—Ü-–∫–∞—Ä—Ç–∏–Ω–æ–∫
        # NEW: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –æ–±—ë—Ä—Ç–∫–∏ (—Ç–µ–∫—Å—Ç + –∫–∞—Ä—Ç–∏–Ω–∫–∏)
        chat_with_gpt_multimodal,
        chat_with_gpt_stream_multimodal,
    )  # type: ignore

    # NEW: –ø—Ä—è–º–æ–π –∏–Ω–¥–µ–∫—Å —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
    from .figures import (
        index_document as fig_index_document,
        load_index   as fig_load_index,
        find_figure  as fig_find,
        figure_display_name,
    )
except Exception:
    from .polza_client import probe_embedding_dim, chat_with_gpt  # type: ignore
    chat_with_gpt_stream = None
    vision_extract_values = None  # —Ñ–æ–ª–±—ç–∫: –µ—Å–ª–∏ –Ω–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏, –Ω–µ –ø–∞–¥–∞–µ–º

    # ‚Üê –ù–û–í–û–ï: –µ—Å–ª–∏ —Å–ø–µ—Ü-—Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü-–∫–∞—Ä—Ç–∏–Ω–æ–∫ –Ω–µ—Ç, –ø—Ä–æ—Å—Ç–æ –æ—Ç–∫–ª—é—á–∞–µ–º –µ—ë
    vision_extract_table_values = None  # type: ignore

    # NEW: –º—è–≥–∫–∏–µ —Ñ–æ–ª–±—ç–∫–∏
    chat_with_gpt_multimodal = None  # type: ignore
    chat_with_gpt_stream_multimodal = None  # type: ignore


    # –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è figures, —á—Ç–æ–±—ã –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –Ω–µ –ø–∞–¥–∞–ª
    fig_index_document = None       # type: ignore
    fig_load_index = None           # type: ignore

    def fig_find(*args, **kwargs):  # type: ignore
        return []

    def figure_display_name(rec):   # type: ignore
        rec = rec or {}
        return str(
            rec.get("title")
            or rec.get("caption")
            or rec.get("num")
            or "–†–∏—Å—É–Ω–æ–∫"
        )




# –ù–û–í–û–ï: –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –ø—Ä–∏—ë–º–∞/–æ–±–æ–≥–∞—â–µ–Ω–∏—è (OCR —Ç–∞–±–ª–∏—Ü-–∫–∞—Ä—Ç–∏–Ω–æ–∫, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–µ–ª)
from .ingest_orchestrator import enrich_sections, ingest_document
# –ù–û–í–û–ï: –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ç–∞–±–ª–∏—Ü
from .analytics import analyze_table_by_num

# —É—Ç–∏–ª–∏—Ç—ã
from .utils import safe_filename, sha256_bytes, split_for_telegram, infer_doc_kind

# –≥–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: —Å–µ–º–∞–Ω—Ç–∏–∫–∞ + FTS/LIKE
from .lexsearch import best_context

# —Å—Ä–∞–∑—É –ø–æ–¥ —Ç–µ–∫—É—â–∏–º–∏ import‚Äô–∞–º–∏
from .paywall_stub import setup_paywall

# –≥–¥–µ —É –≤–∞—Å —Å–æ–∑–¥–∞—é—Ç—Å—è –æ–±—ä–µ–∫—Ç—ã –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞:
bot = Bot(Cfg.TG_TOKEN)
dp = Dispatcher()
# –¥–æ–±–∞–≤—å—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É (–æ–¥–∏–Ω —Ä–∞–∑):
setup_paywall(dp, bot)

register_docs_handlers(dp)

# --------------------- –ü–ê–†–ê–ú–ï–¢–†–´ –°–¢–†–ò–ú–ò–ù–ì–ê (—Å –¥–µ—Ñ–æ–ª—Ç–∞–º–∏) ---------------------

STREAM_ENABLED: bool = getattr(Cfg, "STREAM_ENABLED", True)
STREAM_EDIT_INTERVAL_MS: int = getattr(Cfg, "STREAM_EDIT_INTERVAL_MS", 900)
STREAM_MIN_CHARS: int = getattr(Cfg, "STREAM_MIN_CHARS", 120)
STREAM_MODE: str = getattr(Cfg, "STREAM_MODE", "edit")
TG_MAX_CHARS: int = getattr(Cfg, "TG_MAX_CHARS", 3900)
FIG_MEDIA_LIMIT: int = getattr(Cfg, "FIG_MEDIA_LIMIT", 12)

TG_SPLIT_TARGET: int = getattr(Cfg, "TG_SPLIT_TARGET", 2000)
TG_SPLIT_MAX_PARTS: int = getattr(Cfg, "TG_SPLIT_MAX_PARTS", 6)

# ‚Üì –ù–û–í–û–ï: –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∫—É—Å–∫–∞–º–∏ –ø—Ä–∏ –Ω–µ—Å—Ç—Ä–∏–º–æ–≤–æ–π –æ—Ç–ø—Ä–∞–≤–∫–µ
MULTIPART_SLEEP_MS: int = getattr(Cfg, "MULTIPART_SLEEP_MS", 200)

_SPLIT_ANCHOR_RE = re.compile(
    r"(?m)^(?:### .+|## .+|\*\*[^\n]+?\*\*|\d+[).] .+|- .+)$"
)  # –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã (–∑–∞–≥–æ–ª–æ–≤–∫–∏/—Å–ø–∏—Å–∫–∏)
STREAM_HEAD_START_MS: int = getattr(Cfg, "STREAM_HEAD_START_MS", 250)        # –ø–µ—Ä–≤—ã–π –∞–ø–¥–µ–π—Ç –±—ã—Å—Ç—Ä–µ–µ
FINAL_MAX_TOKENS: int = getattr(Cfg, "FINAL_MAX_TOKENS", 5000)
TYPE_INDICATION_EVERY_MS: int = getattr(Cfg, "TYPE_INDICATION_EVERY_MS", 2000)
# NEW: —Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º –¥–ª—è —Ä–∏—Å—É–Ω–∫–æ–≤ ‚Äî –Ω–µ –æ—Ç–¥–∞—ë–º —á–∏—Å–ª–∞ –±–µ–∑ –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
FIG_STRICT: bool = getattr(Cfg, "FIG_STRICT", True)
# ‚Üì –Ω–æ–≤–æ–µ: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–π –ø–æ–¥–∞—á–µ–π
MULTI_STEP_SEND_ENABLED: bool = getattr(Cfg, "MULTI_STEP_SEND_ENABLED", True)
MULTI_STEP_MIN_ITEMS: int = getattr(Cfg, "MULTI_STEP_MIN_ITEMS", 2)
MULTI_STEP_MAX_ITEMS: int = getattr(Cfg, "MULTI_STEP_MAX_ITEMS", 8)
MULTI_STEP_FINAL_MERGE: bool = getattr(Cfg, "MULTI_STEP_FINAL_MERGE", True)
MULTI_STEP_PAUSE_MS: int = getattr(Cfg, "MULTI_STEP_PAUSE_MS", 120)  # –º/—É –±–ª–æ–∫–∞–º–∏
MULTI_PASS_SCORE: int = getattr(Cfg, "MULTI_PASS_SCORE", 85)         # –ø–æ—Ä–æ–≥ –∫—Ä–∏—Ç–∏–∫–∞ –≤ ace
# –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π –≤–æ–æ–±—â–µ –µ—Å—Ç—å —Å–º—ã—Å–ª –≥–æ—Ä–æ–¥–∏—Ç—å –ø–æ–¥–ø—É–Ω–∫—Ç—ã
MULTI_STEP_MIN_QUESTION_LEN: int = getattr(Cfg, "MULTI_STEP_MIN_QUESTION_LEN", 200)


# --------------------- —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ ---------------------
# Markdown ‚Üí HTML (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ-–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ: –∑–∞–≥–æ–ª–æ–≤–∫–∏, **bold**, *italic*, `code`)
_MD_H_RE       = re.compile(r"(?m)^\s{0,3}#{1,6}\s+(.+?)\s*$")
_MD_BOLD_RE    = re.compile(r"\*\*(.+?)\*\*", re.DOTALL)
_MD_BOLD2_RE   = re.compile(r"__(.+?)__", re.DOTALL)
_MD_ITALIC_RE  = re.compile(r"(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)", re.DOTALL)
_MD_ITALIC2_RE = re.compile(r"(?<!_)_(?!_)(.+?)(?<!_)_(?!_)", re.DOTALL)
_MD_CODE_RE    = re.compile(r"`([^`]+)`")

def _to_html(text: str) -> str:
    if not text:
        return ""
    original = text

    # 0) –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–º–µ–Ω–∏–º –∫–æ–¥–æ–≤—ã–µ —Å–ø–∞–Ω—ã –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞–º–∏
    code_buf = []
    def _stash(m):
        code_buf.append(m.group(1))
        return f"@@CODE{len(code_buf)-1}@@"

    txt = _MD_CODE_RE.sub(_stash, original)
    txt = html.escape(txt)

    # 1) –∑–∞–≥–æ–ª–æ–≤–∫–∏/–∂–∏—Ä–Ω—ã–π/–∫—É—Ä—Å–∏–≤
    txt = _MD_H_RE.sub(r"<b>\1</b>", txt)
    txt = _MD_BOLD_RE.sub(r"<b>\1</b>", txt)
    txt = _MD_BOLD2_RE.sub(r"<b>\1</b>", txt)
    txt = _MD_ITALIC_RE.sub(r"<i>\1</i>", txt)
    txt = _MD_ITALIC2_RE.sub(r"<i>\1</i>", txt)

    # 2) –∑–∞—á–∏—Å—Ç–∫–∞ ¬´–≤–∏—Å—è—á–∏—Ö¬ª ** ‚Äî —É–∂–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ (–≤ –∫–æ–¥–µ –∏—Ö –Ω–µ—Ç)
    txt = re.sub(r"(?<!\*)\*\*(?!\*)", "", txt)

    # 3) –≤–µ—Ä–Ω—É—Ç—å –∫–æ–¥–æ–≤—ã–µ —Å–ø–∞–Ω—ã, —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–≤ –∏—Ö –∫–æ–Ω—Ç–µ–Ω—Ç
    def _restore(m):
        i = int(m.group(1))
        return f"<code>{html.escape(code_buf[i])}</code>"
    txt = re.sub(r"@@CODE(\d+)@@", _restore, txt)

    return txt


# -------- –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ --------
_GREET_RE = re.compile(
    r"(?i)\b(–ø—Ä–∏–≤–µ—Ç|–∑–¥—Ä–∞–≤—Å—Ç–≤|–¥–æ–±—Ä—ã–π\s*(–¥–µ–Ω—å|–≤–µ—á–µ—Ä|—É—Ç—Ä–æ)|hello|hi|hey|—Ö–∞–π|—Å–∞–ª—é—Ç|–∫—É)\b"
)

def _is_greeting(text: str) -> bool:
    t = (text or "").strip()
    if not t:
        return False
    # –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ —Ñ—Ä–∞–∑—ã, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
    return bool(_GREET_RE.search(t))


def _split_multipart(text: str,
                     *,
                     target: int = TG_SPLIT_TARGET,
                     max_parts: int = TG_SPLIT_MAX_PARTS,  # –ø–∞—Ä–∞–º–µ—Ç—Ä –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                     hard: int = TG_MAX_CHARS) -> list[str]:
    s = text or ""
    if not s:
        return []
    parts: list[str] = []
    rest = s

    # —Ä–µ–∂–µ–º –ø–æ ¬´–∫—Ä–∞—Å–∏–≤—ã–º¬ª –≥—Ä–∞–Ω–∏—Ü–∞–º —Å—Ç–æ–ª—å–∫–æ —Ä–∞–∑, —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ
    while len(rest) > target:
        cut = -1
        for m in _SPLIT_ANCHOR_RE.finditer(rest[: min(len(rest), hard)]):
            if m.start() < target:
                cut = m.start()
        if cut <= 0:
            cut = _smart_cut_point(rest, min(hard, target))
        parts.append(rest[:cut].rstrip())
        rest = rest[cut:].lstrip()

    # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ö–≤–æ—Å—Ç –∏ —Å–≤–µ—Ä—Ö–∂—ë—Å—Ç–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ª–∏–º–∏—Ç—É Telegram
    while rest:
        parts.append(rest[:hard])
        rest = rest[hard:]
    return parts


async def _send(m: types.Message, text: str):
    """–ë–µ—Ä–µ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞—Å—Ç—è–º–∏ –≤ HTML-—Ä–µ–∂–∏–º–µ (–Ω–µ—Å—Ç—Ä–∏–º–æ–≤—ã–π —Ñ–æ–ª–±—ç–∫)."""
    chunks = _split_multipart(text or "")
    logger.info(
        "SEND: %d chunk(s) to chat_id=%s (message_id=%s), total_len=%d",
        len(chunks),
        m.chat.id,
        getattr(m, "message_id", None),
        len(text or ""),
    )
    for i, chunk in enumerate(chunks):
        # –Ω–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–ø–∞–º–∏—Ç—å —á–∞—Ç
        if i > 0 and MULTIPART_SLEEP_MS > 0:
            await asyncio.sleep(MULTIPART_SLEEP_MS / 1000)

        try:
            await m.answer(
                _to_html(chunk),
                parse_mode="HTML",
                disable_web_page_preview=True,
            )
            logger.debug(
                "SEND: chunk %d/%d sent, len=%d",
                i + 1,
                len(chunks),
                len(chunk),
            )
        except Exception:
            logger.exception(
                "SEND: failed to send chunk %d/%d (len=%d)",
                i + 1,
                len(chunks),
                len(chunk),
            )


# ---- Verbosity helpers ----
def _detect_verbosity(text: str) -> str:
    t = (text or "").lower()
    detailed = re.search(r"\b(–ø–æ–¥—Ä–æ–±–Ω|–¥–µ—Ç–∞–ª|—Ä–∞–∑–≤—ë—Ä–Ω—É—Ç|—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç|—Ä–∞–∑–±–æ—Ä|explain in detail|detailed)\b", t)
    brief    = re.search(r"\b(–∫—Ä–∞—Ç–∫|–≤\s*–¥–≤—É—Ö\s*—Å–ª–æ–≤|–∫–æ—Ä–æ—Ç–∫|–≤—ã–∂–∏–º–∫|summary|brief)\b", t)
    if detailed:
        return "detailed"
    if brief:
        return "brief"
    # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ ‚Äî —Å–∫–æ—Ä–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç
    if len(t) > 600:
        return "detailed"
    return "normal"


def _verbosity_addendum(verbosity: str, what: str = "–æ—Ç–≤–µ—Ç") -> str:
    """
    –ù–µ–±–æ–ª—å—à–∞—è –ø—Ä–∏–ø–∏—Å–∫–∞ –∫ –ø—Ä–æ–º–ø—Ç—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç—Ä–µ–±—É–µ–º–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏.
    `what` ‚Äî —á—Ç–æ –∏–º–µ–Ω–Ω–æ –Ω—É–∂–Ω–æ –æ–ø–∏—Å—ã–≤–∞—Ç—å: '–æ—Ç–≤–µ—Ç', '–æ–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤' –∏ —Ç.–ø.
    """
    what = (what or "–æ—Ç–≤–µ—Ç").strip()

    if verbosity in ("short", "brief"):
        # –ø—Ä–∏–º–µ—Ä: "–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ (–ø–æ –æ–ø–∏—Å–∞–Ω–∏—é —Ä–∏—Å—É–Ω–∫–æ–≤)."
        return f" –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ (–ø–æ {what})."

    if verbosity == "detailed":
        # –ø—Ä–∏–º–µ—Ä: "–î–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ, –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–∏—Å—É–Ω–∫–æ–≤."
        return f" –î–∞–π —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–µ, –ø–æ–¥—Ä–æ–±–Ω–æ–µ {what}."

    # default ‚Äî –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —É–∫–∞–∑–∞–Ω–∏–π
    return ""

# --------------------- STREAM: –≤—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ---------------------

def _now_ms() -> int:
    return int(time.time() * 1000)

async def _typing_loop(chat_id: int, stop_event: asyncio.Event):
    try:
        while not stop_event.is_set():
            await bot.send_chat_action(chat_id, ChatAction.TYPING)
            try:
                await asyncio.wait_for(stop_event.wait(), timeout=TYPE_INDICATION_EVERY_MS / 1000)
            except asyncio.TimeoutError:
                # –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ü–∏–∫–ª, —á—Ç–æ–±—ã –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–ª–∞—Ç—å "typing"
                pass
    except Exception:
        # –≥–ª—É—à–∏–º –ª—é–±—ã–µ –Ω–µ—Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏, —á—Ç–æ–±—ã –Ω–µ —Ä–æ–Ω—è—Ç—å —Å—Ç—Ä–∏–º
        pass



def _section_context(owner_id: int, doc_id: int, sec: str, *, max_chars: int = 9000) -> str:
    # 1) –≥–µ–Ω–µ—Ä–∏–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø–∏—Å–∏ –Ω–æ–º–µ—Ä–∞
    base = (sec or "").strip()
    variants = {
        base,
        base.replace(" ", ""),
        base.replace(" ", "").replace(",", "."),
        base.replace(" ", "").replace(".", ","),
    }
    prefixes = ["", "–†–∞–∑–¥–µ–ª ", "–ü—É–Ω–∫—Ç ", "–ì–ª–∞–≤–∞ ", "–ü–æ–¥—Ä–∞–∑–¥–µ–ª "]
    patterns = [f"%{v}%" for v in variants]
    patterns += [f"%{p}{base}%" for p in prefixes]
    # —É–±–µ—Ä—ë–º –¥—É–±–ª–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–º —Ä–∞–∑—É–º–Ω—ã–º —á–∏—Å–ª–æ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤
    patterns = list(dict.fromkeys(patterns))[:8]

    con = get_conn()
    cur = con.cursor()

    rows = []
    if patterns:
        placeholders = " OR ".join(["section_path LIKE ?"] * len(patterns))
        cur.execute(
            f"""
            SELECT page, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=? AND ({placeholders})
            ORDER BY page ASC, id ASC
            """,
            (owner_id, doc_id, *patterns),
        )
        rows = cur.fetchall() or []

    # 2) —Ñ–æ–ª–±—ç–∫: –Ω–∞–π–¥—ë–º heading —Å –Ω–æ–º–µ—Ä–æ–º –∏ –≤–æ–∑—å–º—ë–º –µ–≥–æ —Å–µ–∫—Ü–∏—é
    if not rows:
        has_et = _table_has_columns(con, "chunks", ["element_type"])
        if has_et:
            cur.execute(
                """
                SELECT section_path
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                AND (element_type='heading' OR element_type IS NULL)
                AND (section_path LIKE ? OR text LIKE ?)
                ORDER BY page ASC, id ASC LIMIT 1
                """,
                (owner_id, doc_id, f"%{base}%", f"%{base}%"),
            )
        else:
            # —Å—Ç–∞—Ä–∞—è —Å—Ö–µ–º–∞ ‚Äî –±–µ–∑ —É—Å–ª–æ–≤–∏—è –ø–æ element_type
            cur.execute(
                """
                SELECT section_path
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                AND (section_path LIKE ? OR text LIKE ?)
                ORDER BY page ASC, id ASC LIMIT 1
                """,
                (owner_id, doc_id, f"%{base}%", f"%{base}%"),
            )
        h = cur.fetchone()
        if h and h["section_path"]:
            cur.execute(
                """
                SELECT page, section_path, text
                FROM chunks
                WHERE owner_id=? AND doc_id=? AND section_path=?
                ORDER BY page ASC, id ASC
                """,
                (owner_id, doc_id, h["section_path"]),
            )
            rows = cur.fetchall() or []

    con.close()
    if not rows:
        return ""

    parts, total = [], 0
    header_inserted = False
    for r in rows:
        secpath = (r["section_path"] or "").strip()
        t = (r["text"] or "").strip()
        if not t:
            continue
        chunk = (f"[{secpath}]\n{t}") if not header_inserted else t
        header_inserted = True
        if total + len(chunk) > max_chars:
            parts.append(chunk[: max_chars - total])
            break
        parts.append(chunk)
        total += len(chunk)
    return "\n\n".join(parts)


def _ensure_iterable(stream_obj) -> Iterable[str]:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ (a)—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏—Ç–µ—Ä–∞—Ç–æ—Ä —Å—Ç—Ä–æ–∫; –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞ –ø—Ä–∏–ª–µ—Ç–µ–ª–∞ –∫–æ—Ä—É—Ç–∏–Ω–∞."""
    import inspect

    # –ï—Å–ª–∏ —ç—Ç–æ –∫–æ—Ä—É—Ç–∏–Ω–∞ ‚Äî –æ–±–µ—Ä–Ω—ë–º –≤ async-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∞—á–∞–ª–∞ –µ—ë await-–∏—Ç,
    # –∞ –ø–æ—Ç–æ–º —É–∂–µ –∏—Ç–µ—Ä–∏—Ä—É–µ—Ç—Å—è –ø–æ —Ä–µ–∞–ª—å–Ω–æ–º—É —Å—Ç—Ä–∏–º—É.
    if inspect.iscoroutine(stream_obj):
        async def _await_then_iter():
            real = await stream_obj
            if hasattr(real, "__aiter__"):
                async for chunk in real:
                    yield chunk
            else:
                for chunk in real:
                    yield chunk
        return _await_then_iter()

    if hasattr(stream_obj, "__aiter__"):
        async def _drain_to_queue(q: asyncio.Queue):
            try:
                async for chunk in stream_obj:  # type: ignore
                    await q.put(chunk or "")
            except Exception:
                pass
            finally:
                await q.put(None)

        queue: asyncio.Queue = asyncio.Queue()

        async def _producer():
            await _drain_to_queue(queue)

        asyncio.create_task(_producer())

        async def _async_iter():
            while True:
                item = await queue.get()
                if item is None:
                    break
                yield item
        return _async_iter()

    return stream_obj

async def _iterate_chunks(stream_obj) -> AsyncIterable[str]:
    """–ï–¥–∏–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —á–∞–Ω–∫–æ–≤ (—É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∏ —Å sync-, –∏ —Å async-–∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞–º–∏)."""
    if hasattr(stream_obj, "__aiter__"):
        async for ch in stream_obj:
            if ch:
                yield str(ch)
        return
    for ch in stream_obj:
        if ch:
            yield str(ch)

def _smart_cut_point(s: str, limit: int) -> int:
    """–ò—â–µ–º ¬´–∫—Ä–∞—Å–∏–≤–æ–µ¬ª –º–µ—Å—Ç–æ —Ä–∞–∑—Ä–µ–∑–∞ <= limit (–ø–æ –ø–µ—Ä–µ–Ω–æ—Å—É/—Ç–æ—á–∫–µ/–ø—Ä–æ–±–µ–ª—É)."""
    if len(s) <= limit:
        return len(s)
    cut = s.rfind("\n", 0, limit)
    if cut == -1:
        cut = s.rfind(". ", 0, limit)
    if cut == -1:
        cut = s.rfind(" ", 0, limit)
    if cut == -1:
        cut = limit
    return max(1, cut)

# --- [VISION] helpers: –≤—ã–±—Ä–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ –ø–∞—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π ---
def _pick_images_from_hits(hits: list[dict], limit: int = 3) -> list[str]:
    acc: list[str] = []
    for h in hits or []:
        attrs = (h.get("attrs") or {})

        # 1) —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å: attrs.images
        for p in (attrs.get("images") or []):
            if p and os.path.exists(p) and p not in acc:
                acc.append(p)
            if len(acc) >= limit:
                return acc

        # 2) —Ñ–æ–ª–±—ç–∫: –∏–Ω–æ–≥–¥–∞ –ø—É—Ç—å –ª–µ–∂–∏—Ç –ø—Ä—è–º–æ –≤ —Å–∞–º–æ–º —Ö–∏—Ç–µ
        for p in (
            h.get("image_path"),
            h.get("image"),
        ):
            if p and os.path.exists(p) and p not in acc:
                acc.append(p)
            if len(acc) >= limit:
                return acc

    return acc


def _pairs_to_bullets(pairs: list[dict]) -> str:
    """
    –ê–∫–∫—É—Ä–∞—Ç–Ω–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–∞—Ä—ã (label, value, unit):
    - 0.25 –ø—Ä–∏ unit='%‚Äô ‚Üí 25%;
    - —á–∏—Å–ª–∞ –æ–∫—Ä—É–≥–ª—è–µ–º –¥–æ —Ü–µ–ª—ã—Ö –∏–ª–∏ 2 –∑–Ω–∞–∫–æ–≤;
    - —É–±–∏—Ä–∞–µ–º —Ö–≤–æ—Å—Ç—ã –≤–∏–¥–∞ 0.42000000000000004.
    """
    def _fmt(value, unit: str) -> str:
        unit = (unit or "").strip()
        sval = ""
        v_num: float | None = None

        # –ø—Ä–æ–±—É–µ–º –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª—É
        if isinstance(value, (int, float, Decimal)):
            v_num = float(value)
        else:
            try:
                v_num = float(str(value).replace(",", "."))
            except Exception:
                sval = str(value) if value is not None else ""

        if v_num is not None:
            # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –¥–æ–ª–∏ —Å unit='%' ‚Üí –ø—Ä–æ—Ü–µ–Ω—Ç—ã
            if unit and "%" in unit and 0.0 <= v_num <= 1.2:
                v_num *= 100.0

            if abs(v_num - round(v_num)) < 0.05:
                sval = str(int(round(v_num)))
            else:
                sval = f"{v_num:.2f}".rstrip("0").rstrip(".")

        # –¥–æ–±–∞–≤–ª—è–µ–º –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
        if unit:
            if "%" in unit and not sval.endswith("%"):
                sval += "%"
            else:
                sval += f" {unit}"
        return sval

    lines: list[str] = []
    for r in (pairs or []):
        lab = (str(r.get("label") or "")).strip()
        unit = (str(r.get("unit") or "")).strip()
        raw_val = r.get("value")
        val = _fmt(raw_val, unit)

        if not lab and not val:
            continue
        if lab and val:
            lines.append(f"‚Äî {lab}: {val}")
        elif lab:
            lines.append(f"‚Äî {lab}")
        else:
            lines.append(f"‚Äî {val}")
    return "\n".join(lines)


async def _stream_to_telegram(m: types.Message, stream, head_text: str = "‚åõÔ∏è –ü–µ—á–∞—Ç–∞—é –æ—Ç–≤–µ—Ç‚Ä¶") -> None:
    logger.info(
        "STREAM: start for chat_id=%s message_id=%s",
        m.chat.id,
        getattr(m, "message_id", None),
    )
    current_text = ""
    sent_parts = 0
    initial = await m.answer(
        _to_html(head_text),
        parse_mode="HTML",
        disable_web_page_preview=True,
    )
    last_edit_at = _now_ms() - STREAM_HEAD_START_MS
    stop_typer = asyncio.Event()
    typer_task = asyncio.create_task(_typing_loop(m.chat.id, stop_event=stop_typer))

    # üîß –Ω–æ–≤–æ–µ: –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –≤ multi –±–æ–ª—å—à–µ –Ω–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º initial
    freeze_initial = False

    try:
        async for delta in _iterate_chunks(_ensure_iterable(stream)):
            current_text += delta

            # 3.a) –º—É–ª—å—Ç–∏-—Ä–µ–∂–∏–º: —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–æ—Ä—Ü–∏—è–º–∏
            if STREAM_MODE == "multi" and len(current_text) >= TG_SPLIT_TARGET:
                cut = -1
                for mm in _SPLIT_ANCHOR_RE.finditer(current_text[: min(len(current_text), TG_MAX_CHARS)]):
                    if mm.start() < TG_SPLIT_TARGET:
                        cut = mm.start()
                if cut <= 0:
                    cut = _smart_cut_point(current_text, min(TG_MAX_CHARS, TG_SPLIT_TARGET))

                part = current_text[:cut].rstrip()
                try:
                    if sent_parts == 0:
                        await initial.edit_text(
                            _to_html(part),
                            parse_mode="HTML",
                            disable_web_page_preview=True,
                        )
                        freeze_initial = True  # <- –±–æ–ª—å—à–µ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º initial
                    else:
                        await m.answer(
                            _to_html(part),
                            parse_mode="HTML",
                            disable_web_page_preview=True,
                        )
                except TelegramBadRequest:
                    await m.answer(
                        _to_html(part),
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )

                sent_parts += 1  # ### –î–û–ë–ê–í–õ–ï–ù–û: —Å—á–∏—Ç–∞–µ–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
                current_text = current_text[cut:].lstrip()
                last_edit_at = _now_ms()
                continue

            # 3.b) –∑–∞—â–∏—Ç–∞ –æ—Ç –ª–∏–º–∏—Ç–∞
            if len(current_text) >= TG_MAX_CHARS:
                cut = _smart_cut_point(current_text, TG_MAX_CHARS)
                final_part = current_text[:cut]

                if STREAM_MODE == "multi" and (freeze_initial or sent_parts > 0):
                    # üîß –≤ multi –Ω–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º initial –ø–æ—Å–ª–µ 1-–π —á–∞—Å—Ç–∏
                    await m.answer(
                        _to_html(final_part),
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )
                else:
                    try:
                        await initial.edit_text(
                            _to_html(final_part),
                            parse_mode="HTML",
                            disable_web_page_preview=True,
                        )
                    except TelegramBadRequest:
                        await m.answer(
                            _to_html(final_part),
                            parse_mode="HTML",
                            disable_web_page_preview=True,
                        )

                sent_parts += 1  # ### –î–û–ë–ê–í–õ–ï–ù–û: —ç—Ç–∞ —á–∞—Å—Ç—å —Ç–æ–∂–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π
                current_text = current_text[cut:].lstrip()
                # üîß –Ω–æ–≤—ã–π –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –≤ edit-—Ä–µ–∂–∏–º–µ
                if STREAM_MODE == "edit":
                    initial = await m.answer(
                        _to_html("‚Ä¶"),
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )
                last_edit_at = _now_ms()
                continue

            # 3.c) –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∫–∏ ‚Äî üîß –¢–û–õ–¨–ö–û –≤ —Ä–µ–∂–∏–º–µ edit
            now = _now_ms()
            if (
                STREAM_MODE == "edit"
                and (now - last_edit_at) >= STREAM_EDIT_INTERVAL_MS
                and len(current_text) >= STREAM_MIN_CHARS
            ):
                try:
                    await initial.edit_text(
                        _to_html(current_text),
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )
                    last_edit_at = now
                except TelegramBadRequest:
                    pass

        # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ö–≤–æ—Å—Ç
        if current_text:
            logger.info(
                "STREAM: finishing with tail, len=%d, sent_parts=%d",
                len(current_text),
                sent_parts,
            )
            # –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —Ä–µ–∂–µ–º –æ—Å—Ç–∞—Ç–æ–∫ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ –Ω–µ—Å—Ç—Ä–∏–º–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
            tail_parts = _split_multipart(current_text or "")

            if STREAM_MODE == "multi" and sent_parts > 0:
                # –≤ multi-—Ä–µ–∂–∏–º–µ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
                for part in tail_parts:
                    html_part = _to_html(part)
                    try:  # ### –î–û–ë–ê–í–õ–ï–ù–û: –Ω–µ –¥–∞—ë–º –∏—Å–∫–ª—é—á–µ–Ω–∏—é —É–±–∏—Ç—å –≤–µ—Å—å —Ö–≤–æ—Å—Ç
                        await m.answer(
                            html_part,
                            parse_mode="HTML",
                            disable_web_page_preview=True,
                        )
                    except TelegramBadRequest:
                        # –µ—Å–ª–∏ HTML/–¥–ª–∏–Ω–∞ —Å–ª–æ–º–∞–ª–∏ —Ä–∞–∑–º–µ—Ç–∫—É ‚Äî —à–ª—ë–º –∫–∞–∫ –µ—Å—Ç—å
                        await m.answer(part)
            else:
                # –≤ edit-—Ä–µ–∂–∏–º–µ (–∏–ª–∏ –∫–æ–≥–¥–∞ –µ—â—ë –Ω–µ –±—ã–ª–æ —á–∞—Å—Ç–µ–π) –ø–µ—Ä–≤—ã–π –∫—É—Å–æ–∫ –ø—ã—Ç–∞–µ–º—Å—è
                # –ø–æ–ª–æ–∂–∏—Ç—å –≤ initial, –∞ –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
                first = True
                for part in tail_parts:
                    html_part = _to_html(part)
                    if first:
                        try:
                            await initial.edit_text(
                                html_part,
                                parse_mode="HTML",
                                disable_web_page_preview=True,
                            )
                        except TelegramBadRequest:
                            # –µ—Å–ª–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å ‚Äî —à–ª—ë–º –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
                            try:
                                await m.answer(
                                    html_part,
                                    parse_mode="HTML",
                                    disable_web_page_preview=True,
                                )
                            except TelegramBadRequest:
                                await m.answer(part)
                        first = False
                    else:
                        try:
                            await m.answer(
                                html_part,
                                parse_mode="HTML",
                                disable_web_page_preview=True,
                            )
                        except TelegramBadRequest:
                            await m.answer(part)

    except Exception:
        logger.exception(
            "STREAM: unexpected error for chat_id=%s",
            m.chat.id,
        )
    finally:
        stop_typer.set()
        try:
            await typer_task
        except Exception:
            pass
        logger.info(
            "STREAM: stop for chat_id=%s message_id=%s",
            m.chat.id,
            getattr(m, "message_id", None),
        )


def _plan_subtasks_via_gpt(question: str, max_items: int = 8) -> list[dict]:
    """
    –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ –±–µ–∑ ACE.
    –ë–µ—Ä—ë—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å –∏ –ø—Ä–æ—Å–∏—Ç GPT —Ä–∞–∑–±–∏—Ç—å –µ–≥–æ –Ω–∞ 2‚ÄìN –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ dict: {"id": int, "ask": str}.
    """
    question = (question or "").strip()
    if not question:
        return []

    if "chat_with_gpt" not in globals() or chat_with_gpt is None:
        return []

    system_prompt = (
        "–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å —Å—Ç—É–¥–µ–Ω—Ç—É —Å –¥–∏–ø–ª–æ–º–æ–º. –ü–æ–ª—É—á–∏–≤ —Å–ª–æ–∂–Ω—ã–π –∏–ª–∏ –º–Ω–æ–≥–æ—á–∞—Å—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å, "
        "—Ä–∞–∑–±–µ–π –µ–≥–æ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã—Ö –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å. "
        "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON-–º–∞—Å—Å–∏–≤ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥, —Ñ–æ—Ä–º–∞—Ç–∞:\n"
        "[{\"id\": 1, \"ask\": \"...\"}, {\"id\": 2, \"ask\": \"...\"}, ...].\n"
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–π, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ JSON."
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question},
    ]

    try:
        raw = chat_with_gpt(messages, temperature=0.0, max_tokens=400) or ""
    except Exception as e:
        logging.exception("plan_subtasks_via_gpt failed: %s", e)
        return []

    raw = raw.strip()

    # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–¥–µ—Ä–Ω—É—Ç—å JSON-–º–∞—Å—Å–∏–≤
    data = None
    try:
        data = json.loads(raw)
    except Exception:
        m = re.search(r"\[[\s\S]*\]", raw)
        if not m:
            return []
        try:
            data = json.loads(m.group(0))
        except Exception:
            return []

    if not isinstance(data, list):
        return []

    items: list[dict] = []
    for i, it in enumerate(data, start=1):
        if isinstance(it, str):
            ask = it.strip()
            if not ask:
                continue
            items.append({"id": i, "ask": ask})
        elif isinstance(it, dict):
            ask = str(it.get("ask") or it.get("question") or it.get("text") or "").strip()
            if not ask:
                continue
            iid = it.get("id") or i
            items.append({"id": iid, "ask": ask})
        if len(items) >= max_items:
            break

    return items


def _answer_subpoint_via_gpt(
    ask: str,
    ctx_text: str,
    base_question: str,
    *,
    verbosity: str = "normal",
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –ø–æ –æ–¥–Ω–æ–º—É –ø–æ–¥–ø—É–Ω–∫—Ç—É —á–µ—Ä–µ–∑ GPT (–±–µ–∑ ACE).
    """
    ask = (ask or "").strip()
    if not ask:
        return ""

    if "chat_with_gpt" not in globals() or chat_with_gpt is None:
        return ""

    ctx = (ctx_text or "").strip()

    system_prompt = (
        "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –¢–µ–±–µ –¥–∞–ª–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –¥–∏–ø–ª–æ–º–∞ "
        "–∏ –æ–¥–∏–Ω –ø–æ–¥–ø—É–Ω–∫—Ç –≤–æ–ø—Ä–æ—Å–∞.\n"
        "–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –ø–æ —ç—Ç–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç–æ–≤, —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å, "
        "–∫–æ—Ç–æ—Ä—ã—Ö –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ —É–ø–æ–º–∏–Ω–∞–π –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç–æ–≤, –≤—ã—Ä—É—á–∫—É, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø., "
        "–µ—Å–ª–∏ —ç—Ç–∏—Ö —Å–ª–æ–≤ –Ω–µ—Ç –≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ). –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, "
        "—á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –∏ –ø—Ä—è–º–æ –Ω–∞–ø–∏—à–∏, —á—Ç–æ –≤ —ç—Ç–æ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ –æ–± —ç—Ç–æ–º –Ω–µ —Å–∫–∞–∑–∞–Ω–æ.\n"
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π —Ä–∞–∑–¥–µ–ª—ã –≤–∏–¥–∞ ¬´—á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç¬ª."
    )


    if ctx:
        assistant_ctx = f"[–§—Ä–∞–≥–º–µ–Ω—Ç –¥–∏–ø–ª–æ–º–∞]\n{ctx}"
    else:
        assistant_ctx = "[–§—Ä–∞–≥–º–µ–Ω—Ç –ø–æ —ç—Ç–æ–º—É –ø–æ–¥–ø—É–Ω–∫—Ç—É –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ç–µ–∫—Å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞]"

    user_prompt = (
        f"–ò—Å—Ö–æ–¥–Ω—ã–π –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{base_question}\n\n"
        f"–¢–µ–∫—É—â–∏–π –ø–æ–¥–ø—É–Ω–∫—Ç (–ø–æ–¥–≤–æ–ø—Ä–æ—Å): {ask}\n\n"
        "–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ —ç—Ç–æ–º—É –ø–æ–¥–ø—É–Ω–∫—Ç—É, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–∏–ø–ª–æ–º–∞."
        f"{_verbosity_addendum(verbosity)}"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": assistant_ctx},
        {"role": "user", "content": user_prompt},
    ]

    try:
        ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS) or ""
    except Exception as e:
        logging.exception("answer_subpoint_via_gpt failed: %s", e)
        return ""

    return ans.strip()


def _merge_subanswers_via_gpt(
    base_question: str,
    items: list[dict],
    subanswers: list[str],
    *,
    verbosity: str = "normal",
) -> str:
    """
    –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ –≤—Å–µ–º –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º –±–µ–∑ ACE.
    """
    if not subanswers:
        return ""

    if "chat_with_gpt" not in globals() or chat_with_gpt is None:
        return ""

    blocks: list[str] = []
    for i, ans in enumerate(subanswers, start=1):
        it = items[i - 1] if i - 1 < len(items) else {}
        ask = (isinstance(it, dict) and (it.get("ask") or "")) or ""
        ask = str(ask).strip()
        header = f"[–ü–æ–¥–ø—É–Ω–∫—Ç {i}" + (f": {ask}]" if ask else "]")
        blocks.append(f"{header}\n{ans}")

    ctx = "\n\n".join(blocks)

    system_prompt = (
        "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –ù–∏–∂–µ —Å–æ–±—Ä–∞–Ω—ã –æ—Ç–≤–µ—Ç—ã –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º "
        "–æ–¥–Ω–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–¥–µ–ª–∞—Ç—å –æ–¥–∏–Ω —Å–≤—è–∑–Ω—ã–π –æ–±—â–∏–π –æ—Ç–≤–µ—Ç.\n"
        "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –¥–æ—Å–ª–æ–≤–Ω–æ –≤—Å–µ –ø–æ–¥–ø—É–Ω–∫—Ç—ã, –∞ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏—Ö –æ–±—ä–µ–¥–∏–Ω—è–π. "
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤, —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ø–æ–¥–ø—É–Ω–∫—Ç–∞—Ö "
        "(–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç–æ–≤, –≤—ã—Ä—É—á–∫—É, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø., –µ—Å–ª–∏ —ç—Ç–æ–≥–æ –Ω–µ—Ç "
        "–≤ —Å–∞–º–∏—Ö –ø–æ–¥–ø—É–Ω–∫—Ç–∞—Ö).\n"
        "–ù–µ –ø–∏—à–∏ —Ä–∞–∑–¥–µ–ª—ã –≤–∏–¥–∞ ¬´—á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç¬ª."
    )


    user_prompt = (
        f"–ò—Å—Ö–æ–¥–Ω—ã–π –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{base_question}\n\n"
        "–ù–∞ –Ω–µ–≥–æ —É–∂–µ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç—ã –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º (—Å–º. –Ω–∏–∂–µ). "
        "–°–æ–±–µ—Ä–∏ –∏–∑ –Ω–∏—Ö –æ–¥–∏–Ω —Ü–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."
        f"{_verbosity_addendum(verbosity)}\n\n"
        "[–û—Ç–≤–µ—Ç—ã –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º]\n"
        f"{ctx}"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    try:
        merged = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS) or ""
    except Exception as e:
        logging.exception("merge_subanswers_via_gpt failed: %s", e)
        return ""

    return merged.strip()


async def _run_multistep_answer(
    m: types.Message,
    uid: int,
    doc_id: int,
    q_text: str,
    *,
    discovered_items: list[dict] | None = None,
) -> bool:
    """
    –ú–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç –±–µ–∑ ACE:
    1) –ü–ª–∞–Ω –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ ‚Äî —á–µ—Ä–µ–∑ _plan_subtasks_via_gpt –∏–ª–∏ coverage.
    2) –ü–æ –∫–∞–∂–¥–æ–º—É –ø–æ–¥–ø—É–Ω–∫—Ç—É ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ GPT —Å –∂—ë—Å—Ç–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
    3) (–æ–ø—Ü.) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π merge —á–µ—Ä–µ–∑ _merge_subanswers_via_gpt.
    """
    if not MULTI_STEP_SEND_ENABLED:
        return False

    # GPT –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —ç—Ç–æ–≥–æ —Ä–µ–∂–∏–º–∞
    if "chat_with_gpt" not in globals() or chat_with_gpt is None:
        return False

    verbosity = _detect_verbosity(q_text)

    # 1) –ø–ª–∞–Ω –∏–∑ coverage/discovered_items –∏–ª–∏ —Å—Ç—Ä–æ–∏–º —á–µ—Ä–µ–∑ GPT
    items = (discovered_items or [])
    if not items:
        items = _plan_subtasks_via_gpt(q_text, max_items=MULTI_STEP_MAX_ITEMS)

    # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∏ dict, –∏ str
    norm_items: list[dict] = []
    for idx, it in enumerate(items, start=1):
        if isinstance(it, str):
            ask = it.strip()
            if ask:
                norm_items.append({"id": idx, "ask": ask})
        elif isinstance(it, dict):
            ask = (it.get("ask") or it.get("text") or it.get("q") or "").strip()
            if ask:
                norm_items.append({"id": it.get("id") or idx, "ask": ask})

    items = [it for it in norm_items if (it.get("ask") or "").strip()]
    if len(items) < MULTI_STEP_MIN_ITEMS:
        return False

    # –æ—Ç—Å–µ—á—ë–º —Ö–≤–æ—Å—Ç –ø–æ –ª–∏–º–∏—Ç—É
    items = items[:MULTI_STEP_MAX_ITEMS]

    # –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–æ–Ω—Å
    preview = "\n".join([f"{i+1}) {(it['ask'] or '').strip()}" for i, it in enumerate(items)])
    await _send(
        m,
        f"–í–æ–ø—Ä–æ—Å –º–Ω–æ–≥–æ—á–∞—Å—Ç–Ω—ã–π. –û—Ç–≤–µ—á–∞—é –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º ({len(items)} —à—Ç.):\n\n{preview}",
    )

    subanswers: list[str] = []

    # coverage-aware —Ä–∞–∑–¥–∞—á–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: —Ä–∞–∑–ª–æ–∂–∏–º –≤—ã–∂–∏–º–∫–∏ –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º
    cov = None
    try:
        cov = retrieve_coverage(owner_id=uid, doc_id=doc_id, question=q_text)
    except Exception:
        cov = None
    cov_map = (cov or {}).get("by_item") or {}

    # –ø–æ –æ—á–µ—Ä–µ–¥–∏: A ‚Üí send, B ‚Üí send, ...
    for i, it in enumerate(items, start=1):
        ask = (it.get("ask") or "").strip()
        if not ask:
            continue

        # –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞
        ctx_text = ""
        try:
            # –µ—Å–ª–∏ –µ—Å—Ç—å coverage-–±–∞–∫–µ—Ç ‚Äî —Å–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä—è–º–æ –∏–∑ —á–∞–Ω–∫–æ–≤ –ø–æ–¥–ø—É–Ω–∫—Ç–∞
            bucket = cov_map.get(str(it.get("id") or i)) or []
            if bucket:
                ctx_text = build_context_coverage(bucket, items_count=1)
        except Exception:
            ctx_text = ""

        # —Ñ–æ–ª–±—ç–∫–∏ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        if not ctx_text:
            ctx_text = best_context(uid, doc_id, ask, max_chars=6000) or ""
        if not ctx_text:
            hits = retrieve(uid, doc_id, ask, top_k=8)
            if hits:
                ctx_text = build_context(hits)
        if not ctx_text:
            ctx_text = _first_chunks_context(uid, doc_id, n=12, max_chars=6000)

        # –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç—É —á–µ—Ä–µ–∑ GPT (–±–µ–∑ ACE)
        try:
            part = _answer_subpoint_via_gpt(
                ask=ask,
                ctx_text=ctx_text,
                base_question=q_text,
                verbosity=verbosity,
            )
        except Exception as e:
            logging.exception("answer_subpoint_via_gpt failed: %s", e)
            part = ""

        # –æ—Ç–ø—Ä–∞–≤–∫–∞ –±–ª–æ–∫–∞
        header = f"**{i}. {ask}**\n\n"
        await _send(m, header + (part or "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –ø–æ —ç—Ç–æ–º—É –ø–æ–¥–ø—É–Ω–∫—Ç—É."))
        subanswers.append(f"{header}{part}")

        # –º–∏–∫—Ä–æ–ø–∞—É–∑a, —á—Ç–æ–±—ã –Ω–µ —É–ø–µ—Ä–µ—Ç—å—Å—è –≤ rate/—á–∞—Ç—ã
        await asyncio.sleep(MULTI_STEP_PAUSE_MS / 1000)

    # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –±–ª–æ–∫
    if MULTI_STEP_FINAL_MERGE and subanswers:
        try:
            merged = _merge_subanswers_via_gpt(
                base_question=q_text,
                items=items,
                subanswers=subanswers,
                verbosity=verbosity,
            ).strip()
            if merged:
                await _send(m, "**–ò—Ç–æ–≥–æ–≤—ã–π —Å–≤–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç**\n\n" + merged)
        except Exception as e:
            logging.exception("merge_subanswers_via_gpt failed: %s", e)

    return True

def _should_use_multistep(q_text: str, discovered_items: list[dict] | None) -> bool:
    """
    –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –≤–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π —Ä–µ–∂–∏–º, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏:
      ‚Äî –º—É–ª—å—Ç–∏–æ—Ç–≤–µ—Ç –≤–æ–æ–±—â–µ —Ä–∞–∑—Ä–µ—à—ë–Ω –∫–æ–Ω—Ñ–∏–≥–æ–º;
      ‚Äî –µ—Å—Ç—å –ø–æ–¥–ø—É–Ω–∫—Ç—ã –∏–∑ coverage/general_subitems;
      ‚Äî –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ –Ω–µ –º–µ–Ω—å—à–µ MULTI_STEP_MIN_ITEMS;
      ‚Äî –≤–æ–ø—Ä–æ—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π (—á—Ç–æ–±—ã —Å—Ç–æ–∏–ª–æ –≥–æ—Ä–æ–¥–∏—Ç—å –ø–æ–¥–ø—É–Ω–∫—Ç—ã).
    """
    if not MULTI_STEP_SEND_ENABLED:
        return False

    if not discovered_items:
        return False

    if len(discovered_items) < MULTI_STEP_MIN_ITEMS:
        return False

    if len((q_text or "").strip()) < MULTI_STEP_MIN_QUESTION_LEN:
        return False

    return True


# summarizer (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç)
try:
    from .summarizer import is_summary_intent, overview_context  # –º–æ–≥—É—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å ‚Äî –µ—Å—Ç—å —Ñ–æ–ª–±—ç–∫–∏ –Ω–∏–∂–µ
except Exception:
    def is_summary_intent(text: str) -> bool:
        return bool(re.search(
            r"\b(—Å—É—Ç—å|–∫—Ä–∞—Ç–∫–æ|–æ—Å–Ω–æ–≤–Ω|–≥–ª–∞–≤–Ω|summary|overview|–∏—Ç–æ–≥|–≤—ã–≤–æ–¥)\w*\b",
            text or "",
            re.IGNORECASE,
        ))

    def overview_context(owner_id: int, doc_id: int, max_chars: int = 6000) -> str:
        con = get_conn()
        cur = con.cursor()
        cur.execute(
            """
            SELECT page, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
              AND (text LIKE '[–ó–∞–≥–æ–ª–æ–≤–æ–∫]%%'
                   OR text LIKE '%%–¶–µ–ª—å%%'
                   OR text LIKE '%%–ó–∞–¥–∞—á%%'
                   OR text LIKE '%%–í–≤–µ–¥–µ–Ω%%'
                   OR text LIKE '%%–ó–∞–∫–ª—é—á–µ–Ω%%'
                   OR text LIKE '%%–í—ã–≤–æ–¥%%')
            ORDER BY page ASC, id ASC
            LIMIT 14
            """,
            (owner_id, doc_id),
        )
        rows = cur.fetchall()
        con.close()
        if not rows:
            return ""
        parts, total = [], 0
        for r in rows:
            block = f"{(r['text'] or '').strip()}"
            if total + len(block) > max_chars:
                break
            parts.append(block)
            total += len(block)
        return "\n\n".join(parts)

# NEW: —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ (—á–µ—Ä–µ–∑ vision-–∞–Ω–∞–ª–∏–∑ –≤ summarizer.py)
try:
    from .summarizer import extract_figure_value as summ_extract_figure_value  # type: ignore
except Exception:
    # –µ—Å–ª–∏ summarizer –∏–ª–∏ —Å–∞–º–∞ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã ‚Äî –ø—Ä–æ—Å—Ç–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ–ª–±—ç–∫ –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º
    summ_extract_figure_value = None  # type: ignore

# vision-–æ–ø–∏—Å–∞–Ω–∏–µ —Ä–∏—Å—É–Ω–∫–æ–≤ (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –æ—Ç–≤–µ—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ñ–æ–ª–±—ç–∫–æ–º)
try:
    from .summarizer import describe_figures as vision_describe_figures
except Exception:
    def vision_describe_figures(owner_id: int, doc_id: int, numbers: list[str]) -> str:
        if not numbers:
            return "–ù–µ —É–∫–∞–∑–∞–Ω—ã –Ω–æ–º–µ—Ä–∞ —Ä–∏—Å—É–Ω–∫–æ–≤."
        return "–û–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (vision-–º–æ–¥—É–ª—å –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω)."


# NEW: —Ç–æ—á–µ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏ (—Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç + —á–∏—Å–ª–∞)
try:
    from .vision_analyzer import analyze_figure as va_analyze_figure  # type: ignore
except Exception:
    va_analyze_figure = None  # type: ignore

# –ì–û–°–¢-–≤–∞–ª–∏–¥–∞—Ç–æ—Ä (–º—è–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç)
try:
    from .validators_gost import validate_gost, render_report
except Exception:
    validate_gost = None
    render_report = None


# ¬´–ê–∫—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç¬ª –≤ –ø–∞–º—è—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
ACTIVE_DOC: dict[int, int] = {}  # user_id -> doc_id
# NEW: –∫–æ—Ä–æ—Ç–∫–∞—è ¬´–ø–∞–º—è—Ç—å¬ª –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É–ø–æ–º—è–Ω—É—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
LAST_REF: dict[int, dict] = {}   # {uid: {"figure_nums": list[str], "area": "3.2"}}
FIG_INDEX: dict[int, dict] = {}
OOXML_INDEX: dict[int, dict] = {}
# NEW: –∫–µ—à –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö ¬´—Ç–∞–±–ª–∏—Ü-–∫–∞—Ä—Ç–∏–Ω–æ–∫¬ª (doc_id, num) -> —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫
_OCR_TABLE_CACHE: dict[tuple[int, str], str] = {}

# NEW: –∂–¥—ë–º –ª–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ¬´–¥–∞/–Ω–µ—Ç¬ª –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç [–º–æ–¥–µ–ª—å]
MODEL_EXTRA_PENDING: dict[int, dict] = {}   # {uid: {"question": str}}

# NEW: –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–æ–º–µ—Ä–∞ —Ä–∞–∑–¥–µ–ª–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –∏ –¥–ª—è –∞–Ω–∞—Ñ–æ—Ä—ã ¬´—ç—Ç–æ—Ç –ø—É–Ω–∫—Ç/—Ä–∏—Å—É–Ω–æ–∫¬ª
# NEW: –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–æ–º–µ—Ä–∞ —Ä–∞–∑–¥–µ–ª–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –∏ –¥–ª—è –∞–Ω–∞—Ñ–æ—Ä—ã ¬´—ç—Ç–æ—Ç –ø—É–Ω–∫—Ç/—Ä–∏—Å—É–Ω–æ–∫¬ª
_SECTION_NUM_RE = re.compile(
    r"(?i)\b(?:–≥–ª–∞–≤–∞\w*|—Ä–∞–∑–¥–µ–ª\w*|–ø—É–Ω–∫—Ç\w*|–ø–æ–¥—Ä–∞–∑–¥–µ–ª\w*|sec(?:tion)?\.?|chapter)"
    r"\s*(?:‚Ññ\s*)?((?:[A-Za-z–ê-–Ø–∞-—è](?=[\.\d]))?\s*\d+(?:[.,]\d+)*)"
)
_ANAPH_HINT_RE = re.compile(r"(?i)\b(—ç—Ç–æ—Ç|—ç—Ç–∞|—ç—Ç–æ|–¥–∞–Ω–Ω\w+|–ø—Ä–æ –Ω–µ–≥–æ|–ø—Ä–æ –Ω–µ—ë|–ø—Ä–æ –Ω–µ–µ)\b")

# —Å—á–∏—Ç–∞–µ–º "—É—Ç–æ—á–Ω—è—é—â–∏–º" –ª—é–±–æ–π –∑–∞–ø—Ä–æ—Å, –≥–¥–µ –µ—Å—Ç—å –≥–ª–∞–≥–æ–ª + —Å–ª–æ–≤–æ "–ø–æ–¥—Ä–æ–±–Ω–µ–µ" –≤ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–µ
_FOLLOWUP_MORE_RE = re.compile(
    r"(?i)\b(–æ–ø–∏—à–∏|—Ä–∞—Å–ø–∏—à–∏|–æ–±—ä—è—Å–Ω–∏|—Ä–∞—Å—Å–∫–∞–∂–∏)\b.*\b–ø–æ–¥—Ä–æ–±–Ω–µ–µ\b|\b–ø–æ–¥—Ä–æ–±–Ω–µ–µ\b.*\b(–æ–ø–∏—à–∏|—Ä–∞—Å–ø–∏—à–∏|–æ–±—ä—è—Å–Ω–∏|—Ä–∞—Å—Å–∫–∞–∂–∏)\b"
)

def _expand_with_last_referent(uid: int, text: str) -> str:
    """
    –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—ä–µ–∫—Ç (—Ç–∞–±–ª–∏—Ü–∞/—Ä–∏—Å—É–Ω–æ–∫/–ø—É–Ω–∫—Ç) –¥–ª—è —Ä–µ–ø–ª–∏–∫ –≤–∏–¥–∞:
      - ¬´–æ–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ¬ª
      - ¬´—Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –Ω–µ–≥–æ¬ª
      - ¬´–æ–ø–∏—à–∏ –µ—ë –ø–æ–¥—Ä–æ–±–Ω–µ–µ¬ª
    —á—Ç–æ–±—ã –æ–Ω–∏ –ø—Ä–µ–≤—Ä–∞—Ç–∏–ª–∏—Å—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤
      - ¬´–æ–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ (–∏–º–µ–µ—Ç—Å—è –≤ –≤–∏–¥—É —Ç–∞–±–ª–∏—Ü–∞ 4)¬ª.
    """
    t = (text or "").strip()
    if not t:
        return text

    # –µ—Å–ª–∏ —É–∂–µ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞/—Ä–∏—Å—É–Ω–æ–∫/–ø—É–Ω–∫—Ç ‚Äî –Ω–∏—á–µ–≥–æ –Ω–µ –º–µ–Ω—è–µ–º
    if _TABLE_NUM_IN_TEXT_RE.search(t) or FIG_NUM_RE.search(t) or _SECTION_NUM_RE.search(t):
        return text

    # –Ω–µ—Ç –Ω–∏ –∞–Ω–∞—Ñ–æ—Ä—ã (¬´—ç—Ç–æ—Ç/–ø—Ä–æ –Ω–µ—ë¬ª), –Ω–∏ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ñ–æ–ª–ª–æ—É-–∞–ø–∞ ¬´–æ–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ¬ª ‚Äî –≤—ã—Ö–æ–¥–∏–º
    if not (_ANAPH_HINT_RE.search(t) or _FOLLOWUP_MORE_RE.search(t)):
        return text

    last = LAST_REF.get(uid) or {}

    # 1) –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Äî –ø–æ—Å–ª–µ–¥–Ω—è—è —Ç–∞–±–ª–∏—Ü–∞
    tables = last.get("table_nums") or []
    if tables:
        num = str(tables[0])
        return f"{text} (–∏–º–µ–µ—Ç—Å—è –≤ –≤–∏–¥—É —Ç–∞–±–ª–∏—Ü–∞ {num})"

    # 2) –∑–∞—Ç–µ–º ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∏—Å—É–Ω–æ–∫
    figs = last.get("figure_nums") or []
    if figs:
        num = str(figs[0])
        return f"{text} (–∏–º–µ–µ—Ç—Å—è –≤ –≤–∏–¥—É —Ä–∏—Å—É–Ω–æ–∫ {num})"

    # 3) –∑–∞—Ç–µ–º ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—É–Ω–∫—Ç/—Ä–∞–∑–¥–µ–ª
    area = (last.get("area") or "").strip()
    if area:
        if not re.search(r"(?i)\b(–≥–ª–∞–≤–∞|—Ä–∞–∑–¥–µ–ª|–ø—É–Ω–∫—Ç|–ø–æ–¥—Ä–∞–∑–¥–µ–ª)\b", t):
            return f"{text} (–∏–º–µ–µ—Ç—Å—è –≤ –≤–∏–¥—É –ø—É–Ω–∫—Ç {area})"
        return f"{text} ({area})"

    return text


# ------------------------ –ì–∞—Ä–¥—Ä–µ–π–ª—ã ------------------------

_BANNED_PATTERNS = [
    r"jail ?break|system\s*prompt|developer\s*mode|dan\b|ignore (all|previous) (rules|instructions)",
    r"\b–≤–∑–ª–æ–º|—Ö–∞–∫–∏?|–∫–µ–π–≥–µ–Ω|–∫—Ä—è–∫|—Å–æ—Ü–∏–∞–ª—å–Ω(–∞—è|—ã–µ) –∏–Ω–∂–µ–Ω–µ—Ä–∏—è",
    r"\b–≤–∏—Ä—É—Å|–≤—Ä–µ–¥–æ–Ω–æ—Å|—ç–∫—Å–ø–ª–æ–π—Ç|–±–æ—Ç–Ω–µ—Ç|ddos\b",
    r"\b–æ—Ä—É–∂–∏|–≤–∑—Ä—ã–≤—á–∞—Ç|–±–æ–º–±|–Ω–∞—Ä–∫–æ—Ç|–ø–æ—Ä–Ω–æ|—ç—Ä–æ—Ç–∏–∫|18\+",
    r"\b–ø–∞—Å–ø–æ—Ä—Ç|—Å–Ω–∏–ª—Å|–∏–Ω–Ω\b.*(—Å–≥–µ–Ω–µ—Ä|–ø–æ–¥–¥–µ–ª)",
    r"\b–æ–±–æ–π(–¥|—Ç–∏)\b.*–∞–Ω—Ç–∏–ø–ª–∞–≥|–∞–Ω—Ç–∏–ø–ª–∞–≥–∏–∞—Ç|antiplagiat",
    r"sql.?–∏–Ω—ä–µ–∫|–∏–Ω—ä–µ–∫—Ü–∏(—è|–∏) sql",
]

def safety_check(text: str) -> str | None:
    t = (text or "").lower()
    for p in _BANNED_PATTERNS:
        if re.search(p, t, flags=re.IGNORECASE):
            return ("–ó–∞–ø—Ä–æ—Å –Ω–∞—Ä—É—à–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ "
                    "(–≤–∑–ª–æ–º/–≤—Ä–µ–¥–æ–Ω–æ—Å/–æ–±—Ö–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π/NSFW/–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ).")
    return None

_ALLOWED_HINT_WORDS = [
    "–≤–∫—Ä", "–¥–∏–ø–ª–æ–º", "–∫—É—Ä—Å–æ–≤", "–º–µ—Ç–æ–¥–æ–ª–æ–≥", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–ª–∏—Ç–æ–±–∑–æ—Ä",
    "–≥–∏–ø–æ—Ç–µ–∑", "—Ü–µ–ª—å", "–∑–∞–¥–∞—á", "–≤–≤–µ–¥–µ–Ω–∏–µ", "–∑–∞–∫–ª—é—á–µ–Ω", "–æ–±–∑–æ—Ä",
    "–æ—Ñ–æ—Ä–º–ª–µ–Ω", "–≥–æ—Å—Ç", "—Ç–∞–±–ª–∏—Ü", "—Ä–∏—Å—É–Ω–∫", "–∞–Ω—Ç–∏–ø–ª–∞–≥", "–ø–ª–∞–≥–∏–∞—Ç",
    "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü", "–∑–∞—â–∏—Ç—É", "–æ–ø—Ä–æ—Å", "–∞–Ω–∫–µ—Ç–∞", "–º–µ—Ç–æ–¥—ã", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫",
]

def topical_check(text: str) -> str | None:
    """
    –ú—è–≥–∫–æ–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–õ–¨–ö–û –∫–∞–∫ –ø–æ–¥—Å–∫–∞–∑–∫—É,
    –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    """
    t = (text or "").lower()
    if not any(w in t for w in _ALLOWED_HINT_WORDS):
        return ("–ü–æ–¥—Å–∫–∞–∑–∫–∞: —Å–∏–ª—å–Ω–µ–µ –≤—Å–µ–≥–æ –æ—Ç–≤–µ—á–∞—é –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –í–ö–† (–≥–ª–∞–≤—ã, —Ç–∞–±–ª–∏—Ü—ã, —Ä–∏—Å—É–Ω–∫–∏, –≤—ã–≤–æ–¥—ã). "
                "–ï—Å–ª–∏ –ø—Ä–∏—à–ª—ë—Ç–µ —Ñ–∞–π–ª –¥–∏–ø–ª–æ–º–∞ ‚Äî —Å–º–æ–≥—É –æ–±—ä—è—Å–Ω—è—Ç—å –ø—Ä—è–º–æ –ø–æ –≤–∞—à–µ–º—É —Ç–µ–∫—Å—Ç—É.")
    return None

# --------------------- –ë–î / —É—Ç–∏–ª–∏—Ç—ã ---------------------

def _table_has_columns(con, table: str, cols: list[str]) -> bool:
    cur = con.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    have = {row[1] for row in cur.fetchall()}
    return all(c in have for c in cols)


def _current_embedding_profile() -> str:
    """
    –¢–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –ø–∏—à–µ–º –≤ layout_profile.
    –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ –Ω–∏—á–µ–≥–æ –Ω–µ –∑–∞–¥–∞–Ω–æ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º 'default'.
    """
    return getattr(Cfg, "EMBEDDING_PROFILE", "default")


# --------------------- –¢–∞–±–ª–∏—Ü—ã: –ø–∞—Ä—Å–∏–Ω–≥/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ---------------------

_TABLE_ANY = re.compile(r"\b—Ç–∞–±–ª–∏—Ü\w*|\b—Ç–∞–±–ª\.\b|\b—Ç–∞–±–ª–∏—Ü–∞\w*|(?:^|\s)table(s)?\b", re.IGNORECASE)
# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º: 2.1, 3, A.1, –ê.1, –ü1.2
_TABLE_TITLE_RE = re.compile(r"(?i)\b—Ç–∞–±–ª–∏—Ü–∞\s+(\d+(?:[.,]\d+)*|[a-z–∞-—è]\.?\s*\d+(?:[.,]\d+)*)\b(?:\s*[‚Äî\-‚Äì]\s*(.+))?")
_COUNT_HINT = re.compile(r"\b—Å–∫–æ–ª—å–∫–æ\b|how many", re.IGNORECASE)
_WHICH_HINT = re.compile(r"\b–∫–∞–∫–∏(–µ|—Ö)\b|\b—Å–ø–∏—Å–æ–∫\b|\b–ø–µ—Ä–µ—á–∏—Å–ª\w*\b|\b–Ω–∞–∑–æ–≤\w*\b", re.IGNORECASE)

# –ù–û–í–û–ï: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º "—Ç–∞–±–ª–∏—Ü–∞ 6", "—Ç–∞–±–ª. 6", "table 6.1" –∏ —Ç.–ø.
_TABLE_NUM_IN_TEXT_RE = re.compile(
    r"(?i)\b(?:—Ç–∞–±–ª–∏—Ü[–∞-—è]*|—Ç–∞–±–ª\.?|table)\s*([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*)"
)


def _extract_table_nums(text: str) -> list[str]:
    """–î–æ—Å—Ç–∞—ë–º –≤—Å–µ –Ω–æ–º–µ—Ä–∞ —Ç–∞–±–ª–∏—Ü –∏–∑ —Ñ—Ä–∞–∑—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    nums: list[str] = []
    for m in _TABLE_NUM_IN_TEXT_RE.finditer(text or ""):
        raw = (m.group(1) or "").strip()
        #  " 4 , 1 " -> "4.1"
        norm = raw.replace(" ", "").replace(",", ".")
        if norm:
            nums.append(norm)
    return nums

def _is_pure_table_request(text: str) -> bool:
    """
    –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∑–∞–ø—Ä–æ—Å –¢–û–õ–¨–ö–û –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
    (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–æ–ø–∏—à–∏ —Ç–∞–±–ª–∏—Ü—É 4", "—á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü–∞ 2.3"),
    –±–µ–∑ —Ä–∏—Å—É–Ω–∫–æ–≤, —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.
    """
    t = (text or "").strip()
    if not t:
        return False

    # –Ω–µ—Ç —Å–ª–æ–≤–∞ "—Ç–∞–±–ª–∏—Ü–∞" ‚Äî —Ç–æ—á–Ω–æ –Ω–µ –Ω–∞—à —Å–ª—É—á–∞–π
    if not _TABLE_ANY.search(t):
        return False

    # –Ω–µ—Ç –Ω–æ–º–µ—Ä–∞ –ø–æ—Å–ª–µ "—Ç–∞–±–ª–∏—Ü—ã" ‚Äî —Ç–æ–∂–µ –Ω–µ —á–∏—Å—Ç—ã–π –∑–∞–ø—Ä–æ—Å
    if not _TABLE_NUM_IN_TEXT_RE.search(t):
        return False

    # –µ—Å–ª–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –ø—Ä–æ —Ä–∏—Å—É–Ω–∫–∏ –∏–ª–∏ —Ä–∞–∑–¥–µ–ª—ã ‚Äî —ç—Ç–æ —É–∂–µ —Å–º–µ—à–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å
    if FIG_NUM_RE.search(t) or _SECTION_NUM_RE.search(t):
        return False

    return True

def _plural_tables(n: int) -> str:
    n_abs = abs(n) % 100
    n1 = n_abs % 10
    if 11 <= n_abs <= 14:
        return "—Ç–∞–±–ª–∏—Ü"
    if n1 == 1:
        return "—Ç–∞–±–ª–∏—Ü–∞"
    if 2 <= n1 <= 4:
        return "—Ç–∞–±–ª–∏—Ü—ã"
    return "—Ç–∞–±–ª–∏—Ü"

def _strip_table_prefix(s: str) -> str:
    return re.sub(r"^\[\s*—Ç–∞–±–ª–∏—Ü–∞\s*\]\s*", "", s or "", flags=re.IGNORECASE)

def _last_segment(name: str) -> str:
    s = (name or "").strip()
    if "/" in s:
        s = s.split("/")[-1].strip()
    s = _strip_table_prefix(s)
    s = re.sub(r"\s*[-‚Äì‚Äî]\s*", " ‚Äî ", s)
    s = re.sub(r"\s+", " ", s).strip(" .")
    return s

def _parse_table_title(text: str) -> tuple[str | None, str | None]:
    t = (text or "").strip()
    m = _TABLE_TITLE_RE.search(t)
    if not m:
        return (None, None)
    num = (m.group(1) or "").strip() or None
    title = (m.group(2) or "").strip() or None
    return (num, title)

def _table_num_variants(num: str) -> list[str]:
    """
    –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–∞–ø–∏—Å–∞–Ω–∏—è –Ω–æ–º–µ—Ä–∞ —Ç–∞–±–ª–∏—Ü—ã:
    6.1 ‚Üî 6,1; —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã.
    –ù—É–∂–Ω—ã, —á—Ç–æ–±—ã –Ω–∞—Ö–æ–¥–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É –¥–∞–∂–µ –µ—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ —Ç–æ—á–∫–∞,
    –∞ –≤ —Ç–µ–∫—Å—Ç–µ –∑–∞–ø—è—Ç–∞—è (–∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç).
    """
    raw = (str(num) or "").strip()
    if not raw:
        return []
    base = raw.replace(" ", "")
    dot = base.replace(",", ".")
    comma = base.replace(".", ",")
    variants = {base, dot, comma}
    return [v for v in variants if v]


def _shorten(s: str, limit: int = 120) -> str:
    """
    –ê–∫–∫—É—Ä–∞—Ç–Ω–æ –æ–±—Ä–µ–∑–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–æ limit —Å–∏–º–≤–æ–ª–æ–≤ —Å —Ç—Ä–æ–µ—Ç–æ—á–∏–µ–º.
    –°—Ç–∞—Ä–∞–µ–º—Å—è —Ä–µ–∑–∞—Ç—å –ø–æ –≥—Ä–∞–Ω–∏—Ü–µ —Å–ª–æ–≤–∞.
    """
    s = (s or "").strip()
    if len(s) <= limit:
        return s

    # –ø—ã—Ç–∞–µ–º—Å—è —Ä–µ–∑–∞—Ç—å –ø–æ –ø—Ä–æ–±–µ–ª—É, —á—Ç–æ–±—ã –Ω–µ —Ä—É–±–∏—Ç—å —Å–ª–æ–≤–æ –ø–æ–ø–æ–ª–∞–º
    cut = s.rfind(" ", 0, limit)
    # –µ—Å–ª–∏ –ø—Ä–æ–±–µ–ª —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ (–∏–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω) ‚Äî —Ä–µ–∂–µ–º —Ä–æ–≤–Ω–æ –ø–æ –ª–∏–º–∏—Ç—É
    if cut < max(10, limit // 2):
        cut = limit

    return s[:cut].rstrip(" .,;:‚Äì-") + "‚Ä¶"




# -------- –¢–∞–±–ª–∏—Ü—ã: –ø–æ–¥—Å—á—ë—Ç –∏ —Å–ø–∏—Å–æ–∫ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –ë–î) --------

def _distinct_table_basenames(uid: int, doc_id: int) -> list[str]:
    """
    –°–æ–±–∏—Ä–∞–µ–º ¬´–±–∞–∑–æ–≤—ã–µ¬ª –∏–º–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü (section_path –±–µ–∑ —Ö–≤–æ—Å—Ç–∞ ' [row ‚Ä¶]').
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ —Å –Ω–æ–≤—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ (table_row) –∏ —Å–æ —Å—Ç–∞—Ä—ã–º–∏.

    –ù–û–í–û–ï:
    - –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ attrs –∏ –ø–∞—Ä—Å–µ—Ä –ø—Ä–æ—Å—Ç–∞–≤–∏–ª is_table=true,
      —Å—á–∏—Ç–∞–µ–º ¬´–∂–∏–≤—ã–º–∏¬ª —Ç–æ–ª—å–∫–æ —Ç–∞–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã;
    - —Å—Ç–∞—Ä—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–µ–∑ attrs / –±–µ–∑ is_table –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –∫–∞–∫ —Ä–∞–Ω—å—à–µ.
    """
    con = get_conn()
    cur = con.cursor()

    has_et    = _table_has_columns(con, "chunks", ["element_type"])
    has_attrs = _table_has_columns(con, "chunks", ["attrs"])

    # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–ø–µ—Ä–µ—Ç—å—Å—è –Ω–∞ —Ç–∏–ø—ã
    if has_et:
        if has_attrs:
            # —Ç–æ–ª—å–∫–æ "–Ω–∞—Å—Ç–æ—è—â–∏–µ" DOCX-—Ç–∞–±–ª–∏—Ü—ã (–∏–ª–∏ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –±–µ–∑ attrs)
            cur.execute(
                """
                SELECT DISTINCT
                    CASE
                        WHEN instr(section_path, ' [row ')>0
                            THEN substr(section_path, 1, instr(section_path,' [row ')-1)
                        ELSE section_path
                    END AS base_name
                FROM chunks
                WHERE doc_id=? AND owner_id=?
                  AND element_type IN ('table','table_row')
                  AND (attrs IS NULL OR attrs LIKE '%"is_table": true%')
                """,
                (doc_id, uid),
            )
        else:
            # –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å –±–µ–∑ attrs ‚Äî –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∫–∞–∫ —Ä–∞–Ω—å—à–µ
            cur.execute(
                """
                SELECT DISTINCT
                    CASE
                        WHEN instr(section_path, ' [row ')>0
                            THEN substr(section_path, 1, instr(section_path,' [row ')-1)
                        ELSE section_path
                    END AS base_name
                FROM chunks
                WHERE doc_id=? AND owner_id=? AND element_type IN ('table','table_row')
                """,
                (doc_id, uid),
            )
    else:
        # –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å ‚Äî —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–∫—Å—Ç—É/section_path
        cur.execute(
            """
            SELECT DISTINCT
                CASE
                    WHEN instr(section_path, ' [row ')>0
                        THEN substr(section_path, 1, instr(section_path,' [row ')-1)
                    ELSE section_path
                END AS base_name
            FROM chunks
            WHERE doc_id=? AND owner_id=? AND (
                  lower(section_path) LIKE '%—Ç–∞–±–ª–∏—Ü–∞%'
               OR lower(text)        LIKE '%—Ç–∞–±–ª–∏—Ü–∞%'
               OR section_path LIKE '–¢–∞–±–ª–∏—Ü–∞ %' COLLATE NOCASE
               OR text        LIKE '[–¢–∞–±–ª–∏—Ü–∞]%' COLLATE NOCASE
               OR lower(section_path) LIKE '%table %'
               OR lower(text)        LIKE '%table %'
            )
            """,
            (doc_id, uid),
        )

    base_items = [r["base_name"] for r in cur.fetchall() if r["base_name"]]
    con.close()
    base_items = sorted(set(base_items), key=lambda s: s.lower())
    return base_items

def _count_tables(uid: int, doc_id: int) -> int:
    return len(_distinct_table_basenames(uid, doc_id))

def _compose_display_from_attrs(attrs_json: str | None, base: str, first_row_text: str | None) -> str:
    """
    –ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è:
      1) –µ—Å—Ç—å caption_num ‚Üí '–¢–∞–±–ª–∏—Ü–∞ N ‚Äî tail/header/firstrow' (–≤—Å–µ–≥–¥–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å).
      2) –Ω–µ—Ç –Ω–æ–º–µ—Ä–∞ ‚Üí –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ: caption_tail/ header_preview/ first_row.
      3) —Ñ–æ–ª–±—ç–∫: –ø–∞—Ä—Å–∏–º –Ω–æ–º–µ—Ä –∏ —Ö–≤–æ—Å—Ç –∏–∑ base ('–¢–∞–±–ª–∏—Ü–∞ N ‚Äî ...') –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å –Ω–æ–º–µ—Ä–æ–º.
      4) –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã—à–ª–æ ‚Äî –±–µ—Ä—ë–º –∫–æ—Ä–æ—Ç–∫–∏–π base –±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤.
    """
    num = None
    tail = None
    header_preview = None
    if attrs_json:
        try:
            a = json.loads(attrs_json or "{}")
            num = a.get("caption_num") or a.get("label")
            tail = a.get("caption_tail") or a.get("title")
            header_preview = a.get("header_preview")
        except Exception:
            pass

    if num:
        num = str(num).replace(",", ".").strip()
        tail_like = (tail or header_preview or first_row_text or "").strip()
        return f"–¢–∞–±–ª–∏—Ü–∞ {num}" + (f" ‚Äî {_shorten(tail_like, 160)}" if tail_like else "")

    # –±–µ–∑ –Ω–æ–º–µ—Ä–∞ –≤ attrs ‚Äî –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∏–∑ base –∏ –ø–æ–∫–∞–∑–∞—Ç—å –° –Ω–æ–º–µ—Ä–æ–º
    num_b, title_b = _parse_table_title(_last_segment(base))
    if num_b:
        text_tail = title_b or first_row_text or header_preview
        return f"–¢–∞–±–ª–∏—Ü–∞ {num_b}" + (f" ‚Äî {_shorten(text_tail, 160)}" if text_tail else "")

    # –±–µ–∑ –Ω–æ–º–µ—Ä–∞ ‚Äî —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ
    if tail:
        return _shorten(str(tail), 160)
    if header_preview:
        return _shorten(str(header_preview), 160)
    if first_row_text:
        return _shorten(first_row_text, 160)

    s = _last_segment(base)
    s = re.sub(r"(?i)^\s*—Ç–∞–±–ª–∏—Ü–∞\s+\d+(?:\.\d+)*\s*", "", s).strip(" ‚Äî‚Äì-")
    return _shorten(s or "–¢–∞–±–ª–∏—Ü–∞", 160)


# ------------------------------ –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ------------------------------

_SOURCES_HINT = re.compile(
    r"\b(–∏—Å—Ç–æ—á–Ω–∏–∫(?:–∏|–æ–≤)?|—Å–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã|—Å–ø–∏—Å–æ–∫\s+–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤|–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ\w*|references?|bibliograph\w*)\b",
    re.IGNORECASE
)
_REF_LINE_RE = re.compile(r"^\s*(?:\[\d+\]|\d+[.)])\s+.+", re.MULTILINE)

def _count_sources(uid: int, doc_id: int) -> int:
    """
    –ü–æ–¥—Å—á—ë—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
      1) –µ—Å–ª–∏ –≤ –ë–î –µ—Å—Ç—å element_type='reference' ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ;
      2) –∏–Ω–∞—á–µ —Å–æ–±–∏—Ä–∞–µ–º –ª—é–±—ã–µ –Ω–µ–ø—É—Å—Ç—ã–µ –∞–±–∑–∞—Ü—ã –≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–π ¬´–∏—Å—Ç–æ—á–Ω–∏–∫–∏/–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞/‚Ä¶¬ª
         (–±–µ–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–ª–∞—Å—å —Å –Ω–æ–º–µ—Ä–∞).
    """
    con = get_conn()
    cur = con.cursor()
    has_type = _table_has_columns(con, "chunks", ["element_type"])

    total = 0
    if has_type:
        cur.execute(
            "SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=? AND element_type='reference'",
            (uid, doc_id),
        )
        row = cur.fetchone()
        total = int(row["c"] or 0)

    if total == 0:
        items = set()
        cur.execute(
            """
            SELECT element_type, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
            ORDER BY page ASC, id ASC
            """,
            (uid, doc_id),
        )
        raw_rows = cur.fetchall()
        for r in raw_rows:
            sec = (r["section_path"] or "").lower()
            if not any(k in sec for k in ("–∏—Å—Ç–æ—á–Ω–∏–∫", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ", "reference", "bibliograph")):
                continue
            et = (r["element_type"] or "").lower()
            if et in ("heading", "table", "figure", "table_row"):
                continue
            t = (r["text"] or "").strip()
            if not t:
                continue
            k = re.sub(r"\s+", " ", t).strip().rstrip(".").lower()
            if len(k) >= 5:
                items.add(k)
        total = len(items)

    con.close()
    return total


# --------- –±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç: –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏ ---------
_PRACTICAL_Q = re.compile(r"(–µ—Å—Ç—å –ª–∏|–Ω–∞–ª–∏—á–∏–µ|–ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ª–∏|–∏–º–µ–µ—Ç—Å—è –ª–∏).{0,40}–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫", re.IGNORECASE)

def _has_practical_part(uid: int, doc_id: int) -> bool:
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        """
        SELECT 1
        FROM chunks
        WHERE owner_id=? AND doc_id=? AND (
            lower(section_path) LIKE '%–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫%' OR
            lower(text)         LIKE '%–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫%'
        )
        LIMIT 1
        """,
        (uid, doc_id),
    )
    row = cur.fetchone()
    con.close()
    return row is not None


# ------------- –ì–û–°–¢-–∏–Ω—Ç–µ–Ω—Ç –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ -------------

_GOST_HINT = re.compile(r"\b(–≥–æ—Å—Ç|–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏|—à—Ä–∏—Ñ—Ç|–º–µ–∂—Å—Ç—Ä–æ—á|–∫–µ–≥–ª|–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏|–ø–æ–ª—è|–æ—Ñ–æ—Ä–º–∏—Ç—å)\w*\b", re.IGNORECASE)

async def _maybe_run_gost(m: types.Message, uid: int, doc_id: int, text: str) -> bool:
    """–ï—Å–ª–∏ –ø–æ—Ö–æ–∂–µ, —á—Ç–æ –ø—Ä–æ—Å—è—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –ì–û–°–¢. –í–æ–∑–≤—Ä–∞—â–∞–µ–º True, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç–∏–ª–∏."""
    if not validate_gost or not render_report:
        return False
    if not _GOST_HINT.search(text or ""):
        return False

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    if not row:
        return False

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
    except Exception:
        return False

    report = validate_gost(sections)
    text_rep = render_report(report, max_issues=25)
    await _send(m, text_rep)
    return True

def _cap(s: str, limit: int = 950) -> str:
    """–û–±—Ä–µ–∑–∞–µ–º caption –¥–ª—è media (—É TG –ª–∏–º–∏—Ç ~1024 —Å–∏–º–≤–æ–ª–∞)."""
    s = (s or "").strip()
    if len(s) <= limit:
        return s
    return s[:limit - 1].rstrip() + "‚Ä¶"

def _safe_fs_input(path: str) -> FSInputFile | None:
    try:
        p = os.path.abspath(path or "")
        if not os.path.isfile(p):
            return None
        return FSInputFile(p)
    except Exception:
        return None

def _media_groups_from_cards(cards: list[dict], *, per_group: int = 10, per_figure: int = 4) -> list[list[InputMediaPhoto]]:
    """
    –°–æ–±–∏—Ä–∞–µ–º InputMediaPhoto –∏–∑ –∫–∞—Ä—Ç–æ—á–µ–∫ describe_figures_by_numbers.
    –ù–µ –±–æ–ª—å—à–µ FIG_MEDIA_LIMIT –≤—Å–µ–≥–æ, per_group ‚Äî –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Telegram (10).
    """
    media: list[InputMediaPhoto] = []
    total = 0
    for c in cards or []:
        disp = c.get("display") or f"–†–∏—Å—É–Ω–æ–∫ {c.get('num') or ''}".strip()
        imgs = (c.get("images") or [])[:per_figure]
        if not imgs:
            continue
        cap = _cap(disp)
        first = True
        for img in imgs:
            if total >= FIG_MEDIA_LIMIT:
                break
            fh = _safe_fs_input(img)
            if not fh:
                continue
            # caption —Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–µ —Ñ–æ—Ç–æ —Ä–∏—Å—É–Ω–∫–∞ (TG best-practice)
            media.append(InputMediaPhoto(media=fh, caption=cap if first else None))
            total += 1
            first = False
        if total >= FIG_MEDIA_LIMIT:
            break

    # —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—É
    groups: list[list[InputMediaPhoto]] = []
    for i in range(0, len(media), per_group):
        groups.append(media[i:i + per_group])
    return groups

async def _send_media_from_cards(m: types.Message, cards: list[dict]) -> bool:
    """
    –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–µ–¥–∏–∞–≥—Ä—É–ø–ø—ã –ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏.
    """
    groups = _media_groups_from_cards(cards)
    sent_any = False
    for g in groups:
        if not g:
            continue
        try:
            await m.answer_media_group(g)
            sent_any = True
        except TelegramBadRequest:
            # –µ—Å–ª–∏ –º–µ–¥–∏–∞-–≥—Ä—É–ø–ø–∞ –Ω–µ –∑–∞—à–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–¥–Ω–æ —Ñ–æ—Ç–æ) ‚Äî –æ—Ç–ø—Ä–∞–≤–∏–º –ø–æ—à—Ç—É—á–Ω–æ
            for item in g:
                try:
                    await m.answer_photo(item.media, caption=item.caption)
                    sent_any = True
                except Exception:
                    pass
        except Exception:
            pass
    return sent_any

# ------------------------------ helpers ------------------------------

def _parse_by_ext(path: str) -> list[dict]:
    fname = (os.path.basename(path) or "").lower()
    if fname.endswith(".docx"):
        return parse_docx(path)
    if fname.endswith(".doc"):
        return parse_doc(path)
    raise RuntimeError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é —Ç–æ–ª—å–∫–æ .doc –∏ .docx.")

def _first_chunks_context(owner_id: int, doc_id: int, n: int = 10, max_chars: int = 6000) -> str:
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT page, section_path, text FROM chunks "
        "WHERE owner_id=? AND doc_id=? "
        "ORDER BY page ASC, id ASC LIMIT ?",
        (owner_id, doc_id, n)
    )
    rows = cur.fetchall()
    con.close()
    if not rows:
        return ""
    parts, total = [], 0
    for r in rows:
        block = f"{(r['text'] or '').strip()}"
        if total + len(block) > max_chars:
            break
        parts.append(block)
        total += len(block)
    return "\n\n".join(parts)


def _ooxml_get_index(doc_id: int) -> dict | None:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç OOXML-–∏–Ω–¥–µ–∫—Å –∏–∑ –ø–∞–º—è—Ç–∏ –∏–ª–∏ —Å –¥–∏—Å–∫–∞. –°–Ω–∞—á–∞–ª–∞ runtime/indexes/<doc_id>.json,
    –∑–∞—Ç–µ–º —Ñ–æ–ª–±—ç–∫ ‚Äî –∏—â–µ–º json —Å —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º meta.file (–ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É)."""
    idx = OOXML_INDEX.get(doc_id)
    if idx:
        return idx

    p = os.path.join("runtime", "indexes", f"{doc_id}.json")
    if os.path.isfile(p):
        try:
            with open(p, "r", encoding="utf-8") as f:
                idx = json.load(f)
            OOXML_INDEX[doc_id] = idx
            return idx
        except Exception:
            pass

    # —Ñ–æ–ª–±—ç–∫: –ø–æ–¥–æ–±—Ä–∞—Ç—å –∏–Ω–¥–µ–∫—Å –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
    try:
        con = get_conn()
        cur = con.cursor()
        cur.execute("SELECT path FROM documents WHERE id=?", (doc_id,))
        row = cur.fetchone()
        con.close()
        doc_path = os.path.abspath(row["path"]) if row else None
        if doc_path:
            idx_dir = os.path.join("runtime", "indexes")
            if os.path.isdir(idx_dir):
                for name in os.listdir(idx_dir):
                    if not name.endswith(".json"):
                        continue
                    try:
                        with open(os.path.join(idx_dir, name), "r", encoding="utf-8") as f:
                            cand = json.load(f)
                        if (cand.get("meta") or {}).get("file") == doc_path:
                            OOXML_INDEX[doc_id] = cand
                            return cand
                    except Exception:
                        continue
    except Exception:
        pass
    return None


# –ù–µ–±–æ–ª—å—à–æ–π —Ö–µ–ª–ø–µ—Ä: –ø–æ–Ω—è—Ç—å, —á—Ç–æ –ø–æ–¥–ø–∏—Å—å –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂–∞ –Ω–∞ ¬´—Ç–∞–±–ª–∏—Ü—É¬ª, –∞ –Ω–µ –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ä–∏—Å—É–Ω–æ–∫
_TABLE_CAPTION_HINT_RE = re.compile(
    r"(?i)\b—Ç–∞–±–ª–∏—Ü\w*|\b—Ç–∞–±–ª\.\b|\b—Ç–∞–±–ª–∏—Ü–∞\w*|(?:^|\s)table(s)?\b"
)

def _looks_like_table_caption(rec: dict) -> bool:
    """
    –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∑–∞–ø–∏—Å—å –ø–æ—Ö–æ–∂–∞ –Ω–∞ —Ç–∞–±–ª–∏—Ü—É, –µ—Å–ª–∏:
      ‚Äî —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã —Ç–∏–ø/–≤–∏–¥ 'table/—Ç–∞–±–ª–∏—Ü–∞' –ò–õ–ò
      ‚Äî –≤ caption/title/name –µ—Å—Ç—å —Å–ª–æ–≤–æ ¬´—Ç–∞–±–ª–∏—Ü–∞¬ª/¬´table¬ª.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–±–æ—Ä–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ _ooxml_find_table_image.
    """
    if not isinstance(rec, dict):
        return False

    kind = str(rec.get("kind") or rec.get("type") or "").lower()
    if "table" in kind or "—Ç–∞–±–ª–∏—Ü" in kind:
        return True

    cap = (rec.get("caption") or rec.get("title") or rec.get("name") or "").strip()
    if not cap:
        return False

    return bool(_TABLE_CAPTION_HINT_RE.search(cap))


def _ooxml_find_figure_by_label(idx: dict, num_str: str) -> dict | None:
    """
    –ò—â–µ–º –∑–∞–ø–∏—Å—å –æ —Ä–∏—Å—É–Ω–∫–µ –ø–æ –Ω–æ–º–µ—Ä—É –≤–∏–¥–∞ '2.3' –∏–∑ –ø–æ–¥–ø–∏—Å–∏.
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–º–µ–Ω–Ω–æ —Ç–µ–∫—Å—Ç –≤ caption/title, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Ü–µ–ª—É—é —á–∞—Å—Ç—å.
    """
    target = _num_norm_fig(num_str)
    if not target:
        return None
    figs = (idx or {}).get("figures") or []
    for f in figs:
        cap = (f.get("caption") or f.get("title") or "").strip()
        m = _FIG_TITLE_RE.search(cap)
        if not m:
            continue
        cap_num = _num_norm_fig(m.group(2))
        if cap_num == target:
            return f
    return None


# NEW: –ø–æ–∏—Å–∫ ¬´—Ç–∞–±–ª–∏—Ü—ã-–∫–∞—Ä—Ç–∏–Ω–∫–∏¬ª –ø–æ –ø–æ–¥–ø–∏—Å–∏ "–¢–∞–±–ª–∏—Ü–∞ N ..."
# NEW: –ø–æ–∏—Å–∫ ¬´—Ç–∞–±–ª–∏—Ü—ã-–∫–∞—Ä—Ç–∏–Ω–∫–∏¬ª –ø–æ –ø–æ–¥–ø–∏—Å–∏ "–¢–∞–±–ª–∏—Ü–∞ N ..."
def _ooxml_find_table_image(idx: dict, num: str) -> dict | None:
    """
    –ò—â–µ–º –∑–∞–ø–∏—Å—å, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Ç–∞–±–ª–∏—Ü–µ {num}, –∫–æ—Ç–æ—Ä–∞—è –≤—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–æ–π.
    –°–º–æ—Ç—Ä–∏–º –∏ –≤ figures, –∏ –≤ tables (–µ—Å–ª–∏ —Ç–∞–º –µ—Å—Ç—å image_path).

    –í–ê–ñ–ù–û:
      ‚Äî –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ tables –±–µ—Ä—ë–º –≤—Å—ë;
      ‚Äî –∏–∑ figures –±–µ—Ä—ë–º –¢–û–õ–¨–ö–û –∑–∞–ø–∏—Å–∏, –ø–æ–¥–ø–∏—Å—å –∫–æ—Ç–æ—Ä—ã—Ö –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ —Ç–∞–±–ª–∏—Ü–∞
        (_looks_like_table_caption), —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–ø—É—Ç–∞—Ç—å —Å ¬´–†–∏—Å—É–Ω–æ–∫ 6¬ª.

    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
      1) —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø–æ–ª—è–º (caption_num/label/num/n)
         –¢–û–õ–¨–ö–û —Å—Ä–µ–¥–∏ ¬´—Ç–∞–±–ª–∏—á–Ω—ã—Ö¬ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤;
      2) —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –ø–æ–¥–ø–∏—Å–∏ '–¢–∞–±–ª–∏—Ü–∞ {num} ...'.
    """
    if not idx:
        return None

    target = str(num).replace(" ", "").replace(",", ".")

    def _iter_candidates():
        """
        –î–∞—ë–º (kind, rec):
          kind == 'tables'  ‚Üí –≤—Å–µ–≥–¥–∞ —Å—á–∏—Ç–∞–µ–º —Ç–∞–±–ª–∏—Ü–µ–π;
          kind == 'figures' ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–æ–¥–ø–∏—Å—å –ø–æ—Ö–æ–∂–∞ –Ω–∞ —Ç–∞–±–ª–∏—Ü—É.
        """
        for kind in ("figures", "tables"):
            coll = (idx.get(kind) or [])
            if isinstance(coll, dict):
                coll = list(coll.values())
            if not isinstance(coll, list):
                continue
            for rec in coll:
                if not isinstance(rec, dict):
                    continue
                # –≤—Å—ë –∏–∑ tables –±–µ—Ä—ë–º –±–µ–∑ —É—Å–ª–æ–≤–∏–π,
                # –∞ –∏–∑ figures ‚Äî —Ç–æ–ª—å–∫–æ ¬´—Ç–∞–±–ª–∏—á–Ω—ã–µ¬ª –ø–æ–¥–ø–∏—Å–∏
                if kind == "figures" and not _looks_like_table_caption(rec):
                    continue
                yield kind, rec

    # 1) –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø–æ–ª—è–º (caption_num/label/num/n)
    for _kind, rec in _iter_candidates():
        for fld in ("caption_num", "label", "num", "n"):
            raw = rec.get(fld)
            if not raw:
                continue
            cand = str(raw).replace(" ", "").replace(",", ".")
            if cand == target:
                return rec

    # 2) –§–æ–ª–±—ç–∫ ‚Äî –ø–æ —Ç–µ–∫—Å—Ç—É –ø–æ–¥–ø–∏—Å–∏
    for _kind, rec in _iter_candidates():
        cap = (rec.get("caption") or rec.get("title") or rec.get("name") or "").strip()
        if not cap:
            continue
        # "–¢–∞–±–ª–∏—Ü–∞ 6", "—Ç–∞–±–ª–∏—Ü–∞ 6 ‚Äì ...", "–¢–∞–±–ª–∏—Ü–∞ 6. ..."
        m = re.search(r"(?i)\b—Ç–∞–±–ª–∏—Ü\w*\s+(\d+(?:[.,]\d+)*)", cap)
        if not m:
            continue
        cap_num = (m.group(1) or "").replace(" ", "").replace(",", ".")
        if cap_num == target:
            return rec

    return None


# NEW: OCR-—Ñ–æ–ª–±—ç–∫ –¥–ª—è —Ç–∞–±–ª–∏—Ü, –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–∏–Ω–∫–æ–π
def _ocr_table_block_from_image(uid: int, doc_id: int, num: str) -> str | None:
    key = (doc_id, str(num))
    cached = _OCR_TABLE_CACHE.get(key)
    if cached is not None:
        logging.info("TAB[img] cache hit for doc=%s, table=%s", doc_id, num)
        return cached

    # 1) —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º OOXML-–∏–Ω–¥–µ–∫—Å
    idx = _ooxml_get_index(doc_id)
    if not idx:
        logging.info("TAB[img] no OOXML index for doc=%s", doc_id)
    rec = _ooxml_find_table_image(idx, num) if idx else None

    img_path: str | None = None
    if rec:
        # 1) —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ–ª—è
        img_path = rec.get("image_path") or rec.get("image")

        # 2) –∏–Ω–æ–≥–¥–∞ ooxml_lite –∫–ª–∞–¥—ë—Ç —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫
        if not img_path:
            imgs = rec.get("images") or rec.get("imgs") or []
            if isinstance(imgs, (list, tuple)) and imgs:
                img_path = imgs[0]

        logging.info(
            "TAB[img] OOXML candidate for table %s: image_path=%r",
            num,
            img_path,
        )


    # 2) –µ—Å–ª–∏ –∏–∑ OOXML –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –ø—Ä–æ–±—É–µ–º –¥–æ—Å—Ç–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É —á–µ—Ä–µ–∑ retrieve(...)
    if not img_path:
        try:
            logging.info("TAB[img] fallback retrieve() for table %s", num)
            hits = retrieve(uid, doc_id, f"–¢–∞–±–ª–∏—Ü–∞ {num}", top_k=6)
        except Exception as e:
            logging.exception("TAB[img] retrieve() failed for table %s: %s", num, e)
            hits = []

        if hits:
            paths = _pick_images_from_hits(hits, limit=1)
            logging.info("TAB[img] retrieve() returned image paths=%r", paths)
            if paths:
                img_path = paths[0]

    if not img_path or not os.path.isfile(img_path):
        logging.info(
            "TAB[img] no image found on disk for table %s (img_path=%r)",
            num,
            img_path,
        )
        return None

    # –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–∞—Ä –∑–Ω–∞—á–µ–Ω–∏–π
    all_pairs: list[dict] = []
    # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫—É—Å–∫–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–≤ —Ç.—á. ¬´–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ¬ª)
    extra_text_parts: list[str] = []

    def _add_pairs(pairs: list[dict] | None) -> None:
        """–ê–∫–∫—É—Ä–∞—Ç–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä—ã –±–µ–∑ –≥—Ä—É–±—ã—Ö –¥—É–±–ª–µ–π –ø–æ (label, value, unit)."""
        nonlocal all_pairs
        if not pairs:
            return
        seen = {(str(p.get("label") or "").strip(),
                 str(p.get("value") or "").strip(),
                 str(p.get("unit") or "").strip())
                for p in all_pairs}
        for p in pairs:
            key_p = (
                str(p.get("label") or "").strip(),
                str(p.get("value") or "").strip(),
                str(p.get("unit") or "").strip(),
            )
            if key_p in seen:
                continue
            seen.add(key_p)
            all_pairs.append(p)

    # 3.a) —Å–ø–µ—Ü-—Ñ—É–Ω–∫—Ü–∏—è –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º-–∫–∞—Ä—Ç–∏–Ω–∫–∞–º
    if vision_extract_table_values is not None:
        try:
            pairs1 = vision_extract_table_values(img_path, lang="ru") or []
            _add_pairs(pairs1)
        except Exception as e:
            logging.exception(
                "ocr_table_block_from_image: vision_extract_table_values failed for %s (table %s): %s",
                img_path,
                num,
                e,
            )

    # 3.b) –æ–±—â–∏–π vision-–∞–Ω–∞–ª–∏–∑: –∏ –ø–∞—Ä—ã, –∏ —Ç–µ–∫—Å—Ç (—á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç ¬´–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ¬ª)
    if va_analyze_figure is not None:
        try:
            try:
                res = va_analyze_figure(
                    img_path,
                    caption_hint=f"–¢–∞–±–ª–∏—Ü–∞ {num}",
                    lang="ru",
                )
            except TypeError:
                res = va_analyze_figure(img_path, lang="ru")  # type: ignore

            if isinstance(res, dict):
                pairs2 = res.get("data") or []
                _add_pairs(pairs2)
                txt2 = (res.get("text") or "").strip()
                if txt2:
                    extra_text_parts.append(txt2)
            else:
                txt2 = (str(res) or "").strip()
                if txt2:
                    extra_text_parts.append(txt2)
        except Exception as e:
            logging.exception(
                "ocr_table_block_from_image: va_analyze_figure failed for %s (table %s): %s",
                img_path,
                num,
                e,
            )

    # 3.c) –æ–±—â–∏–π extractor –ø–∞—Ä label/value
    if vision_extract_values is not None:
        try:
            pairs3 = vision_extract_values(img_path, lang="ru") or []
            _add_pairs(pairs3)
        except Exception as e:
            logging.exception(
                "ocr_table_block_from_image: vision_extract_values failed for %s (table %s): %s",
                img_path,
                num,
                e,
            )



    values_block = _pairs_to_bullets(all_pairs) if all_pairs else ""
    values_block = (values_block or "").strip()
    extra_text = "\n".join(
        t.strip() for t in extra_text_parts if t and t.strip()
    )

    if not values_block and not extra_text:
        return None

    lines: list[str] = [f"–¢–∞–±–ª–∏—Ü–∞ {num} (—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é):"]
    if values_block:
        lines.append(values_block)
    if extra_text:
        lines.append("")  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        lines.append("[–¢–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è]")
        lines.append(extra_text)

    out = "\n".join(lines).strip()
    _OCR_TABLE_CACHE[key] = out
    return out


# ---------- verbatim fallback –ø–æ —Ü–∏—Ç–∞—Ç–µ (—à–∏–Ω–≥–ª—ã + LIKE/NOCASE) ----------

def _normalize_for_like(s: str) -> str:
    s = (s or "")
    s = s.replace("\u00A0", " ")  # NBSP -> –ø—Ä–æ–±–µ–ª
    s = s.replace("¬´", '"').replace("¬ª", '"').replace("‚Äú", '"').replace("‚Äù", '"')
    s = s.replace("‚Äô", "'").replace("‚Äò", "'")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _make_shingles(s: str, min_len: int = 30, max_len: int = 90, step: int = 25) -> list[str]:
    s = _normalize_for_like(s)
    if not s:
        return []
    if len(s) <= max_len:
        return [s]
    out = []
    i = 0
    while i < len(s):
        chunk = s[i:i + max_len]
        if len(chunk) >= min_len:
            out.append(chunk)
        i += step
    return out[:6]

def verbatim_find(owner_id: int, doc_id: int, q_text: str, max_hits: int = 3) -> list[dict]:
    shingles = _make_shingles(q_text)
    if not shingles:
        return []
    con = get_conn()
    cur = con.cursor()
    hits: list[dict] = []
    for sh in shingles:
        pattern = f"%{sh}%"
        cur.execute(
            """
            SELECT page, section_path, text FROM chunks
            WHERE owner_id=? AND doc_id=? AND
                  replace(text, char(160), ' ') LIKE ? COLLATE NOCASE
            ORDER BY page ASC, id ASC
            LIMIT ?
            """,
            (owner_id, doc_id, pattern, max_hits - len(hits)),
        )
        for r in cur.fetchall():
            t = (r["text"] or "")
            t_norm = _normalize_for_like(t)
            pos = t_norm.lower().find(_normalize_for_like(sh).lower())
            if pos >= 0:
                s = max(pos - 120, 0)
                e = min(pos + len(sh) + 120, len(t_norm))
                hits.append({
                    "page": r["page"],
                    "section_path": r["section_path"],
                    "snippet": t_norm[s:e].strip(),
                })
            if len(hits) >= max_hits:
                con.close()
                return hits
    con.close()
    return hits


# ------------------------------ /start ------------------------------

@dp.message(Command("start"))
async def start(m: types.Message):
    ensure_user(str(m.from_user.id))
    await _send(m,
        "–ü—Ä–∏–≤–µ—Ç! –Ø —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ —Ç–≤–æ–µ–π –í–ö–†. –ü—Ä–∏—à–ª–∏ —Ñ–∞–π–ª –í–ö–† ‚Äî —è –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä—É—é –∏ –±—É–¥—É –æ–±—ä—è—Å–Ω—è—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: –≥–ª–∞–≤—ã –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, —Å–º—ã—Å–ª —Ç–∞–±–ª–∏—Ü/—Ä–∏—Å—É–Ω–∫–æ–≤, –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –∫ –∑–∞—â–∏—Ç–µ. –ú–æ–∂–µ—à—å –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç—å –≤–æ–ø—Ä–æ—Å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –Ω–∞–ø–∏—Å–∞—Ç—å –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º."
    )


# ------------------------------ /diag ------------------------------

@dp.message(Command("diag"))
async def cmd_diag(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid) or get_user_active_doc(uid)
    if not doc_id:
        await _send(m, "–ê–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ—Ç. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª –í–ö–† —Å–Ω–∞—á–∞–ª–∞.")
        return

    # –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ë–î
    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    path = row["path"] if row else "‚Äî"

    cur.execute("SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=?", (uid, doc_id))
    chunks_cnt = int(cur.fetchone()["c"])

    con.close()

    tables_cnt = _count_tables(uid, doc_id)
    figures_cnt = _list_figures_db(uid, doc_id, limit=999999)["count"]
    # NEW: –µ—Å–ª–∏ –≤ –ë–î 0 ‚Äî –≤–æ–∑—å–º—ë–º —á–∏—Å–ª–æ —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ OOXML-–∏–Ω–¥–µ–∫—Å–∞
    if figures_cnt == 0:
        idx_oox = _ooxml_get_index(doc_id)
        if idx_oox:
            figures_cnt = len(idx_oox.get("figures", []))
    sources_cnt = _count_sources(uid, doc_id)

    indexer_ver = get_document_indexer_version(doc_id) or 0

    txt = (
        f"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ #{doc_id}\n"
        f"‚Äî –ü—É—Ç—å: {path}\n"
        f"‚Äî –ß–∞–Ω–∫–æ–≤: {chunks_cnt}\n"
        f"‚Äî –¢–∞–±–ª–∏—Ü: {tables_cnt}\n"
        f"‚Äî –†–∏—Å—É–Ω–∫–æ–≤: {figures_cnt}\n"
        f"‚Äî –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {sources_cnt}\n"
        f"‚Äî –í–µ—Ä—Å–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: {indexer_ver} (—Ç–µ–∫—É—â–∞—è {CURRENT_INDEXER_VERSION})\n"
    )
    await _send(m, txt)


# ------------------------------ /reindex ------------------------------

@dp.message(Command("reindex"))
async def cmd_reindex(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid) or get_user_active_doc(uid)
    if not doc_id:
        await _send(m, "–ê–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ—Ç. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª —Å–Ω–∞—á–∞–ª–∞.")
        return

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()

    if not row:
        await _send(m, "–ù–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–Ω–æ–≤–æ.")
        return

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
        # –æ–±–æ–≥–∞—â–∞–µ–º —Å–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–æ–º
        sections = enrich_sections(sections, doc_kind=os.path.splitext(path)[1].lower().strip("."))
        delete_document_chunks(doc_id, uid)
        index_document(uid, doc_id, sections)
        invalidate_cache(uid, doc_id)
        set_document_indexer_version(doc_id, CURRENT_INDEXER_VERSION)
        update_document_meta(doc_id, layout_profile=_current_embedding_profile())
        await _send(m, f"–î–æ–∫—É–º–µ–Ω—Ç #{doc_id} –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω.")
    except Exception as e:
        logging.exception("reindex failed: %s", e)
        await _send(m, f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}")



# ---------- –†–∏—Å—É–Ω–∫–∏: –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–ª–æ–∫–∞–ª—å–Ω—ã–µ, –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç retrieval.py) ----------

_FIG_TITLE_RE = re.compile(
    r"(?i)\b(—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|—Å—Ö–µ–º(?:–∞|—ã)?|–∫–∞—Ä—Ç–∏–Ω(?:–∫–∞|–∫–∏)?|figure|fig\.?|picture|pic\.?)"
    r"\s*(?:‚Ññ\s*)?(\d+(?:[.,]\d+)*)\b(?:\s*[‚Äî\-‚Äì:\u2013\u2014]\s*(.+))?"
)

# –í–∫–ª—é—á–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å –∫–∞—Ä—Ç–∏–Ω–æ–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
FIG_VALUES_DEFAULT: bool = getattr(Cfg, "FIG_VALUES_DEFAULT", True)

def _compose_figure_display(attrs_json: str | None, section_path: str, title_text: str | None) -> str:
    """–î–µ–ª–∞–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∏—Å—É–Ω–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º."""
    num = None
    tail = None
    if attrs_json:
        try:
            a = json.loads(attrs_json or "{}")
            num  = str(a.get("caption_num") or a.get("label") or "").strip()
            tail = str(a.get("caption_tail") or a.get("title") or "").strip()
        except Exception:
            pass

    if not num or not num.strip():
        cand = title_text or section_path or ""
        m = _FIG_TITLE_RE.search(cand)
        if m:
            num = (m.group(2) or "").replace(",", ".").strip()
            if not tail:
                tail = (m.group(3) or "").strip()

    if num:
        return f"–†–∏—Å—É–Ω–æ–∫ {num}" + (f" ‚Äî {_shorten(tail, 160)}" if tail else "")
    base = title_text or _last_segment(section_path or "")
    base = re.sub(
    r"(?i)^\s*(—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|—Å—Ö–µ–º(?:–∞|—ã)?|–∫–∞—Ä—Ç–∏–Ω(?:–∫–∞|–∫–∏)?|figure|fig\.?|picture|pic\.?)\s*",
        "", base
    ).strip(" ‚Äî‚Äì-")
    return _shorten(base or "–†–∏—Å—É–Ω–æ–∫", 160)

# ---------- NEW: —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ DOCX-–≥—Ä–∞—Ñ–∏–∫–æ–≤ (chart_data) ----------

def _fetch_figure_row_by_num(uid: int, doc_id: int, num: str):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É chunks –¥–ª—è —Ä–∏—Å—É–Ω–∫–∞ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –Ω–æ–º–µ—Ä–æ–º (–µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–∞),
    –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É—è—Å—å –¢–û–õ–¨–ö–û –Ω–∞ caption_num (–Ω–æ–º–µ—Ä —Ä–∏—Å—É–Ω–∫–∞ –≤ –ø–æ–¥–ø–∏—Å–∏).
    –ü–æ label –±–æ–ª—å—à–µ –Ω–µ –∏—â–µ–º, —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å –Ω–æ–º–µ—Ä —Ä–∏—Å—É–Ω–∫–∞ —Å –ø–æ–¥–ø–∏—Å—è–º–∏ —Å–µ—Ä–∏–π/–∫–∞—Ç–µ–≥–æ—Ä–∏–π.
    """
    con = get_conn()
    cur = con.cursor()
    like1 = f'%\"caption_num\": \"{num}\"%'
    row = None

    # 1) –ø–æ –Ω–æ–º–µ—Ä—É –≤ attrs (caption_num)
    try:
        cur.execute(
            """
            SELECT id, page, section_path, attrs, text
            FROM chunks
            WHERE owner_id=? AND doc_id=? AND element_type='figure'
              AND attrs LIKE ?
            ORDER BY id ASC LIMIT 1
            """,
            (uid, doc_id, like1),
        )
        row = cur.fetchone()
    except Exception:
        row = None

    # 2) —Ñ–æ–ª–±—ç–∫ ‚Äî –ø–æ section_path
    if not row:
        try:
            cur.execute(
                """
                SELECT id, page, section_path, attrs, text
                FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='figure'
                  AND section_path LIKE ? COLLATE NOCASE
                ORDER BY id ASC LIMIT 1
                """,
                (uid, doc_id, f'%–†–∏—Å—É–Ω–æ–∫ {num}%'),
            )
            row = cur.fetchone()
        except Exception:
            row = None

    con.close()
    return row

def _figure_following_paragraphs(
    uid: int,
    doc_id: int,
    fig_row,
    max_paragraphs: int = 3,
    max_chars: int = 1500,
) -> list[str]:
    """
    –ë–µ—Ä—ë–º 2‚Äì3 –∞–±–∑–∞—Ü–∞ —Ç–µ–∫—Å—Ç–∞ –ü–û–°–õ–ï —Ä–∏—Å—É–Ω–∫–∞ –≤ —Ç–æ–º –∂–µ section_path.
    –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è, –∫–∞–∫ —Ç–æ–ª—å–∫–æ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π heading/table/figure.
    """
    if not fig_row:
        return []

    base_id = fig_row["id"]
    sec = fig_row["section_path"] or ""

    con = get_conn()
    cur = con.cursor()

    has_et = _table_has_columns(con, "chunks", ["element_type"])

    if has_et:
        cur.execute(
            """
            SELECT text, element_type
            FROM chunks
            WHERE owner_id=? AND doc_id=? AND id>? AND section_path=?
            ORDER BY id ASC
            LIMIT 20
            """,
            (uid, doc_id, base_id, sec),
        )
    else:
        cur.execute(
            """
            SELECT text, NULL AS element_type
            FROM chunks
            WHERE owner_id=? AND doc_id=? AND id>? AND section_path=?
            ORDER BY id ASC
            LIMIT 20
            """,
            (uid, doc_id, base_id, sec),
        )

    rows = cur.fetchall() or []
    con.close()

    paras: list[str] = []
    total = 0

    for r in rows:
        et = (r["element_type"] or "").lower() if "element_type" in r.keys() else ""
        if et in ("heading", "table", "figure", "table_row"):
            # –¥–æ—à–ª–∏ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –±–ª–æ–∫–∞ ‚Äî –¥–∞–ª—å—à–µ —É–∂–µ –Ω–µ ¬´–ø—Ä–æ —ç—Ç–æ—Ç —Ä–∏—Å—É–Ω–æ–∫¬ª
            break
        t = (r["text"] or "").strip()
        if not t:
            continue

        paras.append(t)
        total += len(t)
        if len(paras) >= max_paragraphs or total >= max_chars:
            break

    return paras


def _parse_chart_data(attrs_json: str | None) -> tuple[list | None, str | None, dict]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å—Ö–µ–º attrs.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (data_rows, chart_type, attrs_dict), –≥–¥–µ data_rows ‚Äî —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    –≤–∏–¥–∞ {"label": ..., "value": ..., "unit": ...}.
    """
    try:
        a = json.loads(attrs_json or "{}")

        # —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        raw = (a.get("chart_data")
            or (a.get("chart") or {}).get("data")
            or a.get("data")
            or a.get("series"))
        ctype = (a.get("chart_type")
                or (a.get("chart") or {}).get("type")
                or a.get("type"))


        # –£–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ [{label, value, unit?}]
        if isinstance(raw, list) and raw:
            return raw, ctype, a

        # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞: {"categories":[...], "series":[{"name":..., "values":[...], "unit":"%"}]}
        if isinstance(raw, dict) and raw.get("categories") and raw.get("series"):
            cats = list(raw.get("categories") or [])
            s0   = (raw.get("series") or [{}])[0] or {}
            vals = list(s0.get("values") or s0.get("data") or [])
            unit = s0.get("unit")
            rows = []
            for i in range(min(len(cats), len(vals))):
                rows.append({
                    "label": str(cats[i]),
                    "value": vals[i],
                    "unit": unit
                })
            if rows:
                return rows, (ctype or s0.get("type") or "chart"), a
    except Exception:
        pass
    return None, None, {}

def _extract_raw_values_from_attrs(attrs_json: str | None) -> dict | None:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –≤—ã—Ç–∞—â–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã –ò–ó OOXML-–∞—Ç—Ä–∏–±—É—Ç–æ–≤.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict –≤–∏–¥–∞:
    {
      "categories": [...],
      "series": [
        {"name": "–ù–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å", "unit": "%", "values": [65, 60, ...]},
        ...
      ]
    }
    –∏–ª–∏ None, –µ—Å–ª–∏ —Ç–∞–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ—Ç.

    –í–ê–ñ–ù–û:
    - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –∏–∑ –Ω–æ–≤–æ–≥–æ ooxml_lite (chart.cats + chart.series);
    - –µ—Å–ª–∏ chart_data ‚Äî —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ [{label, value, unit, series_name}],
      —Å–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–µ —Å–µ—Ä–∏–∏, –∞ –Ω–µ —Å—Ö–ª–æ–ø—ã–≤–∞–µ–º –≤—Å—ë –≤ –æ–¥–Ω—É.
    """
    if not attrs_json:
        return None
    try:
        a = json.loads(attrs_json or "{}")
    except Exception:
        return None

    # 1) –¢–∏–ø–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç OOXML-–∏–Ω–¥–µ–∫—Å–∞: {"chart_data": {"categories": [...], "series": [...]}},
    #    –ª–∏–±–æ {"chart": {...}}, –ª–∏–±–æ {"data": {...}}.
    for key in ("chart_data", "chart", "data"):
        raw = a.get(key)
        if isinstance(raw, dict) and raw.get("categories") and raw.get("series"):
            cats = list(raw.get("categories") or [])
            series_out: list[dict] = []
            for s in (raw.get("series") or []):
                if not isinstance(s, dict):
                    continue
                name = s.get("name")
                unit = s.get("unit")
                vals = list(s.get("values") or s.get("data") or [])
                series_out.append({
                    "name": name,
                    "unit": unit,
                    "values": vals,
                })
            if cats and series_out:
                return {"categories": cats, "series": series_out}

    # 1–∞) –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–∑ ooxml_lite: "chart": {"cats": [...], "series": [...]}
    chart = a.get("chart")
    if isinstance(chart, dict) and chart.get("series"):
        cats = chart.get("cats") or chart.get("categories") or []
        cats = [str(c) for c in cats]
        series_out: list[dict] = []
        for s in (chart.get("series") or []):
            if not isinstance(s, dict):
                continue
            name = s.get("name")
            unit = s.get("unit")
            # –≤ ooxml_lite –º—ã —É–∂–µ –∫–ª–∞–¥—ë–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ s["values"]
            vals = list(s.get("values") or s.get("data") or s.get("vals") or [])
            series_out.append({
                "name": name,
                "unit": unit,
                "values": vals,
            })
        if cats and series_out:
            return {"categories": cats, "series": series_out}

    # 2) chart_data –∫–∞–∫ –°–ü–ò–°–û–ö —Å—Ç—Ä–æ–∫ [{label, value, unit, series_name, ...}]
    raw_rows = a.get("chart_data")
    if isinstance(raw_rows, list) and raw_rows:
        # 2.1. –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Äî —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ label'—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è
        categories: list[str] = []
        cat_index: dict[str, int] = {}
        for r in raw_rows:
            label = str(
                r.get("label")
                or r.get("name")
                or r.get("category")
                or ""
            ).strip()
            if label not in cat_index:
                cat_index[label] = len(categories)
                categories.append(label)

        if not categories:
            return None

        # 2.2. –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ series_name (–µ—Å–ª–∏ –µ—Å—Ç—å), –∏–Ω–∞—á–µ –≤—Å—ë –≤ –æ–¥–Ω—É —Å–µ—Ä–∏—é None
        series_map: dict[str | None, dict] = {}
        any_named_series = False

        for r in raw_rows:
            # –∏–º—è —Å–µ—Ä–∏–∏
            raw_sname = (
                r.get("series_name")
                or r.get("series")
                or r.get("name")
            )
            sname = str(raw_sname).strip() if raw_sname is not None else None
            if sname:
                any_named_series = True
                key = sname
            else:
                key = None  # –±–µ–∑—ã–º—è–Ω–Ω–∞—è —Å–µ—Ä–∏—è

            # –∫–∞—Ç–µ–≥–æ—Ä–∏—è ‚Üí –∏–Ω–¥–µ–∫—Å
            label = str(
                r.get("label")
                or r.get("name")
                or r.get("category")
                or ""
            ).strip()
            idx = cat_index.get(label)
            if idx is None:
                continue

            # –∑–Ω–∞—á–µ–Ω–∏–µ –∏ unit
            v = r.get("value")
            if v is None:
                v = r.get("y") or r.get("x") or r.get("v") or r.get("count")

            unit = r.get("unit")
            unit_str = str(unit).strip() if isinstance(unit, str) else None

            if key not in series_map:
                series_map[key] = {
                    "name": key,
                    "unit": unit_str,
                    "values": [None] * len(categories),
                }

            # –µ—Å–ª–∏ –≤ —Å–µ—Ä–∏–∏ unit –µ—â—ë –Ω–µ –ø—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω, –∞ —Ç—É—Ç –µ—Å—Ç—å ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ–º
            if unit_str and not series_map[key].get("unit"):
                series_map[key]["unit"] = unit_str

            series_map[key]["values"][idx] = v

        series_out = list(series_map.values())

        # –µ—Å–ª–∏ –∏–º—ë–Ω —Å–µ—Ä–∏–π –Ω–µ—Ç –≤–æ–æ–±—â–µ ‚Äî –≤—Å—ë —Ä–∞–≤–Ω–æ –≤–µ—Ä–Ω—ë–º –æ–¥–Ω—É —Å–µ—Ä–∏—é,
        # —á—Ç–æ–±—ã _format_exact_values –º–æ–≥ –µ—ë –∫—Ä–∞—Å–∏–≤–æ –æ—Ñ–æ—Ä–º–∏—Ç—å
        if not series_out and raw_rows:
            vals: list = []
            unit: str | None = None
            for r in raw_rows:
                vv = r.get("value")
                if vv is None:
                    vv = r.get("y") or r.get("x") or r.get("v") or r.get("count")
                vals.append(vv)
                if isinstance(r.get("unit"), str):
                    unit = r.get("unit")
            if vals:
                series_out = [{
                    "name": None,
                    "unit": unit,
                    "values": vals,
                }]

        if categories and series_out:
            return {
                "categories": categories,
                "series": series_out,
            }

    return None



def _format_exact_values(raw_values: dict) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º raw_values, –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ–±—Ä–∞—â–∞—è—Å—å —Å –¥–æ–ª—è–º–∏:
    –µ—Å–ª–∏ unit —Å–æ–¥–µ—Ä–∂–∏—Ç '%' –∏ –í–°–ï –∑–Ω–∞—á–µ–Ω–∏—è –ª–µ–∂–∞—Ç –≤ [0 .. 1.2],
    —Å—á–∏—Ç–∞–µ–º –∏—Ö –¥–æ–ª—è–º–∏ –∏ –≤—ã–≤–æ–¥–∏–º –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç—ã (0.7 ‚Üí 70 %).
    """
    if not raw_values:
        return ""

    cats = list(raw_values.get("categories") or [])
    series = list(raw_values.get("series") or [])

    if not cats or not series:
        return ""

    lines: list[str] = ["–¢–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ):", ""]
    n = len(cats)

    for s in series:
        name = (s.get("name") or "").strip()
        unit = (s.get("unit") or "").strip()
        vals = list(s.get("values") or [])

        # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ ¬´—ç—Ç–æ –¥–æ–ª–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö¬ª
        numeric_vals: list[float] = []
        for v in vals:
            try:
                numeric_vals.append(float(str(v).replace(",", ".")))
            except Exception:
                # —Ç–µ–∫—Å—Ç/–ø—É—Å—Ç–æ ‚Äî –∏–≥–Ω–æ—Ä–∏–º –¥–ª—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
                pass

        has_percent_unit = bool(unit) and "%" in unit
        is_share_like = bool(
            has_percent_unit
            and numeric_vals
            and all(0.0 <= x <= 1.2 for x in numeric_vals)
        )

        header = name or "–°–µ—Ä–∏—è"
        # –µ—Å–ª–∏ –µ–¥–∏–Ω–∏—Ü—ã –ù–ï –ø—Ä–æ—Ü–µ–Ω—Ç—ã ‚Äî –ø–æ–∫–∞–∂–µ–º –∏—Ö –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        if unit and "%" not in unit:
            header = f"{header} ({unit})"
        lines.append(f"{header}:")

        for i in range(min(n, len(vals))):
            label = str(cats[i]).strip() or str(i + 1)
            raw_v = vals[i]

            v_num: float | None = None
            sval = ""

            if isinstance(raw_v, (int, float, Decimal)):
                v_num = float(raw_v)
            else:
                try:
                    v_num = float(str(raw_v).replace(",", "."))
                except Exception:
                    sval = str(raw_v) if raw_v is not None else ""

            if v_num is not None:
                if is_share_like:
                    v_num *= 100.0  # 0.7 ‚Üí 70.0

                if abs(v_num - round(v_num)) < 0.05:
                    sval = str(int(round(v_num)))
                else:
                    sval = f"{v_num:.2f}".rstrip("0").rstrip(".")

            # —Å—É—Ñ—Ñ–∏–∫—Å –µ–¥–∏–Ω–∏—Ü
            unit_suffix = ""
            if has_percent_unit:
                # —Ö–æ—Ç–∏–º ¬´70%¬ª, –±–µ–∑ –ø—Ä–æ–±–µ–ª–∞
                if not sval.endswith("%"):
                    unit_suffix = "%"
            elif unit:
                unit_suffix = f" {unit}"

            line = f"‚Äî {label}: {sval}{unit_suffix}".strip()
            if line:
                lines.append(line)

        lines.append("")  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É —Å–µ—Ä–∏—è–º–∏

    return "\n".join(l for l in lines if l.strip())



def _format_chart_values(chart_data: list) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º chart_data –ë–ï–ó –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏ —Å—É–º–º –∏ –±–µ–∑ ¬´–ø–æ–¥–≥–æ–Ω–∫–∏¬ª –∫ 100%.

    –õ–æ–≥–∏–∫–∞:
      - –µ—Å–ª–∏ unit —Å–æ–¥–µ—Ä–∂–∏—Ç '%' –∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ [0..1.2],
        —Ç—Ä–∞–∫—Ç—É–µ–º –∏—Ö –∫–∞–∫ –¥–æ–ª–∏ (0.8 ‚Üí 80) –∏ –¥–æ–º–Ω–æ–∂–∞–µ–º –Ω–∞ 100;
      - –¥–∞–ª—å—à–µ –ø—Ä–æ—Å—Ç–æ –ø–µ—á–∞—Ç–∞–µ–º: ¬´‚Äî label: 80%¬ª.
    """
    rows = chart_data or []
    if not rows:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–≤–æ–¥–∞."

    # —Å–æ–±–µ—Ä—ë–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –µ–≤—Ä–∏—Å—Ç–∏–∫–∏ ¬´—ç—Ç–æ –¥–æ–ª–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö¬ª
    numeric_vals: list[float] = []
    has_percent_unit = False
    for r in rows:
        unit = r.get("unit")
        if isinstance(unit, str) and "%" in unit:
            has_percent_unit = True

        v = r.get("value")
        if v is None:
            v = r.get("y") or r.get("x") or r.get("v") or r.get("count")
        try:
            numeric_vals.append(float(str(v).replace(",", ".")))
        except Exception:
            # –µ—Å–ª–∏ —Ö–æ—Ç—å –æ–¥–∏–Ω –Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è –∫ —á–∏—Å–ª—É ‚Äî –ø—Ä–æ—Å—Ç–æ –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ–º –µ–≤—Ä–∏—Å—Ç–∏–∫—É
            pass

    is_share_like = bool(
        has_percent_unit
        and numeric_vals
        and all(0.0 <= x <= 1.2 for x in numeric_vals)
    )

    lines: list[str] = []
    for r in rows:
        label = (str(r.get("label") or r.get("name") or r.get("category") or "")).strip()

        raw_v = r.get("value")
        if raw_v is None:
            raw_v = r.get("y") or r.get("x") or r.get("v") or r.get("count")

        unit = r.get("unit")

        v_num: float | None = None
        sval = ""

        # –ø—Ä–æ–±—É–µ–º –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª—É
        if isinstance(raw_v, (int, float, Decimal)):
            v_num = float(raw_v)
        else:
            try:
                v_num = float(str(raw_v).replace(",", "."))
            except Exception:
                # —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞/—Ç–µ–∫—Å—Ç ‚Äî –æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å
                sval = str(raw_v) if raw_v is not None else ""

        # —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Üí —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
        if v_num is not None:
            if is_share_like:
                v_num *= 100.0  # 0.8 ‚Üí 80.0

            if abs(v_num - round(v_num)) < 0.05:
                sval = str(int(round(v_num)))
            else:
                sval = f"{v_num:.2f}".rstrip("0").rstrip(".")

        # –¥–æ–±–∞–≤–ª—è–µ–º –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
        unit_suffix = ""
        if isinstance(unit, str) and unit.strip():
            u = unit.strip()
            # –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∏ –≤ —Å—Ç—Ä–æ–∫–µ –µ—â—ë –Ω–µ—Ç '%', –¥–æ–±–∞–≤–∏–º –±–µ–∑ –ø—Ä–æ–±–µ–ª–∞
            if "%" in u and not sval.endswith("%"):
                unit_suffix = "%"
            else:
                unit_suffix = f" {u}"

        text = (f"‚Äî {label}: {sval}{unit_suffix}").strip()
        if text:
            lines.append(text)

    return "\n".join(lines) if lines else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–≤–æ–¥–∞."

# --- –Ω–µ–±–æ–ª—å—à–∞—è –∫–æ—Å–º–µ—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∏–∑ OOXML-–≥—Ä–∞—Ñ–∏–∫–æ–≤ ---

# –¥–≤–æ–µ—Ç–æ—á–∏–µ + –ø—Ä–æ–±–µ–ª—ã, —Å—Ä–∞–∑—É –ø–µ—Ä–µ–¥ ';' –∏–ª–∏ –∫–æ–Ω—Ü–æ–º —Å—Ç—Ä–æ–∫–∏
_EMPTY_PERCENT_RE = re.compile(r"(:\s*)(?=;|$)")

def _fill_empty_percents(text: str) -> str:
    """
    'label:' –∏–ª–∏ 'label: ;' ‚Üí 'label: 0%' –ø–µ—Ä–µ–¥ ';' –∏–ª–∏ –∫–æ–Ω—Ü–æ–º —Å—Ç—Ä–æ–∫–∏.
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è –∫—É—Å–æ—á–∫–æ–≤ –≤–∏–¥–∞ '‚Ä¶; 3:' –∏ '‚Ä¶; 3: ;'.
    """
    return _EMPTY_PERCENT_RE.sub(lambda m: m.group(1) + "0%", text)


def _list_figures_db(uid: int, doc_id: int, limit: int = 25) -> dict:
    """–°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ –ë–î (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏)."""
    con = get_conn()
    cur = con.cursor()
    has_type = _table_has_columns(con, "chunks", ["element_type", "attrs"])

    if has_type:
        cur.execute(
            "SELECT DISTINCT section_path, attrs, text FROM chunks "
            "WHERE owner_id=? AND doc_id=? AND element_type='figure' "
            "ORDER BY id ASC",
            (uid, doc_id),
        )
    else:
        # —Å—Ç–∞—Ä—ã–µ –∏–Ω–¥–µ–∫—Å—ã ‚Äî –∫–æ–ª–æ–Ω–∫–∏ attrs –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å, –Ω–µ –≤—ã–±–∏—Ä–∞–µ–º –µ—ë
        cur.execute(
            "SELECT DISTINCT section_path, text FROM chunks "
            "WHERE owner_id=? AND doc_id=? AND (text LIKE '[–†–∏—Å—É–Ω–æ–∫]%' OR lower(section_path) LIKE '%—Ä–∏—Å—É–Ω–æ–∫%') "
            "ORDER BY id ASC",
            (uid, doc_id),
        )
    rows = cur.fetchall() or []
    con.close()

    items: list[str] = []
    for r in rows:
        section_path = r["section_path"] or ""
        attrs_json = r["attrs"] if ("attrs" in r.keys()) else None  # –≤ else –µ—ë –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç ‚Äî –æ–∫
        txt = r["text"] or None
        disp = _compose_figure_display(attrs_json, section_path, txt)
        items.append(disp)

    seen = set()
    uniq = []
    for it in items:
        k = it.strip().lower()
        if k and k not in seen:
            seen.add(k)
            uniq.append(it)

    total = len(uniq)
    return {
        "count": total,
        "list": uniq[:limit],
        "more": max(0, total - limit),
    }



# -------- –†–∞–Ω–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤–∏–¥–∞ ¬´—Ä–∏—Å—É–Ω–æ–∫ 2.1¬ª, ¬´—Ä–∏—Å. 3¬ª, ¬´figure 1.2¬ª --------

FIG_NUM_RE = re.compile(
    r"(?i)\b(?:—Ä–∏—Å\w*|—Å—Ö–µ–º\w*|–∫–∞—Ä—Ç–∏–Ω\w*|–¥–∏–∞–≥—Ä–∞–º\w*|–≥–∏—Å—Ç–æ–≥—Ä–∞–º\w*|diagram|chart|figure|fig\.?|picture|pic\.?)"
    r"\s*(?:‚Ññ\s*|no\.?\s*|–Ω–æ–º–µ—Ä\s*)?([A-Za-z–ê-–Ø–∞-—è]?\s*[\d.,\s]+(?:\s*(?:–∏|and)\s*[\d.,\s]+)*)"
)

# –Ω–æ–≤—ã–π —Ö–∏–Ω—Ç –¥–ª—è —Ä–µ–∂–∏–º–∞ ¬´–∏–∑–≤–ª–µ—á—å –∑–Ω–∞—á–µ–Ω–∏—è¬ª
_VALUES_HINT = re.compile(r"(?i)\b(–∑–Ω–∞—á–µ–Ω–∏[—è–µ]|—Ü–∏—Ñ—Ä[–∞—ã]|–ø—Ä–æ—Ü–µ–Ω—Ç[–∞-—è]*|values?|numbers?)\b")
_SPLIT_FIG_LIST_RE = re.compile(r"\s*(?:,|;|\band\b|–∏)\s*", re.IGNORECASE)

def _extract_fig_nums(text: str) -> list[str]:
    nums: list[str] = []
    for mm in FIG_NUM_RE.finditer(text or ""):
        seg = (mm.group(1) or "").strip()
        # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: –∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, "–∏/and"
        parts = _SPLIT_FIG_LIST_RE.split(seg)
        for p in parts:
            p = p.strip()
            if not p:
                continue
            nums.append(p)
    return nums

_ALL_FIGS_HINT = re.compile(r"(?i)\b(–≤—Å–µ\s+—Ä–∏—Å—É–Ω–∫\w*|–≤—Å–µ\s+—Å—Ö–µ–º\w*|–≤—Å–µ\s+–∫–∞—Ä—Ç–∏–Ω\w*|all\s+pictures?|all\s+figs?)\b")

def _num_norm_fig(s: str | None) -> str:
    s = (s or "").strip()
    s = s.replace("\u00A0", " ")   # NBSP -> –ø—Ä–æ–±–µ–ª
    s = s.replace(" ", "")
    s = s.replace(",", ".")        # 4,1 -> 4.1
    s = re.sub(r"[.:;)\]]+$", "", s)  # —Å—Ä–µ–∑ —Ö–≤–æ—Å—Ç–æ–≤–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏: "4." -> "4"
    return s


def _is_pure_figure_request(text: str) -> bool:
    """
    –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∑–∞–ø—Ä–æ—Å –¢–û–õ–¨–ö–û –ø—Ä–æ —Ä–∏—Å—É–Ω–∫–∏ (–æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä–æ–≤),
    –±–µ–∑ —Ç–∞–±–ª–∏—Ü, —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.

    –ò—Å–ø–æ–ª—å–∑—É–µ–º, —á—Ç–æ–±—ã:
    ‚Äî —É–π—Ç–∏ –≤ –µ–¥–∏–Ω—ã–π figure-–ø–∞–π–ø–ª–∞–π–Ω;
    ‚Äî –Ω–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–æ—Ç–æ–º –æ–±—â–∏–π RAG-–ø–∞–π–ø–ª–∞–π–Ω, –∫–æ—Ç–æ—Ä—ã–π –¥—É–±–ª–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã
      –∏ –º–æ–∂–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å ¬´–¥–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ¬ª.
    """
    t = (text or "").strip()
    if not t:
        return False

    # ¬´–≤—Å–µ —Ä–∏—Å—É–Ω–∫–∏¬ª ‚Äî –æ—Ç–¥–µ–ª—å–Ω–∞—è –≤–µ—Ç–∫–∞ (_ALL_FIGS_HINT)
    if _ALL_FIGS_HINT.search(t):
        return False

    # –Ω–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤ ‚Äî –Ω–µ –Ω–∞—à —Å–ª—É—á–∞–π
    if not FIG_NUM_RE.search(t):
        return False

    # –µ—Å–ª–∏ —è–≤–Ω–æ —É–ø–æ–º–∏–Ω–∞—é—Ç —Ç–∞–±–ª–∏—Ü—ã –∏–ª–∏ —Ä–∞–∑–¥–µ–ª—ã/–≥–ª–∞–≤—ã ‚Äî —ç—Ç–æ —É–∂–µ —Å–º–µ—à–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    if _TABLE_ANY.search(t) or _SECTION_NUM_RE.search(t):
        return False

    return True


def _is_pure_section_request(text: str, intents: dict | None = None) -> bool:
    """
    –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: ¬´—á–∏—Å—Ç—ã–π¬ª –∑–∞–ø—Ä–æ—Å –ø—Ä–æ –≥–ª–∞–≤—É/—Ä–∞–∑–¥–µ–ª/–ø—É–Ω–∫—Ç:
      ‚Äî –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞ –Ω–∞ –≥–ª–∞–≤—É/—Ä–∞–∑–¥–µ–ª/–ø—É–Ω–∫—Ç;
      ‚Äî –Ω–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã, —Ä–∏—Å—É–Ω–∫–∏ –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

    –í—Å—ë, —á—Ç–æ —Å–º–µ—à–∞–Ω–Ω–æ–µ (–≥–ª–∞–≤–∞ + —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏/–∏—Å—Ç–æ—á–Ω–∏–∫–∏), –¥–æ–ª–∂–Ω–æ –∏–¥—Ç–∏
    –≤ –æ–±—â–∏–π –º—É–ª—å—Ç–∏–∏ÃÜ–Ω—Ç–µ–Ω—Ç–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω, –∞ –Ω–µ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ–∫—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.
    """
    t = (text or "").strip()
    if not t:
        return False

    # –Ω–µ—Ç —É–∫–∞–∑–∞–Ω–∏—è –ø—É–Ω–∫—Ç–∞/–≥–ª–∞–≤—ã ‚Äî –Ω–µ –Ω–∞—à —Å–ª—É—á–∞–π
    if not _SECTION_NUM_RE.search(t):
        return False

    # –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ —è–≤–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏/–∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äî —ç—Ç–æ —É–∂–µ –º–∏–∫—Å
    if _TABLE_ANY.search(t) or FIG_NUM_RE.search(t) or _SOURCES_HINT.search(t):
        return False

    # –µ—Å–ª–∏ detect_intents —É–∂–µ —É–≤–∏–¥–µ–ª —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏/–∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äî —Ç–æ–∂–µ –Ω–µ —á–∏—Å—Ç—ã–π —Ä–∞–∑–¥–µ–ª
    if intents:
        if (
            intents.get("tables", {}).get("want")
            or intents.get("figures", {}).get("want")
            or intents.get("sources", {}).get("want")
        ):
            return False

    return True


def _build_figure_records(uid: int, doc_id: int, nums: list[str]) -> list[dict]:
    """
    –ï–¥–∏–Ω–∞—è "—Å–±–æ—Ä–∫–∞" –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∏—Å—É–Ω–∫–∞—Ö:
    ‚Äî –Ω–æ–º–µ—Ä –∏ –∫—Ä–∞—Å–∏–≤—ã–π display;
    ‚Äî –ø—É—Ç–∏ –∫ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º;
    ‚Äî —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∏–∑ chart_data);
    ‚Äî –ø–æ–¥–ø–∏—Å—å –∏ —Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º;
    ‚Äî vision-–æ–ø–∏—Å–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å –∏ –Ω–µ ¬´–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ¬ª).
    """
    if not nums:
        return []

    # –∑–∞—Ä–∞–Ω–µ–µ —Ç—è–Ω–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –∏–∑ retrieval, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å images/—Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º/vision
    try:
        cards = describe_figures_by_numbers(
            uid,
            doc_id,
            nums,
            sample_chunks=2,
            use_vision=True,
            lang="ru",
            vision_first_image_only=True,
        ) or []
    except Exception:
        cards = []

    cards_by_norm: dict[str, dict] = {}
    for c in cards:
        key = _num_norm_fig(str(c.get("num") or ""))
        if key and key not in cards_by_norm:
            cards_by_norm[key] = c

    idx_oox = _ooxml_get_index(doc_id)
    fig_idx = FIG_INDEX.get(doc_id)

    # –∫–ª—é—á: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –Ω–æ–º–µ—Ä —Ä–∏—Å—É–Ω–∫–∞ ‚Üí record
    records_by_num: dict[str, dict] = {}

    for orig in nums:
        norm = _num_norm_fig(orig)
        if not norm:
            continue

        # –µ—Å–ª–∏ —ç—Ç–æ—Ç –Ω–æ–º–µ—Ä —É–∂–µ —Å–æ–±—Ä–∞–Ω ‚Äî –Ω–µ —Å–æ–∑–¥–∞—ë–º –¥—É–±–ª—å
        if norm in records_by_num:
            continue

        card = cards_by_norm.get(norm)
        rec: dict = {
            "owner_id": uid,      # –Ω—É–∂–µ–Ω –¥–ª—è –≤—ã–∑–æ–≤–∞ summarizer.extract_figure_value
            "doc_id": doc_id,     # –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
            "num": norm,
            "orig": orig,
            "display": None,
            "images": [],
            # –ï–î–ò–ù–´–ô –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã –ø–æ —á–∏—Å–ª–∞–º
            "raw_values": None,        # —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ OOXML
            "values_text": None,       # —É–∂–µ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–ª–æ–∫ ¬´–¢–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª
            "values_source": None,     # "ooxml" | "summary" | "vision" | ...
            # –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º
            "values": None,
            "near_text": [],
            "caption": None,
            "vision_desc": None,
        }


        # --- 1) –¥–∞–Ω–Ω—ã–µ –∏–∑ RAG-–∫–∞—Ä—Ç–æ—á–µ–∫ ---
        if card:
            rec["display"] = card.get("display") or rec["display"]
            rec["images"] = [p for p in (card.get("images") or []) if p]
            rec["near_text"] = [
                (h or "").strip()
                for h in (card.get("highlights") or [])
                if (h or "").strip()
            ]
            vis = (card.get("vision") or {}).get("description") or ""
            vis_clean = vis.strip()
            low = vis_clean.lower()
            # –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –≤–∏–¥–∞ ¬´—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ)¬ª
            if vis_clean and "–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ" not in low and "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è" not in low:
                rec["vision_desc"] = vis_clean

        # --- 2) OOXML-–∏–Ω–¥–µ–∫—Å: –ø–æ–¥–ø–∏—Å—å –∏ image_path ---
        if idx_oox:
            oox_rec = _ooxml_find_figure_by_label(idx_oox, norm) or _ooxml_find_figure_by_label(idx_oox, orig)
            if oox_rec:
                cap = (oox_rec.get("caption") or "").strip()
                if cap:
                    rec["caption"] = cap
                if not rec["display"]:
                    label = oox_rec.get("n") or norm
                    rec["display"] = f"–†–∏—Å—É–Ω–æ–∫ {label}" + (f" ‚Äî {cap}" if cap else "")
                path = oox_rec.get("image_path")
                if path and path not in rec["images"]:
                    rec["images"].append(path)

                # --- 3) –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å figures.py: –ø—É—Ç—å –∫ –∫–∞—Ä—Ç–∏–Ω–∫–µ + –ø–æ–¥–ø–∏—Å—å ---
        if fig_idx:
            try:
                recs = fig_find(fig_idx, number=orig) or fig_find(fig_idx, number=norm) or []
            except Exception:
                recs = []
            for r in recs:
                if not rec["display"]:
                    rec["display"] = figure_display_name(r)
                ap = r.get("abs_path")
                if ap and ap not in rec["images"]:
                    rec["images"].append(ap)
                cap_text = r.get("caption") or r.get("title")
                if cap_text and not rec["caption"]:
                    rec["caption"] = cap_text
                if not rec["near_text"] and cap_text:
                    rec["near_text"].append(cap_text)

        # --- 4) chart_data –∏–∑ attrs (–¢–û–ß–ù–´–ï —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ OOXML) ---
        row = _fetch_figure_row_by_num(uid, doc_id, orig)
        if not row and norm != orig:
            row = _fetch_figure_row_by_num(uid, doc_id, norm)

        if row:
            attrs_json = row["attrs"] if ("attrs" in row.keys()) else None

            # 4.a) –ø—Ä–æ–±—É–µ–º –¥–æ—Å—Ç–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ raw_values –∏–∑ OOXML
            raw = _extract_raw_values_from_attrs(attrs_json)
            if raw:
                rec["raw_values"] = raw
                rec["values_text"] = _format_exact_values(raw)
                rec["values_source"] = "ooxml"
                rec["values"] = rec["values_text"]  # –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞

            # —Ñ–æ–ª–±—ç–∫: –µ—Å–ª–∏ raw_values –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å, –Ω–æ —Å—Ç–∞—Ä—ã–π
            # _parse_chart_data –≤–µ—Ä–Ω—É–ª –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ ‚Äî –∞–∫–∫—É—Ä–∞—Ç–Ω–æ
            # –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –µ–≥–æ –≤ –æ–¥–Ω—É —Å–µ—Ä–∏—é –±–µ–∑ –¥–æ–ø. –º–∞–≥–∏–∏
            if not rec.get("raw_values"):
                cd, _ctype, _attrs = _parse_chart_data(attrs_json)
                if cd:
                    categories = [
                        str(r.get("label") or r.get("name") or r.get("category") or "")
                        for r in cd
                    ]
                    values = []
                    for r in cd:
                        v = r.get("value")
                        if v is None:
                            v = r.get("y") or r.get("x") or r.get("v") or r.get("count")
                        values.append(v)
                    rec["raw_values"] = {
                        "categories": categories,
                        "series": [{
                            "name": None,
                            "unit": (cd[0].get("unit") if cd and isinstance(cd[0].get("unit"), str) else None),
                            "values": values,
                        }],
                    }
                    rec["values_text"] = _format_exact_values(rec["raw_values"])
                    rec["values_source"] = "ooxml"
                    rec["values"] = rec["values_text"]

            # 4.b) display ‚Äî –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ
            if not rec["display"]:
                title_text = row["text"] if ("text" in row.keys()) else None
                rec["display"] = _compose_figure_display(
                    attrs_json,
                    row["section_path"],
                    title_text,
                )

            # 4.c) –¢–ï–ö–°–¢ –ü–û–°–õ–ï –†–ò–°–£–ù–ö–ê: 2‚Äì3 –∞–±–∑–∞—Ü–∞ –ø—Ä—è–º–æ –∏–∑ –¥–∏–ø–ª–æ–º–∞
            try:
                follow = _figure_following_paragraphs(uid, doc_id, row, max_paragraphs=3, max_chars=1500)
                if follow:
                    rec["near_text"] = follow
            except Exception:
                # –Ω–µ –ª–æ–º–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä–∏—Å—É–Ω–∫–∞, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
                pass


        if not rec["display"]:
            rec["display"] = f"–†–∏—Å—É–Ω–æ–∫ {norm}"

        records_by_num[norm] = rec

    return list(records_by_num.values())


def _fig_values_text_from_records(
    records: list[dict],
    *,
    need_values: bool,
) -> str:
    """
    –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫ —Å —Ç–æ—á–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —Ä–∏—Å—É–Ω–∫–∞–º.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
      1) rec.raw_values / rec.values_text (OOXML);
      2) oox_fig_lookup (–≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç).
    """
    lines: list[str] = []

    for rec in records:
        # 1) –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Äî —Å—ã—Ä—ã–µ OOXML-–¥–∞–Ω–Ω—ã–µ
        raw = rec.get("raw_values")
        values_text = (rec.get("values_text") or rec.get("values") or "").strip()

        if raw and not values_text:
            values_text = _format_exact_values(raw)
            rec["values_text"] = values_text
            rec["values"] = values_text
            rec["values_source"] = rec.get("values_source") or "ooxml"

        # 2) –§–û–õ–ë–≠–ö: OOXML-–∏–Ω–¥–µ–∫—Å figure_lookup (–≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç)
        if not values_text:
            try:
                doc_id = rec.get("doc_id")
                num = rec.get("orig") or rec.get("num")
                idx = _ooxml_get_index(doc_id) if doc_id else None
                body = ""

                if idx and "oox_fig_lookup" in globals() and num:
                    oox_res = oox_fig_lookup(idx, str(num))
                    if isinstance(oox_res, str):
                        body = oox_res.strip()
                    elif isinstance(oox_res, dict):
                        body = (
                            (oox_res.get("values_text")
                             or oox_res.get("text")
                             or "")
                        ).strip()

                if body:
                    values_text = body
                    rec["values_text"] = body
                    rec["values"] = body
                    rec["values_source"] = rec.get("values_source") or "ooxml_text"
            except Exception:
                pass

        # –µ—Å–ª–∏ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫ —á–∏—Å–µ–ª –Ω–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —Ä–∏—Å—É–Ω–æ–∫
        if not values_text:
            continue

        disp = rec.get("display") or f"–†–∏—Å—É–Ω–æ–∫ {rec.get('num') or ''}".strip()
        # –¥–ª—è OOXML-–¥–∞–Ω–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π
        if rec.get("values_source") == "ooxml":
            title = f"**{disp} ‚Äî —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ)**"
        elif rec.get("values_source") in {"summary", "vision"}:
            title = f"**{disp} ‚Äî –∑–Ω–∞—á–µ–Ω–∏—è, —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (–≤–æ–∑–º–æ–∂–Ω—ã –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏)**"
        else:
            title = f"**{disp} ‚Äî –∑–Ω–∞—á–µ–Ω–∏—è**"

        lines.append(f"{title}\n\n{values_text}")

    if lines:
        return "\n\n".join(lines)

    if need_values:
        return (
            "–ü–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º —Ä–∏—Å—É–Ω–∫–∞–º –Ω–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ—á—å —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ "
            "(–Ω–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö OOXML-–¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º). "
            "–ú–æ–≥—É –¥–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ."
        )

    return ""


async def _send_fig_values_from_records(
    m: types.Message,
    records: list[dict],
    *,
    need_values: bool,
) -> None:
    """
    –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞.
    –í –æ—Å–Ω–æ–≤–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º _fig_values_text_from_records
    –∏ —Å–∫–ª–µ–∏–≤–∞–µ–º —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º.
    """
    text = _fig_values_text_from_records(records, need_values=need_values)
    if text:
        await _send(m, text)


async def _explain_figures_with_gpt(
    m: types.Message,
    records: list[dict],
    question: str,
    *,
    verbosity: str,
    need_values: bool,
    values_prefix: str = "",
) -> None:
    """
    –§–∏–Ω–∞–ª—å–Ω—ã–π —à–∞–≥: GPT –¥–∞—ë—Ç —Å–≤—è–∑–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Ä–∏—Å—É–Ω–∫–∞–º —Å—Ä–∞–∑—É,
    –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–¥–ø–∏—Å–∏, —Ç–µ–∫—Å—Ç —Ä—è–¥–æ–º –∏ —É–∂–µ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.

    –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω values_prefix, —Ç–æ –æ–Ω –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞:
    —Å–Ω–∞—á–∞–ª–∞ –±–ª–æ–∫ ¬´—Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª, –∑–∞—Ç–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è.
    """
    if not (chat_with_gpt or chat_with_gpt_stream):
        return

    if not records:
        return

    ctx_blocks: list[str] = []
    for rec in records:
        disp = rec.get("display") or f"–†–∏—Å—É–Ω–æ–∫ {rec.get('num') or ''}".strip()
        parts: list[str] = [disp]
        if rec.get("caption"):
            parts.append(f"–ü–æ–¥–ø–∏—Å—å: {rec['caption']}")
        if rec.get("near_text"):
            parts.append("–¢–µ–∫—Å—Ç —Ä—è–¥–æ–º: " + " ".join(rec["near_text"][:2]))
        if rec.get("vision_desc"):
            parts.append("–û–ø–∏—Å–∞–Ω–∏–µ –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–µ: " + rec["vision_desc"])

        values_text = (rec.get("values_text") or rec.get("values") or "").strip()
        if values_text:
            parts.append("–¢–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ):\n" + values_text)

        # –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –ø–æ–¥–∞—Ç—å –∏ —Å—ã—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —á—Ç–æ–±—ã LLM –ø–æ–Ω–∏–º–∞–ª–∞ —Å–µ—Ä–∏–∏/–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if rec.get("raw_values"):
            try:
                parts.append("–°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã (JSON):\n" + json.dumps(rec["raw_values"], ensure_ascii=False))
            except Exception:
                pass

        ctx_blocks.append("\n".join(parts))


    ctx = "\n\n---\n\n".join(ctx_blocks)
    if not ctx.strip():
        return

    focus = (
        "—Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –∏—Ö –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é"
        if need_values
        else "–ø–æ–¥—Ä–æ–±–Ω–æ –ø–æ—è—Å–Ω—è—è —Å–º—ã—Å–ª –∏ –≤—ã–≤–æ–¥—ã –ø–æ —Ä–∏—Å—É–Ω–∫–∞–º"
    )

    system_prompt = (
        "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –í –≠–¢–û–ú –≤—ã–∑–æ–≤–µ —Ç—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å —Ç–æ–ª—å–∫–æ —Ä–∏—Å—É–Ω–∫–∏ "
        "(–¥–∏–∞–≥—Ä–∞–º–º—ã, –≥—Ä–∞—Ñ–∏–∫–∏) –∏–∑ –¥–∏–ø–ª–æ–º–∞. "
        "–¢–µ–±–µ —É–∂–µ –¥–∞–Ω—ã –ø–æ–¥–ø–∏—Å–∏, –±–ª–∏–∂–∞–π—à–∏–π —Ç–µ–∫—Å—Ç –∏ —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞. "
        "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –µ—Å—Ç—å: –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ —á–∏—Å–ª–∞, –Ω–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–π –ø—Ä–æ—Ü–µ–Ω—Ç—ã "
        "–∏ –Ω–µ –ø—ã—Ç–∞–π—Å—è –Ω–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å—É–º–º—ã –¥–æ 100%. –ù–µ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü.\n"
        "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∏ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç—ã, –≤—ã—Ä—É—á–∫–∞, "
        "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø.), –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —É–ø–æ–º—è–Ω—É—Ç—ã –≤ –ø–æ–¥–ø–∏—Å—è—Ö –∏–ª–∏ —Ç–µ–∫—Å—Ç–µ —Ä—è–¥–æ–º —Å —Ä–∏—Å—É–Ω–∫–æ–º."
    )


    user_prompt = (
        f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}\n\n"
        "–ß–∏—Å–ª–∞ –ø–æ –∫–∞–∂–¥–æ–º—É —Ä–∏—Å—É–Ω–∫—É —É–∂–µ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω—ã –≤—ã—à–µ –≤ –±–ª–æ–∫–∞—Ö ¬´–¢–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ)¬ª "
        "–∏/–∏–ª–∏ –≤ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä–µ. –ù–ï –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–π —ç—Ç–∏ —á–∏—Å–ª–∞ –∏ –ù–ï –∏–∑–º–µ–Ω—è–π –ø—Ä–æ—Ü–µ–Ω—Ç—ã. "
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Ç–æ–ª—å–∫–æ —Å–º—ã—Å–ª–æ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: —á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–∏—Å—É–Ω–∫–∏, –∫–∞–∫–∏–µ —É—Ä–æ–≤–Ω–∏ –≤—ã—à–µ/–Ω–∏–∂–µ, "
        "–∫–∞–∫–∏–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –≤–∏–¥–Ω—ã, –∫–∞–∫–∏–µ –≤—ã–≤–æ–¥—ã –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å.\n"
        "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π, –∫ –∫–∞–∫–æ–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç—ã, —Ä—ã–Ω–æ–∫, "
        "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø.), –µ—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ –≤ –ø–æ–¥–ø–∏—Å—è—Ö –∏–ª–∏ —Ç–µ–∫—Å—Ç–µ —Ä—è–¥–æ–º. –ï—Å–ª–∏ –ø–æ —Ä–∏—Å—É–Ω–∫–∞–º "
        "–Ω–µ–ø–æ–Ω—è—Ç–Ω–æ, –∫ —á–µ–º—É –æ—Ç–Ω–æ—Å—è—Ç—Å—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏, –ø—Ä—è–º–æ –Ω–∞–ø–∏—à–∏, —á—Ç–æ –ø—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ —É–∫–∞–∑–∞–Ω–∞.\n\n"
        f"–°–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ä–∏—Å—É–Ω–∫–∞—Ö, –æ–ø–∏—à–∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏ —Å–¥–µ–ª–∞–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é {focus}.\n"
        f"{_verbosity_addendum(verbosity, '–æ–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤')}"
    )


    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": f"[–°–æ–±—Ä–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∏—Å—É–Ω–∫–∞—Ö]\n{ctx}"},
        {"role": "user", "content": user_prompt},
    ]

    try:
        ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
    except Exception as e:
        logging.exception("figure explanation failed: %s", e)
        ans = ""

    ans = (ans or "").strip()
    prefix = (values_prefix or "").strip()

    if prefix and ans:
        final = prefix + "\n\n" + ans
    elif prefix:
        final = prefix
    else:
        final = ans

    if final:
        await _send(m, _strip_unwanted_sections(final))

async def _answer_figure_query(
    m: types.Message, uid: int, doc_id: int, text: str, *, verbosity: str = "normal"
) -> bool:
    """
    –ù–æ–≤—ã–π –µ–¥–∏–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π:
    1) –≤—Å–µ–≥–¥–∞ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º –ø–æ —Ä–∏—Å—É–Ω–∫–∞–º (_build_figure_records);
    2) —Å–æ–±–∏—Ä–∞–µ–º –æ–±—â–∏–π –±–ª–æ–∫ —Å —Ç–æ—á–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å);
    3) –¥–∞—ë–º –æ–¥–Ω–æ —Å–≤—è–∑–Ω–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ GPT, –≤ –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–¥–º–µ—à–∞–Ω –±–ª–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π.

    –ü–æ–≤–µ–¥–µ–Ω–∏–µ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ–≥–æ, —Å–ø—Ä–æ—Å–∏–ª–∏ –ª–∏ ¬´–æ–ø–∏—à–∏ —Ä–∏—Å—É–Ω–æ–∫ 2.3¬ª
    –∏–ª–∏ ¬´–¥–∞–π —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —Ä–∏—Å—É–Ω–∫—É 2.3¬ª ‚Äî –º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∞–∫—Ü–µ–Ω—Ç
    –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–∏.
    """
    # 0) –Ω—É–∂–µ–Ω –ª–∏ –æ—Å–æ–±—ã–π –∞–∫—Ü–µ–Ω—Ç –Ω–∞ —á–∏—Å–ª–∞—Ö
    need_values = bool(_VALUES_HINT.search(text or ""))

    # 1) –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –Ω–æ–º–µ—Ä–∞ —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
    raw_list = _extract_fig_nums(text or "")
    seen: set[str] = set()
    nums: list[str] = []
    for token in raw_list:
        n = _num_norm_fig(token)
        if n and n not in seen:
            seen.add(n)
            nums.append(token)   # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –≤–∏–¥ –Ω–æ–º–µ—Ä–∞

    if not nums:
        return False

    # 2) —Å–æ–±–∏—Ä–∞–µ–º –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ –≤—Å–µ–º —Ä–∏—Å—É–Ω–∫–∞–º
    records = _build_figure_records(uid, doc_id, nums)
    if not records:
        # üîß –†–∞–Ω—å—à–µ –∑–¥–µ—Å—å —Å—Ä–∞–∑—É —à—ë–ª –æ—Ç–≤–µ—Ç ¬´–£–∫–∞–∑–∞–Ω–Ω—ã–µ —Ä–∏—Å—É–Ω–∫–∏ –≤ —Ä–∞–±–æ—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.¬ª,
        # –∏–∑-–∑–∞ —á–µ–≥–æ –º—ã –Ω–µ –¥–æ—Ö–æ–¥–∏–ª–∏ –¥–æ –æ–±—â–µ–≥–æ RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ –Ω–µ –º–æ–≥–ª–∏,
        # –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç–≤–µ—Ç–∏—Ç—å –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º –∏–ª–∏ –æ–±—â–µ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.
        # –¢–µ–ø–µ—Ä—å –ø—Ä–æ—Å—Ç–æ –≥–æ–≤–æ—Ä–∏–º –≤—ã–∑—ã–≤–∞—é—â–µ–º—É –∫–æ–¥—É ¬´—è –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å¬ª.
        return False

    # 4) —Å–æ–±–∏—Ä–∞–µ–º –æ–±—â–∏–π –±–ª–æ–∫ —Å —Ç–æ—á–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–±–µ–∑ –æ—Ç–ø—Ä–∞–≤–∫–∏)
    values_block = _fig_values_text_from_records(records, need_values=need_values)

    # 5) –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç: ¬´–∑–Ω–∞—á–µ–Ω–∏—è + –æ–ø–∏—Å–∞–Ω–∏–µ¬ª
    await _explain_figures_with_gpt(
        m,
        records,
        text,
        verbosity=verbosity,
        need_values=need_values,
        values_prefix=values_block,
    )

    # 6) –æ–±–Ω–æ–≤–ª—è–µ–º ¬´–ø–æ—Å–ª–µ–¥–Ω–∏–π —É–ø–æ–º—è–Ω—É—Ç—ã–π —Ä–∏—Å—É–Ω–æ–∫¬ª –¥–ª—è –∞–Ω–∞—Ñ–æ—Ä–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    try:
        LAST_REF.setdefault(uid, {})["figure_nums"] = [r["num"] for r in records]
    except Exception:
        pass

    return True

def _ooxml_table_block(uid: int, doc_id: int, num: str) -> str | None:
    """
    1) –ü—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ OOXML-–∏–Ω–¥–µ–∫—Å–∞ —á–µ—Ä–µ–∑ oox_tbl_lookup.
    2) –ï—Å–ª–∏ —Ç–∞–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø–∞–¥–∞–µ–º –≤ –æ–±—ã—á–Ω—ã–µ chunks (table/table_row) –∏
       —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–∞–±–ª–∏—Ü—ã –ø–æ —Å—Ç—Ä–æ–∫–∞–º. –≠—Ç–æ –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç –≥–ª—é–∫–æ–≤ OOXML-–ø–∞—Ä—Å–µ—Ä–∞.
    """

    # –º–∞–ª–µ–Ω—å–∫–∏–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Ö–µ–ª–ø–µ—Ä: –∫—Ä–∞—Å–∏–≤–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º rows ‚Üí —Ç–µ–∫—Å—Ç–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
    def _format_oox_rows(res: dict) -> str:
        rows = res.get("rows") or []
        lines: list[str] = []
        for row in rows:
            if not isinstance(row, (list, tuple)):
                continue
            cells = [(str(c) if c is not None else "").strip() for c in row]
            # –ø—É—Å—Ç—ã–µ —Ö–≤–æ—Å—Ç–æ–≤—ã–µ —è—á–µ–π–∫–∏ —É–±–∏—Ä–∞–µ–º
            while cells and cells[-1] == "":
                cells.pop()
            lines.append(" | ".join(cells))
        return "\n".join(lines).strip()

    # --- 1. OOXML ---
    idx = _ooxml_get_index(doc_id)
    if idx and "oox_tbl_lookup" in globals():
        try:
            res = oox_tbl_lookup(idx, str(num))
        except Exception:
            res = None

        if res is not None:
            if isinstance(res, str):
                body = res.strip()
            elif isinstance(res, dict) and "rows" in res:
                # –ù–û–†–ú–ê–õ–¨–ù–û–ï —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
                body = _format_oox_rows(res)
            else:
                # –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è ‚Äî –ø—Ä–æ—Å—Ç–æ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º
                try:
                    body = json.dumps(res, ensure_ascii=False, indent=2)
                except Exception:
                    body = str(res)

            body = (body or "").strip()
            if body:
                # –º–µ–Ω—è–µ–º –ø–æ–¥–ø–∏—Å—å, —Ç.–∫. —Ç–µ–ø–µ—Ä—å —ç—Ç–æ –Ω–µ ¬´—Å—ã—Ä–æ–π JSON¬ª, –∞ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
                return f"–¢–∞–±–ª–∏—Ü–∞ {num} (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ):\n{body}"

    # --- 2. –§–æ–ª–±—ç–∫: chunks –∏–∑ –ë–î ---
    con = get_conn()
    cur = con.cursor()

    has_et   = _table_has_columns(con, "chunks", ["element_type"])
    has_attr = _table_has_columns(con, "chunks", ["attrs"])

    rows = []

    try:
        if has_attr and has_et:
            like1 = f'%\"caption_num\": \"{num}\"%'
            like2 = f'%\"label\": \"{num}\"%'
            cur.execute(
                """
                SELECT section_path, text
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                  AND element_type IN ('table','table_row')
                  AND (attrs LIKE ? OR attrs LIKE ?)
                ORDER BY id ASC
                """,
                (uid, doc_id, like1, like2),
            )
            rows = cur.fetchall() or []

        if not rows:
            # —Ñ–æ–ª–±—ç–∫ –ø–æ section_path / —Ç–µ–∫—Å—Ç—É, —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–∞–∂–µ –Ω–∞ —Å—Ç–∞—Ä—ã—Ö –∏–Ω–¥–µ–∫—Å–∞—Ö
            variants = _table_num_variants(num)
            sec_patterns = []
            txt_patterns = []

            for v in variants:
                # "–¢–∞–±–ª–∏—Ü–∞ 6", "—Ç–∞–±–ª–∏—Ü–∞ 6", "—Ç–∞–±–ª. 6"
                sec_patterns.append(f"%–¢–∞–±–ª–∏—Ü–∞ {v}%")
                sec_patterns.append(f"%—Ç–∞–±–ª–∏—Ü–∞ {v}%")
                sec_patterns.append(f"%—Ç–∞–±–ª. {v}%")
                # —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Ç–∏–ø–∞ "[–¢–∞–±–ª–∏—Ü–∞] 6" –∏ "[—Ç–∞–±–ª.] 6"
                txt_patterns.append(f"[–¢–∞–±–ª–∏—Ü–∞]%{v}%")
                txt_patterns.append(f"[—Ç–∞–±–ª.]%{v}%")

            if not sec_patterns and not txt_patterns:
                rows = []
            else:
                conds_sec = " OR ".join(["section_path LIKE ? COLLATE NOCASE"] * len(sec_patterns)) if sec_patterns else "0"
                conds_txt = " OR ".join(["text LIKE ? COLLATE NOCASE"] * len(txt_patterns)) if txt_patterns else "0"

                sql = f"""
                    SELECT section_path, text
                    FROM chunks
                    WHERE owner_id=? AND doc_id=?
                      AND ( ({conds_sec}) OR ({conds_txt}) )
                    ORDER BY page ASC, id ASC
                """

                params = [uid, doc_id] + sec_patterns + txt_patterns
                cur.execute(sql, params)
                rows = cur.fetchall() or []
    finally:
        con.close()

    if not rows:
        return None

    sec = (rows[0]["section_path"] or "").strip()
    lines = []
    if sec:
        lines.append(f"[{sec}]")
    for r in rows:
        t = (r["text"] or "").strip()
        if t:
            lines.append(t)

    body = "\n".join(lines).strip()
    if not body:
        return None

    return f"–¢–∞–±–ª–∏—Ü–∞ {num} (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –ø–æ —Å—Ç—Ä–æ–∫–∞–º —Ç–∞–±–ª–∏—Ü—ã):\n{body}"


def _table_related_context(
    uid: int,
    doc_id: int,
    num: str,
    *,
    max_chars: int = 4000,
) -> str:
    """
    –ò—â–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —Å–≤—è–∑–∞–Ω —Å —Ç–∞–±–ª–∏—Ü–µ–π `num`.

    1) –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø—Ä—è–º—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è ¬´—Ç–∞–±–ª–∏—Ü–∞ N¬ª (–∫—Ä–æ–º–µ —Å–∞–º–∏—Ö —è—á–µ–µ–∫ —Ç–∞–±–ª–∏—Ü—ã).
    2) –ï—Å–ª–∏ —Ç–∞–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç ‚Äì –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∞–±–∑–∞—Ü(—ã) –≤–∏–¥–∞ ¬´–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ ‚Ä¶¬ª
       —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü—ã.
    3) –ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ ‚Äì –¥–µ–ª–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É.
    """
    con = get_conn()
    cur = con.cursor()
    has_et = _table_has_columns(con, "chunks", ["element_type"])

    variants = _table_num_variants(num)
    txt_patterns: list[str] = []
    for v in variants:
        txt_patterns.append(f"%–¢–∞–±–ª–∏—Ü–∞ {v}%")
        txt_patterns.append(f"%—Ç–∞–±–ª–∏—Ü–∞ {v}%")
        txt_patterns.append(f"%—Ç–∞–±–ª. {v}%")

    if not txt_patterns:
        cur.close()
        con.close()
        return ""

    conds_txt = " OR ".join(["text LIKE ? COLLATE NOCASE"] * len(txt_patterns))

    if has_et:
        cur.execute(
            f"""
            SELECT page, section_path, text, element_type
            FROM chunks
            WHERE owner_id=? AND doc_id=?
              AND ({conds_txt})
            ORDER BY page ASC, id ASC
            """,
            (uid, doc_id, *txt_patterns),
        )
    else:
        cur.execute(
            f"""
            SELECT page, section_path, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
              AND ({conds_txt})
            ORDER BY page ASC, id ASC
            """,
            (uid, doc_id, *txt_patterns),
        )

    rows = cur.fetchall() or []
    con.close()

    parts: list[str] = []
    total = 0

    for r in rows:
        et = ""
        if "element_type" in r.keys():
            et = (r["element_type"] or "").lower()
        if et in ("table", "table_row"):
            continue

        t = (r["text"] or "").strip()
        if not t:
            continue

        if total + len(t) > max_chars:
            parts.append(t[: max_chars - total])
            break

        parts.append(t)
        total += len(t)

    extra = "\n\n".join(parts).strip()
    if extra:
        return extra

    # –ù–û–í–û–ï: –µ—Å–ª–∏ –ø—Ä—è–º—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π ¬´—Ç–∞–±–ª–∏—Ü–∞ N¬ª –Ω–µ—Ç,
    # –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–¥–æ–±—Ä–∞—Ç—å ¬´–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ ‚Ä¶¬ª —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü—ã.
    try:
        con = get_conn()
        cur = con.cursor()
        has_et = _table_has_columns(con, "chunks", ["element_type"])
        has_attr = _table_has_columns(con, "chunks", ["attrs"])

        base_row = None
        if has_attr and has_et:
            like1 = f'%\"caption_num\": \"{num}\"%'
            like2 = f'%\"label\": \"{num}\"%'
            cur.execute(
                """
                SELECT id, page, section_path, element_type, text
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                  AND element_type IN ('table','table_row')
                  AND (attrs LIKE ? OR attrs LIKE ?)
                ORDER BY id ASC LIMIT 1
                """,
                (uid, doc_id, like1, like2),
            )
            base_row = cur.fetchone()

        if not base_row:
            cur.execute(
                """
                SELECT id, page, section_path, element_type, text
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                  AND element_type IN ('table','table_row')
                  AND section_path LIKE ? COLLATE NOCASE
                ORDER BY id ASC LIMIT 1
                """,
                (uid, doc_id, f'%–¢–∞–±–ª–∏—Ü–∞ {num}%'),
            )
            base_row = cur.fetchone()

        note_text = ""
        if base_row:
            base_id = base_row["id"]
            page = base_row["page"]

            cur.execute(
                """
                SELECT text, element_type
                FROM chunks
                WHERE owner_id=? AND doc_id=? AND id>? AND page=?
                ORDER BY id ASC LIMIT 10
                """,
                (uid, doc_id, base_id, page),
            )
            note_parts: list[str] = []
            started = False
            for r in cur.fetchall() or []:
                et = (r["element_type"] or "").lower() if "element_type" in r.keys() else ""
                if et in ("heading", "table", "figure", "table_row"):
                    # –¥–∞–ª—å—à–µ —É–∂–µ –¥—Ä—É–≥–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
                    break
                t = (r["text"] or "").strip()
                if not t:
                    continue
                low = t.lower()
                if low.startswith("–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ"):
                    started = True
                    note_parts.append(t)
                    continue
                if started:
                    # —Ü–µ–ø–ª—è–µ–º —Ö–≤–æ—Å—Ç –ø—Ä–∏–º–µ—á–∞–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω–æ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–±–∑–∞—Ü–µ–≤
                    note_parts.append(t)

            if note_parts:
                note_text = "\n".join(note_parts).strip()

        con.close()

        if note_text:
            return note_text
    except Exception:
        # –Ω–µ –ª–æ–º–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
        pass

    # –≤—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É
    query = f"–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ, –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã–≤–æ–¥—ã –ø–æ –¥–∞–Ω–Ω—ã–º —Ç–∞–±–ª–∏—Ü—ã {num}"
    try:
        ctx = best_context(
            uid,
            doc_id,
            query,
            max_chars=max_chars,
        ) or ""
    except Exception:
        ctx = ""

    return (ctx or "").strip()




async def _answer_table_query(
    m: types.Message,
    uid: int,
    doc_id: int,
    text: str,
    *,
    verbosity: str = "normal",
    mode: str = "normal",
) -> bool:
    """
    –°–ø–µ—Ü-–ø—É—Ç—å –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–∏–¥–∞:
      - "–æ–ø–∏—à–∏ —Ç–∞–±–ª–∏—Ü—É 4"
      - "—á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü–∞ 2.3"
      - "—Å–¥–µ–ª–∞–π –≤—ã–≤–æ–¥—ã –ø–æ —Ç–∞–±–ª–∏—Ü–µ 4"
      - –∏ —Ñ–æ–ª–ª–æ—É-–∞–ø–∞ "–æ–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ" –ø–æ —ç—Ç–æ–π –∂–µ —Ç–∞–±–ª–∏—Ü–µ (mode=\"more\").
    """
    nums = _extract_table_nums(text)
    if not nums:
        return False

    # –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é(–∏–µ) —Ç–∞–±–ª–∏—Ü—É(—ã) –¥–ª—è —Ñ—Ä–∞–∑ —Ç–∏–ø–∞ ¬´–æ–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ¬ª
    try:
        LAST_REF.setdefault(uid, {})["table_nums"] = [
            n.replace(" ", "").replace(",", ".") for n in nums
        ]
    except Exception:
        pass

    blocks: list[str] = []
    missing: list[str] = []

    for n in nums:
        # 1) –æ–±—ã—á–Ω—ã–π –ø—É—Ç—å: —Ç–∞–±–ª–∏—Ü–∞ –∏–∑ OOXML
        blk = _ooxml_table_block(uid, doc_id, n)
        if blk:
            blocks.append(blk)
            continue

        # 1a) –ù–û–í–û–ï: –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ "—Ç–∞–±–ª–∏—Ü—É-—Ä–∏—Å—É–Ω–æ–∫" –∫–∞–∫ –¥–∏–∞–≥—Ä–∞–º–º—É/—Ä–∏—Å—É–Ω–æ–∫
        fig_records = _build_figure_records(uid, doc_id, [n])
        logging.info(
            "TAB[query] table %s as figure: %d records",
            n,
            len(fig_records) if fig_records else 0,
        )
        if fig_records:
            values_text = _fig_values_text_from_records(fig_records, need_values=True)
            if values_text:
                blocks.append(
                    f"–¢–∞–±–ª–∏—Ü–∞ {n} (–≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∞ –∫–∞–∫ –¥–∏–∞–≥—Ä–∞–º–º–∞/—Ä–∏—Å—É–Ω–æ–∫):\n"
                    f"{values_text}"
                )
                try:
                    LAST_REF.setdefault(uid, {})["figure_nums"] = [r["num"] for r in fig_records]
                except Exception:
                    pass
                continue

        # 2) fallback: –Ω–∞—Å—Ç–æ—è—â–∞—è OCR –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–µ (–µ—Å–ª–∏ –Ω–∏–∫–∞–∫–∏—Ö chart_data –Ω–µ—Ç)
        ocr_blk = _ocr_table_block_from_image(uid, doc_id, n)
        if ocr_blk:
            blocks.append(ocr_blk)
        else:
            missing.append(n)

    if not blocks:
        # –Ω–∏—á–µ–≥–æ –Ω–µ —Å–º–æ–≥–ª–∏ —Å–æ–±—Ä–∞—Ç—å –Ω–∏ –∏–∑ OOXML, –Ω–∏ –∏–∑ OCR/–¥–∏–∞–≥—Ä–∞–º–º
        bad = ", ".join(missing or nums)
        await _send(
            m,
            f"–¢–∞–±–ª–∏—Ü–∞ {bad} –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. "
            "–ü—Ä–æ–≤–µ—Ä—å, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ —É–∫–∞–∑–∞–Ω –Ω–æ–º–µ—Ä."
        )
        return True  # —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º, –¥–∞–ª—å—à–µ –ø–æ –ø–∞–π–ø–ª–∞–π–Ω—É –Ω–µ –∏–¥—ë–º

    # –µ—Å–ª–∏ —á–∞—Å—Ç—å —Ç–∞–±–ª–∏—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî —è–≤–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º –æ–± —ç—Ç–æ–º –≤ –æ—Ç–≤–µ—Ç–µ
    if missing:
        blocks.append(
            "‚ö†Ô∏è –ü–æ —Å–ª–µ–¥—É—é—â–∏–º —Ç–∞–±–ª–∏—Ü–∞–º –¥–∞–Ω–Ω—ã—Ö –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: "
            + ", ".join(missing)
        )

    ctx_tables = "\n\n---\n\n".join(blocks)

    # –ë–ª–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –í–°–ï–ì–î–ê –ø–æ–π–¥—ë—Ç –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é,
    # —á—Ç–æ–±—ã –æ–Ω –≤–∏–¥–µ–ª –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã —Ä–æ–≤–Ω–æ –≤ —Ç–æ–º –≤–∏–¥–µ, –∫–∞–∫ –º—ã –µ—ë —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏.
    raw_values_text = ""
    if ctx_tables:
        raw_values_text = (
            "**–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ)**\n\n"
            f"{ctx_tables}"
        )

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º (–¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –∏ "–ø–æ–¥—Ä–æ–±–Ω–µ–µ")
    # –í –æ–±—ã—á–Ω–æ–º –æ—Ç–≤–µ—Ç–µ –±–µ—Ä—ë–º –ø–æ–º–µ–Ω—å—à–µ —Å–∏–º–≤–æ–ª–æ–≤, –≤ "–ø–æ–¥—Ä–æ–±–Ω–µ–µ" ‚Äî –ø–æ–±–æ–ª—å—à–µ.
    extra_ctx_parts: list[str] = []

    for n in nums:
        extra = _table_related_context(
            uid,
            doc_id,
            n,
            max_chars=4000 if mode == "more" else 2000,
        )
        if extra:
            extra_ctx_parts.append(
                f"[–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ —Ç–∞–±–ª–∏—Ü–µ {n}]\n{extra}"
            )

    extra_ctx = "\n\n---\n\n".join(extra_ctx_parts).strip()

    # –ï—Å–ª–∏ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å ¬´–ø–æ–¥—Ä–æ–±–Ω–µ–µ¬ª, –Ω–æ –≤ —Å–∞–º–æ–π —Ä–∞–±–æ—Ç–µ –ù–ï–¢ –¥–æ–ø. —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ —ç—Ç—É —Ç–∞–±–ª–∏—Ü—É,
    # –º—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç [–º–æ–¥–µ–ª—å],
    # –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ–ø–∏—Ä–∞—Ç—å—Å—è –Ω–∞ –≠–¢–ò –¥–∞–Ω–Ω—ã–µ.
    if mode == "more" and not extra_ctx:
        nums_str = ", ".join(nums)
        MODEL_EXTRA_PENDING[uid] = {
            "kind": "table_more",
            # —Å–∞–º –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—á–∞—â–µ –≤—Å–µ–≥–æ ¬´–æ–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ (—Ç–∞–±–ª–∏—Ü–∞ N)¬ª)
            "question": text,
            # —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü –∏–∑ OOXML/–∫–∞—Ä—Ç–∏–Ω–∫–∏ ‚Äî —á—Ç–æ–±—ã [–º–æ–¥–µ–ª—å] –∏—Ö –≤–∏–¥–µ–ª–∞
            "ctx_tables": ctx_tables,
            "nums": nums,
            # –Ω—É–∂–µ–Ω, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –µ—â—ë —Ä–∞–∑ —Å—Ö–æ–¥–∏—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            "doc_id": doc_id,
        }
        await _send(
            m,
            "–í —Å–∞–º–æ–π —Ä–∞–±–æ—Ç–µ –Ω–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥—Ä–æ–±–Ω–æ –æ–±—ä—è—Å–Ω—è–µ—Ç —ç—Ç—É —Ç–∞–±–ª–∏—Ü—É. "
            "–ú–æ–≥—É –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ, –∫–∞–∫ [–º–æ–¥–µ–ª—å], –ø–æ–¥—Ä–æ–±–Ω–æ –ø–æ—è—Å–Ω–∏—Ç—å –µ—ë, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Å–∞–º–∏ –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã "
            "–∏ –æ–±—â–∏–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ (–±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–µ–∫—Å—Ç –í–ö–†). "
            "–ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî –Ω–∞–ø–∏—à–∏ ¬´–¥–∞¬ª, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ ‚Äî ¬´–Ω–µ—Ç¬ª."
        )
        return True


    # –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT: —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü +, –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏, –¥–æ–ø. —Ç–µ–∫—Å—Ç
    full_ctx = ctx_tables
    if extra_ctx:
        full_ctx += "\n\n[–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ —ç—Ç–∏ —Ç–∞–±–ª–∏—Ü—ã]\n" + extra_ctx

    system_prompt = (
        "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –ù–∏–∂–µ –¥–∞–Ω—ã —Ç–∞–±–ª–∏—Ü—ã, —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –ø—Ä—è–º–æ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
        "–û—Ç–≤–µ—á–∞–π –°–¢–†–û–ì–û –ø–æ —ç—Ç–∏–º –¥–∞–Ω–Ω—ã–º:\n"
        "‚Äî –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏, —Å—Ç–æ–ª–±—Ü—ã –∏ –∑–Ω–∞—á–µ–Ω–∏—è;\n"
        "‚Äî –Ω–µ –¥–æ–±–∞–≤–ª—è–π —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö;\n"
        "‚Äî –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∏ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç—ã, –≤—ã—Ä—É—á–∫–∞, —Ä—ã–Ω–æ–∫, "
        "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø.), –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö, –ø–æ–¥–ø–∏—Å—è—Ö –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞—Ö —Ç–∞–±–ª–∏—Ü;\n"
        "‚Äî –Ω–µ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å—ã–≤–∞–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ.\n"
        "–ï—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ —É–∫–∞–∑–∞–Ω –Ω–æ–º–µ—Ä —Ç–∞–±–ª–∏—Ü—ã, –Ω–æ —Ç–∞–∫–æ–π —Ç–∞–±–ª–∏—Ü—ã –Ω–µ—Ç –≤ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî "
        "–Ω–∞–ø–∏—à–∏, —á—Ç–æ –ø–æ —ç—Ç–æ–º—É –Ω–æ–º–µ—Ä—É –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç.\n\n"
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –î–û–õ–ñ–ù–ê –±—ã—Ç—å —Ç–∞–∫–æ–π:\n"
        "1) –†–∞–∑–¥–µ–ª ¬´–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü—ã¬ª ‚Äî –∫–æ—Ä–æ—Ç–∫–æ –æ–±—ä—è—Å–Ω–∏, —á—Ç–æ –ø–æ —Å—Ç—Ä–æ–∫–∞–º –∏ —á—Ç–æ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º.\n"
        "2) –†–∞–∑–¥–µ–ª ¬´–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª ‚Äî –≤—ã–ø–∏—à–∏ –í–°–ï —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –ë–ï–ó –ü–†–û–ü–£–°–ö–û–í.\n"
        "   –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–û—Ç—Ü—ã¬ª, ¬´–ú–∞—Ç–µ—Ä–∏¬ª, ¬´–û–±—â–µ–µ¬ª)\n"
        "   –Ω–∞–ø–∏—à–∏ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞:\n"
        "   ¬´–û—Ç—Ü—ã: 31,55; 26,85; 27,1; ‚Ä¶; 51,1¬ª ‚Äî –∑–Ω–∞—á–µ–Ω–∏—è –∏–¥—É—Ç —Å—Ç—Ä–æ–≥–æ –ø–æ –ø–æ—Ä—è–¥–∫—É —Å—Ç–æ–ª–±—Ü–æ–≤.\n"
        "   –ù–µ–ª—å–∑—è –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —Å—Ç—Ä–æ–∫–∏ –∏ –Ω–µ–ª—å–∑—è –≤—ã–±—Ä–∞—Å—ã–≤–∞—Ç—å –∫–∞–∫–∏–µ-–ª–∏–±–æ —á–∏—Å–ª–∞.\n"
        "3) –†–∞–∑–¥–µ–ª ¬´–í—ã–≤–æ–¥—ã¬ª ‚Äî —Å–¥–µ–ª–∞–π –∞–∫–∫—É—Ä–∞—Ç–Ω—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –∏ –≤—ã–≤–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n"
        "–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –∞–±–∑–∞—Ü, –Ω–∞—á–∏–Ω–∞—é—â–∏–π—Å—è —Å ¬´–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ¬ª, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–≤–µ–¥–∏ –µ–≥–æ "
        "–æ—Ç–¥–µ–ª—å–Ω—ã–º –ø–æ–¥–ø—É–Ω–∫—Ç–æ–º ¬´–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ¬ª –∏ –Ω–µ —Å–æ–∫—Ä–∞—â–∞–π —Ç–µ–∫—Å—Ç."
    )


    # –í —Ä–µ–∂–∏–º–µ "–ø–æ–¥—Ä–æ–±–Ω–µ–µ" –ø—Ä—è–º–æ –≥–æ–≤–æ—Ä–∏–º, —á—Ç–æ –Ω—É–∂–µ–Ω –±–æ–ª–µ–µ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π —Ä–∞–∑–±–æ—Ä
    if mode == "more":

        user_prompt = (
            f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {text}\n\n"
            "–ù–∏–∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü –≤ –º–∞—à–∏–Ω–Ω–æ-—á–∏—Ç–∞–µ–º–æ–º –≤–∏–¥–µ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —Ä–∞–±–æ—Ç—ã. "
            "–°–¥–µ–ª–∞–π –ë–û–õ–ï–ï –ü–û–î–†–û–ë–ù–´–ô —Ä–∞–∑–±–æ—Ä –ø–æ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü–µ.\n\n"
            "–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ:\n"
            "‚Äî —Å—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –æ—Ç–≤–µ—Ç–∞ –∏–∑ —Å–∏—Å—Ç–µ–º–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ "
            "(¬´–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü—ã¬ª ‚Üí ¬´–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª ‚Üí ¬´–í—ã–≤–æ–¥—ã¬ª);\n"
            "‚Äî –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª –ø–µ—Ä–µ—á–∏—Å–ª–∏ –í–°–ï –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∏ –∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è, –±–µ–∑ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π "
            "–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (–º–æ–∂–Ω–æ –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–æ–≤ –ø–æ –≥—Ä—É–ø–ø–∞–º/—Å—Ç—Ä–æ–∫–∞–º/—Å—Ç–æ–ª–±—Ü–∞–º);\n"
            "‚Äî –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–í—ã–≤–æ–¥—ã¬ª –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π —Ä–∞–∑–ª–∏—á–∏—è –∏ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
            f"{_verbosity_addendum('detailed', '–ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã')}\n\n"
            "[–¢–∞–±–ª–∏—Ü—ã –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞]\n"
            f"{full_ctx}"
        )

    else:
        user_prompt = (
            f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {text}\n\n"
            "–ù–∏–∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü –≤ –º–∞—à–∏–Ω–Ω–æ-—á–∏—Ç–∞–µ–º–æ–º –≤–∏–¥–µ. "
            "–û—Ç–≤–µ—Ç –æ—Ñ–æ—Ä–º–∏ –≤ —Ç—Ä–∏ —è–≤–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∞: ¬´–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü—ã¬ª, ¬´–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª, ¬´–í—ã–≤–æ–¥—ã¬ª.\n"
            "–°–Ω–∞—á–∞–ª–∞ –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü—ã¬ª –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –æ–±—ä—è—Å–Ω–∏, —á—Ç–æ –ø–æ —Å—Ç—Ä–æ–∫–∞–º –∏ —á—Ç–æ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º.\n"
            "–ó–∞—Ç–µ–º –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª –≤—ã–ø–∏—à–∏ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ "
            "(–º–æ–∂–Ω–æ —Å–ø–∏—Å–∫–∞–º–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º/—Å—Ç—Ä–æ–∫–∞–º/—Å—Ç–æ–ª–±—Ü–∞–º), –Ω–µ —Å–æ–∫—Ä–∞—â–∞—è –∏ –Ω–µ –≤—ã–±—Ä–∞—Å—ã–≤–∞—è —á–∏—Å–ª–∞.\n"
            "–í –∫–æ–Ω—Ü–µ, –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–í—ã–≤–æ–¥—ã¬ª, —Å–¥–µ–ª–∞–π –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–µ –≤—ã–≤–æ–¥—ã: –∫–∞–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã—à–µ/–Ω–∏–∂–µ, "
            "–∫–∞–∫–∏–µ —Ä–∞–∑–ª–∏—á–∏—è –∑–∞–º–µ—Ç–Ω—ã, –∫–∞–∫–∏–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –º–æ–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å.\n"
            "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏–∫–∞–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü."
            f"{_verbosity_addendum(verbosity, '–æ–ø–∏—Å–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã')}\n\n"
            "[–¢–∞–±–ª–∏—Ü—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞]\n"
            f"{full_ctx}"
        )


    try:
        answer = chat_with_gpt(
            [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.2,
            max_tokens=FINAL_MAX_TOKENS,
        )
    except Exception as e:
        logging.exception("table explanation failed: %s", e)
        await _send(
            m,
            "–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ —Ç–∞–±–ª–∏—Ü–µ ‚Äî –ø—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. "
            f"–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –¥–ª—è –ª–æ–≥–æ–≤: {e}"
        )
        return True  # –∑–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –Ω–µ –ø—Ä–æ–≤–∞–ª–∏–≤–∞–µ–º—Å—è –≤ –æ–±—â–∏–π RAG

    answer = (answer or "").strip()

    # --- –ù–û–í–û–ï: –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–∞–ª–∞ –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–æ–≤—Å–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç,
    # –ø—Ä–æ–±—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π fallback-–ø—Ä–æ–º–ø—Ç —Ç–æ–ª—å–∫–æ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º —Ç–∞–±–ª–∏—Ü—ã.
    if not answer or len(answer) < 60:
        if raw_values_text:
            fb_system = (
                "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –ù–∏–∂–µ –¥–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã "
                "–∏–∑ –¥–∏–ø–ª–æ–º–∞ (–≤—Å–µ –µ—ë –∑–Ω–∞—á–µ–Ω–∏—è). "
                "–ü–æ —ç—Ç–∏–º –¥–∞–Ω–Ω—ã–º –æ–ø–∏—à–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, —á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü–∞, "
                "–∫–∞–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã—à–µ/–Ω–∏–∂–µ –∏ –∫–∞–∫–∏–µ 2‚Äì3 –≤—ã–≤–æ–¥–∞ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å. "
                "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã—Ö —á–∏—Å–µ–ª –∏ –Ω–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–π –ø—Ä–æ—Ü–µ–Ω—Ç—ã. "
                "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∏ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç—ã, –≤—ã—Ä—É—á–∫–∞, "
                "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø.), –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤ —Å–∞–º–æ–π —Ç–∞–±–ª–∏—Ü–µ."
            )

            fb_user = (
                f"–¢–∞–±–ª–∏—Ü–∞ –∏–∑ –¥–∏–ø–ª–æ–º–∞:\n{ctx_tables}\n\n"
                f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {text}\n\n"
                "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –ø–æ–Ω—è—Ç–Ω–æ–µ —á–µ–ª–æ–≤–µ–∫—É –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –≤—ã–≤–æ–¥—ã –ø–æ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü–µ."
            )
            try:
                fb_answer = chat_with_gpt(
                    [
                        {"role": "system", "content": fb_system},
                        {"role": "user",   "content": fb_user},
                    ],
                    temperature=0.3,
                    max_tokens=FINAL_MAX_TOKENS,
                )
            except Exception as e:
                logging.exception("table fallback explanation failed: %s", e)
                fb_answer = ""

            fb_answer = (fb_answer or "").strip()
            if fb_answer:
                answer = fb_answer
    # --- /–ù–û–í–û–ï ---

    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤ –∏—Ç–æ–≥–µ —Ç–∞–∫ –∏ –Ω–µ –¥–∞–ª–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è
    if not answer:
        if raw_values_text:
            await _send(
                m,
                raw_values_text
                + "\n\n"
                + "–ú–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã. "
                  "–í–æ—Ç —Å–∞–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–æ—è—Å–Ω–µ–Ω–∏–µ ‚Äî –ø–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
            )
        else:
            await _send(
                m,
                "–ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ —Ç–∞–±–ª–∏—Ü–µ. "
                "–ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–¥–∞—Ç—å –µ–≥–æ –µ—â—ë —Ä–∞–∑."
            )
        return True  # —Ç–æ–∂–µ –Ω–µ –ø–∞–¥–∞–µ–º –≤ –æ–±—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω

    # –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –∫–µ–π—Å: —Å–Ω–∞—á–∞–ª–∞ –í–°–ï –∑–Ω–∞—á–µ–Ω–∏—è, –ø–æ—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
    final_answer = _strip_unwanted_sections(answer)
    if raw_values_text:
        final_answer = raw_values_text + "\n\n\n" + final_answer

    await _send(m, final_answer)
    return True



# -------------------------- –°–ê–ú–û–í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ò–ù–î–ï–ö–°–ê --------------------------

def _count_et(con, uid: int, doc_id: int, et: str) -> int:
    cur = con.cursor()
    if _table_has_columns(con, "chunks", ["element_type"]):
        cur.execute(
            "SELECT COUNT(*) AS c FROM chunks WHERE owner_id=? AND doc_id=? AND element_type=?",
            (uid, doc_id, et),
        )
        row = cur.fetchone()
        return int(row["c"] or 0)
    return 0

def _need_self_heal(uid: int, doc_id: int, need_refs: bool, need_figs: bool) -> tuple[bool, int, int]:
    """
    –°–∞–º–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–ø–µ—Ä—å –∑–∞–≤—è–∑–∞–Ω–æ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ú–ï–î–ò–ê–î–ê–ù–ù–´–•:
      ‚Äî –µ—Å—Ç—å –ª–∏ –≥–¥–µ-—Ç–æ –≤ attrs —Å–ø–∏—Å–æ–∫ images;
      ‚Äî –µ—Å—Ç—å –ª–∏ chart_data (–¥–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º), –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç element_type.
    –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –Ω–∞–π–¥–µ–Ω–æ ‚Äî —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ ¬´—Ñ–∏–≥—É—Ä—ã –µ—Å—Ç—å¬ª (fc=1).
    –§–æ–ª–±—ç–∫ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –ë–î –±–µ–∑ attrs: —Å—á–∏—Ç–∞–µ–º element_type='figure'.
    """
    con = get_conn()
    rc = _count_et(con, uid, doc_id, "reference") if need_refs else 1

    # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ¬´—Ñ–∏–≥—É—Ä—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç¬ª, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã
    fc = 1
    if need_figs:
        media_found = False
        try:
            cur = con.cursor()
            # –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ attrs ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ø—Ä—è–º—É—é –º–µ–¥–∏–∞–¥–∞–Ω–Ω—ã–µ
            if _table_has_columns(con, "chunks", ["attrs"]):
                cur.execute(
                    "SELECT attrs FROM chunks WHERE owner_id=? AND doc_id=? AND attrs IS NOT NULL",
                    (uid, doc_id),
                )
                rows = cur.fetchall() or []
                for r in rows:
                    attrs_json = r["attrs"] or None
                    if not attrs_json:
                        continue
                    # –±—ã—Å—Ç—Ä—ã–π —á–µ–∫ –Ω–∞ images
                    try:
                        a = json.loads(attrs_json)
                        imgs = a.get("images") or []
                        if isinstance(imgs, list) and any(imgs):
                            media_found = True
                            break
                    except Exception:
                        pass
                    # –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏–º chart_data (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ä—Å–µ—Ä)
                    try:
                        cd, _, _ = _parse_chart_data(attrs_json)  # returns (rows|None, type|None, attrs_dict)
                        if cd:
                            media_found = True
                            break
                    except Exception:
                        # –ø–∞—Ä—Å–µ—Ä –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–µ—à–µ–Ω–∏—è ‚Äî –ø—Ä–æ—Å—Ç–æ –∏–¥—ë–º –¥–∞–ª—å—à–µ
                        pass
            else:
                # –æ—á–µ–Ω—å —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å –±–µ–∑ attrs ‚Äî —Ñ–æ–ª–±—ç–∫ –∫ figure-—á–∞–Ω–∫–∞–º
                media_found = (_count_et(con, uid, doc_id, "figure") > 0)
        except Exception:
            # –∑–∞—â–∏—Ç–Ω—ã–π —Ñ–æ–ª–±—ç–∫: —Å—á–∏—Ç–∞–µ–º –ø–æ figure-—á–∞–Ω–∫–∞–º
            media_found = (_count_et(con, uid, doc_id, "figure") > 0)

        fc = 1 if media_found else 0

    con.close()
    return (rc == 0 or fc == 0, rc, fc)


def _reindex_with_sections(uid: int, doc_id: int, sections: list[dict]) -> None:
    delete_document_chunks(doc_id, uid)
    index_document(uid, doc_id, sections)
    invalidate_cache(uid, doc_id)
    set_document_indexer_version(doc_id, CURRENT_INDEXER_VERSION)
    update_document_meta(doc_id, layout_profile=_current_embedding_profile())


async def _ensure_modalities_indexed(m: types.Message, uid: int, doc_id: int, intents: dict):
    """–ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å—Ç–∞—Ä—ã–π –∏ –Ω–µ—Ç reference/figure ‚Äî —Ç–∏—Ö–æ –ø–µ—Ä–µ–ø–∞—Ä—Å–∏–º –Ω–æ–≤—ã–º –ø–∞—Ä—Å–µ—Ä–æ–º –∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º."""
    need_refs = bool(intents.get("sources", {}).get("want"))

    need_figs = bool(intents.get("figures", {}).get("want"))
    if not (need_refs or need_figs):
        return

    should, have_refs, have_figs = _need_self_heal(uid, doc_id, need_refs, need_figs)
    if not should:
        return

    con = get_conn()
    cur = con.cursor()
    cur.execute("SELECT path FROM documents WHERE id=? AND owner_id=?", (doc_id, uid))
    row = cur.fetchone()
    con.close()
    if not row:
        return

    path = row["path"]
    try:
        sections = _parse_by_ext(path)
        sections = enrich_sections(sections, doc_kind=os.path.splitext(path)[1].lower().strip("."))
    except Exception as e:
        logging.exception("re-parse/enrich failed: %s", e)
        return

    new_refs = sum(1 for s in sections if (s.get("element_type") == "reference"))
    new_figs = sum(1 for s in sections if (s.get("element_type") == "figure"))

    do_reindex = False
    if need_refs and have_refs == 0 and new_refs > 0:
        do_reindex = True
    if need_figs and have_figs == 0 and new_figs > 0:
        do_reindex = True

    if do_reindex:
        try:
            _reindex_with_sections(uid, doc_id, sections)
            await _send(m, "–û–±–Ω–æ–≤–∏–ª –∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞: –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ —Ä–∏—Å—É–Ω–∫–∏/–∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–≤–∫–ª—é—á–∞—è OOXML-–¥–∏–∞–≥—Ä–∞–º–º—ã).")
        except Exception as e:
            logging.exception("self-heal reindex failed: %s", e)


# -------------------------- –°–±–æ—Ä —Ñ–∞–∫—Ç–æ–≤ --------------------------

def _gather_facts(uid: int, doc_id: int, intents: dict) -> dict:
    """
    –°–æ–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –ë–î/–∏–Ω–¥–µ–∫—Å–∞, –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.
    """
    facts: dict[str, object] = {"doc_id": doc_id, "owner_id": uid}
    # —Ñ–ª–∞–≥ ¬´—Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–∞ –∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ¬ª
    exact = bool(intents.get("exact_numbers"))
    # –µ—Å–ª–∏ —è–≤–Ω–æ –ø—Ä–æ—Å—è—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–∞–±–ª–∏—Ü—É(—ã) ‚Äî –≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ–º –≤ —Ä–µ–∂–∏–º–µ –¢–û–ß–ù–´–• —á–∏—Å–µ–ª
    if intents.get("tables", {}).get("describe"):
        exact = True
    facts["exact_numbers"] = exact


    # ----- –¢–∞–±–ª–∏—Ü—ã -----
    if intents["tables"]["want"]:
        total_tables = _count_tables(uid, doc_id)
        basenames = _distinct_table_basenames(uid, doc_id)

        con = get_conn()
        cur = con.cursor()
        items: list[str] = []
        for base in basenames:
            cur.execute(
                """
                SELECT attrs FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                  AND section_path LIKE ? || ' [row %'
                ORDER BY id ASC LIMIT 1
                """,
                (uid, doc_id, base),
            )
            r = cur.fetchone()
            attrs_json = r["attrs"] if r else None

            cur.execute(
                """
                SELECT text FROM chunks
                WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                  AND section_path LIKE ? || ' [row %'
                ORDER BY id ASC LIMIT 2
                """,
                (uid, doc_id, base),
            )
            rows = cur.fetchall() or []
            first_row_text = None
            for rr in rows:
                cand = (rr["text"] or "").split("\n")[0]
                cand = " ‚Äî ".join([c.strip() for c in cand.split(" | ") if c.strip()])
                if cand:
                    first_row_text = cand
                    break

            title = _compose_display_from_attrs(attrs_json, base, first_row_text)
            title = _strip_table_prefix(title)
            items.append(title)

        con.close()
        t_limit = int(intents.get("tables", {}).get("limit", 10))
        facts["tables"] = {
            "count": total_tables,
            "list": items[:t_limit],
            "more": max(0, len(items) - t_limit),
            "describe": [],
        }

        # –ê–≤—Ç–æ-–æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –æ–±—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã
        desc_cards = []
        if not intents.get("tables", {}).get("describe"):
            # –≤–æ–∑—å–º—ë–º –ø–µ—Ä–≤—ã–µ 3‚Äì5 —Ç–∞–±–ª–∏—Ü –∏–∑ —Å–ø–∏—Å–∫–∞
            bases = _distinct_table_basenames(uid, doc_id)[:min(5, t_limit)]
            con = get_conn()
            cur = con.cursor()
            for base in bases:
                # attrs + –ø–µ—Ä–≤—ã–µ 1‚Äì2 —Å—Ç—Ä–æ–∫–∏
                cur.execute("""
                    SELECT page, section_path, attrs FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                    AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 1
                """, (uid, doc_id, base, base))
                row = cur.fetchone()
                if not row:
                    continue

                cur.execute("""
                    SELECT text FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                    AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 2
                """, (uid, doc_id, row["section_path"], row["section_path"]))
                rows = cur.fetchall() or []
                highlights = []
                for r in rows:
                    first = (r["text"] or "").split("\n")[0]
                    if first:
                        highlights.append(" ‚Äî ".join([c.strip() for c in first.split(" | ") if c.strip()]))

                attrs_json = row["attrs"] if row else None
                display = _compose_display_from_attrs(attrs_json, row["section_path"], highlights[0] if highlights else None)
                display = _strip_table_prefix(display)

                # –ø–æ–ø—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å –Ω–æ–º–µ—Ä –¥–ª—è stats
                num, _ = _parse_table_title(display)
                stats = None
                if num:
                    try:
                        stats = analyze_table_by_num(uid, doc_id, num, max_series=6)
                    except Exception:
                        stats = None

                desc_cards.append({
                    "num": num,
                    "display": display,
                    "where": {"page": row["page"], "section_path": row["section_path"]},
                    "highlights": highlights,
                    "stats": stats,
                })
            con.close()

        # –û–±—â–∏–π —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü —Å –∫—Ä–∞—Ç–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º
        facts["tables"]["describe"] = desc_cards

        # describe –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –Ω–æ–º–µ—Ä–∞–º + —Ç–æ—á–Ω—ã–µ —Ä–∞—Å—á–µ—Ç—ã (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º describe, –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π)
        if intents.get("tables", {}).get("describe"):
            desc_cards = []
            con = get_conn()
            cur = con.cursor()
            for num in intents["tables"]["describe"]:

                like1 = f'%\"caption_num\": \"{num}\"%'
                like2 = f'%\"label\": \"{num}\"%'
                cur.execute(
                    """
                    SELECT page, section_path, attrs FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                      AND (attrs LIKE ? OR attrs LIKE ?)
                    ORDER BY id ASC LIMIT 1
                    """,
                    (uid, doc_id, like1, like2),
                )
                row = cur.fetchone()

                if not row:
                    cur.execute(
                        """
                        SELECT page, section_path, attrs FROM chunks
                        WHERE owner_id=? AND doc_id=? AND element_type IN ('table','table_row')
                          AND section_path LIKE ? COLLATE NOCASE
                        ORDER BY id ASC LIMIT 1
                        """,
                        (uid, doc_id, f'%–¢–∞–±–ª–∏—Ü–∞ {num}%'),
                    )
                    row = cur.fetchone()

                if not row:
                    continue

                attrs_json = row["attrs"] if row else None
                # 1‚Äì2 –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ highlights
                cur.execute(
                    """
                    SELECT text FROM chunks
                    WHERE owner_id=? AND doc_id=? AND element_type='table_row'
                      AND (section_path=? OR section_path LIKE ? || ' [row %')
                    ORDER BY id ASC LIMIT 2
                    """,
                    (uid, doc_id, row["section_path"], row["section_path"]),
                )
                rows = cur.fetchall()
                highlights = []
                for r in rows or []:
                    first_line = (r["text"] or "").split("\n")[0]
                    if first_line:
                        highlights.append(" ‚Äî ".join([c.strip() for c in first_line.split(" | ") if c.strip()]))

                base = row["section_path"]
                first_row_text = highlights[0] if highlights else None
                display = _compose_display_from_attrs(attrs_json, base, first_row_text)
                display = _strip_table_prefix(display)

                # –ù–û–í–û–ï: —Ç–æ—á–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ —Ç–∞–±–ª–∏—Ü–µ
                stats = None
                try:
                    stats = analyze_table_by_num(uid, doc_id, num, max_series=6)
                except Exception:
                    stats = None

                desc_cards.append({
                    "num": num,
                    "display": display,
                    "where": {"page": row["page"], "section_path": row["section_path"]},
                    "highlights": highlights,
                    "stats": stats,
                })
            con.close()

            facts["tables"]["describe"] = desc_cards

            # –∑–∞–ø–æ–º–Ω–∏–º —ç—Ç–∏ –Ω–æ–º–µ—Ä–∞ —Ç–∞–±–ª–∏—Ü –∫–∞–∫ ¬´–ø–æ—Å–ª–µ–¥–Ω–∏–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ¬ª
            try:
                LAST_REF.setdefault(uid, {})["table_nums"] = [
                    str(c["num"]) for c in desc_cards if c.get("num")
                ]
            except Exception:
                pass

    # ----- –†–∏—Å—É–Ω–∫–∏ -----
    if intents["figures"]["want"]:
        f_limit = int(intents.get("figures", {}).get("limit", 10))
        lst = _list_figures_db(uid, doc_id, limit=f_limit)
        figs_block = {
            "count": int(lst.get("count") or 0),
            "list": list(lst.get("list") or []),
            "more": int(lst.get("more") or 0),
            "describe_lines": [],
            "describe_cards": [],
        }

        nums = list(intents.get("figures", {}).get("describe") or [])
        if nums:
            try:
                # ‚öôÔ∏è –ë–µ—Ä—ë–º –∫–∞—Ä—Ç–æ—á–∫–∏ –¢–û–õ–¨–ö–û –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –Ω–æ–º–µ—Ä–∞–º (5 –∏ 6 ‚Üí —Ç–æ–ª—å–∫–æ 5 –∏ 6)
                cards = describe_figures_by_numbers(
                    uid,
                    doc_id,
                    nums,
                    sample_chunks=2,
                    use_vision=True,
                    lang="ru",
                    vision_first_image_only=True,
                ) or []
                logging.info(
                    "FIG: –ø–æ–ª—É—á–µ–Ω–æ %d —Ä–∏—Å—É–Ω–∫–æ–≤ –¥–ª—è –Ω–æ–º–µ—Ä–æ–≤ %s",
                    len(cards),
                    ", ".join(map(str, nums)),
                )

                if not cards:
                    figs_block["describe_lines"] = ["–î–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ."]
                    figs_block["describe_cards"] = []
                else:
                    # –û—Å–Ω–æ–≤–Ω–æ–µ –¥–ª—è answer_builder: –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä describe_cards
                    figs_block["describe_cards"] = cards

                    # –ß—Ç–æ–±—ã –≤ [Figures]/list –Ω–µ –ø–æ–ø–∞–¥–∞–ª–∏ –ª–∏—à–Ω–∏–µ —Ä–∏—Å—É–Ω–∫–∏,
                    # –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –±—ã–ª —Ç–æ–ª—å–∫–æ –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ–º–µ—Ä–∞.
                    figs_block["list"] = [
                        (c.get("display")
                         or f"–†–∏—Å—É–Ω–æ–∫ {c.get('num') or ''}".strip())
                        for c in cards
                    ]
                    figs_block["count"] = len(figs_block["list"])
                    figs_block["more"] = 0

                    # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ describe_lines
                    lines = []
                    for c in cards:
                        disp = c.get("display") or "–†–∏—Å—É–Ω–æ–∫"
                        vis = (c.get("vision") or {}).get("description", "") or ""
                        vis_clean = vis.strip()
                        low_vis = vis_clean.lower()
                        if ("–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ" in low_vis
                                or "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è" in low_vis):
                            vis_clean = ""
                        hint = "; ".join([h for h in (c.get("highlights") or []) if h])
                        if vis_clean:
                            lines.append(f"{disp}: {vis_clean}")
                        elif hint:
                            lines.append(f"{disp}: {hint}")
                        else:
                            lines.append(disp)
                    figs_block["describe_lines"] = lines[:25]
            except Exception as e:
                figs_block["describe_lines"] = [f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø–∏—Å–∞—Ç—å —Ä–∏—Å—É–Ω–∫–∏: {e}"]
                figs_block["describe_cards"] = []

        facts["figures"] = figs_block


    # ----- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ -----
    if intents["sources"]["want"]:
        con = get_conn()
        cur = con.cursor()
        has_type = _table_has_columns(con, "chunks", ["element_type", "attrs"])
        items: list[str] = []

        if has_type:
            cur.execute(
                "SELECT text FROM chunks WHERE owner_id=? AND doc_id=? AND element_type='reference' ORDER BY id ASC",
                (uid, doc_id),
            )
            items = [(r["text"] or "").strip() for r in cur.fetchall()]

        if not any(items):
            cur.execute(
                """
                SELECT element_type, section_path, text
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                ORDER BY page ASC, id ASC
                """,
                (uid, doc_id),
            )
            raw = []
            for r in cur.fetchall():
                sec = (r["section_path"] or "").lower()
                if not any(k in sec for k in ("–∏—Å—Ç–æ—á–Ω–∏–∫", "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä", "–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ", "reference", "bibliograph")):
                    continue
                et = (r["element_type"] or "").lower()
                if et in ("heading", "table", "figure", "table_row"):
                    continue
                t = (r["text"] or "").strip()
                if t:
                    raw.append(t)

            seen = set()
            items = []
            for t in raw:
                k = re.sub(r"\s+", " ", t).strip().rstrip(".").lower()
                if len(k) < 5 or k in seen:
                    continue
                seen.add(k)
                items.append(t)

        con.close()

        s_limit = int(intents.get("sources", {}).get("limit", 25))
        facts["sources"] = {
            "count": len(items),
            "list": items[:s_limit],
            "more": max(0, len(items) - s_limit),
        }


    # ----- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å -----
    if intents.get("practical"):
        facts["practical_present"] = _has_practical_part(uid, doc_id)

    # ----- Summary -----
    if intents.get("summary"):
        s = overview_context(uid, doc_id, max_chars=6000) or _first_chunks_context(uid, doc_id, n=12, max_chars=6000)
        if s:
            facts["summary_text"] = s

    # ----- –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç / —Ü–∏—Ç–∞—Ç—ã -----
    # app/bot.py (_gather_facts: –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç / —Ü–∏—Ç–∞—Ç—ã)
    if intents.get("general_question"):
        question_text = intents.get("general_question") or ""

        vb = verbatim_find(uid, doc_id, question_text, max_hits=3)

        cov = retrieve_coverage(
            owner_id=uid,
            doc_id=doc_id,
            question=question_text,
        )
        ctx = ""
        if cov and cov.get("snippets"):
            ctx = build_context_coverage(
                cov["snippets"],
                items_count=len(cov.get("items") or []) or None,
            )

        if not ctx:
            ctx = best_context(uid, doc_id, question_text, max_chars=6000)
        if not ctx:
            hits = retrieve(uid, doc_id, question_text, top_k=12)
            if hits:
                ctx = build_context(hits)
        if not ctx:
            ctx = _first_chunks_context(uid, doc_id, n=12, max_chars=6000)

        if ctx:
            facts["general_ctx"] = ctx
        if vb:
            facts["verbatim_hits"] = vb
        if cov and cov.get("items"):
            facts["coverage"] = {"items": cov["items"]}
            facts["general_subitems"] = [
                {"id": i + 1, "ask": s} if isinstance(s, str) else s
                for i, s in enumerate(cov["items"])
            ]

        # --- [VISION] –≤—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥: —á–∏—Å–ª–∞ –∏–∑ –¥–∏–∞–≥—Ä–∞–º–º/–∫–∞—Ä—Ç–∏–Ω–æ–∫ (–ø–æ–¥–º–µ—à–∏–≤–∞–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç) ---
        try:
            vision_block = ""
            if getattr(Cfg, "vision_active", lambda: False)():
                # 1) –±–µ—Ä—ë–º —Ç–æ–ø-—Ö–∏—Ç—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫
                hits_v = retrieve(uid, doc_id, question_text, top_k=10) or []

                # 1–∞) –µ—Å–ª–∏ –≤ —Ö–∏—Ç–∞—Ö –µ—Å—Ç—å chart_data (DOCX-–¥–∏–∞–≥—Ä–∞–º–º—ã) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω—ã–µ —á–∏—Å–ª–∞, –±–µ–∑ vision
                chart_lines: list[str] = []
                for h in hits_v:
                    attrs = (h.get("attrs") or {})
                    cd = attrs.get("chart_data")
                    if cd:
                        # –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä: —É–ø–∞–∫—É–µ–º –≤ attrs-json
                        try:
                            cd_list, _, _ = _parse_chart_data(json.dumps({"chart_data": cd}))
                        except Exception:
                            cd_list = None
                        if cd_list:
                            chart_lines.append(_format_chart_values(cd_list))

                if chart_lines:
                    vision_block = "\n".join(chart_lines[:3])
                else:
                    # 2) –∏–Ω–∞—á–µ ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º 1‚Äì3 –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ –Ω–æ–≤—ã–π vision_analyzer
                    img_paths = _pick_images_from_hits(
                        hits_v,
                        limit=getattr(Cfg, "VISION_MAX_IMAGES_PER_REQUEST", 3),
                    )
                    if img_paths and va_analyze_figure:
                        chunks: list[str] = []
                        hint = question_text[:300]
                        for p in img_paths:
                            try:
                                res = va_analyze_figure(p, caption_hint=hint, lang="ru")
                            except Exception:
                                continue

                            text_block = ""
                            if isinstance(res, dict):
                                pairs = res.get("data") or []
                                text_block = (res.get("text") or "").strip() or _pairs_to_bullets(pairs)
                            else:
                                text_block = (str(res) or "").strip()

                            if text_block:
                                chunks.append("[Text on image]\n" + text_block)

                        if chunks:
                            vision_block = "\n\n".join(chunks)
                        elif FIG_STRICT:
                            vision_block = "[No precise data]"

            if vision_block:
                prev = facts.get("general_ctx") or ""
                glue = ("\n\n" if prev else "")
                facts["general_ctx"] = (prev + glue + vision_block)
        except Exception:
            # –Ω–µ –ª–æ–º–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ vision –¥–∞–ª —Å–±–æ–π
            pass

        # --- [/VISION] ---


    # –ª–æ–≥–∏—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Å—Ä–µ–∑ —Ñ–∞–∫—Ç–æ–≤ (–±–µ–∑ –æ–≥—Ä–æ–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤)
    log_snapshot = dict(facts)
    if "general_ctx" in log_snapshot and isinstance(log_snapshot["general_ctx"], str):
        log_snapshot["general_ctx"] = log_snapshot["general_ctx"][:300] + "‚Ä¶" if len(log_snapshot["general_ctx"]) > 300 else log_snapshot["general_ctx"]
    if "summary_text" in log_snapshot and isinstance(log_snapshot["summary_text"], str):
        log_snapshot["summary_text"] = log_snapshot["summary_text"][:300] + "‚Ä¶" if len(log_snapshot["summary_text"]) > 300 else log_snapshot["summary_text"]
    logging.debug("FACTS: %s", json.dumps(log_snapshot, ensure_ascii=False))
    return facts


def _strip_unwanted_sections(s: str) -> str:
    """
    –ê–∫–∫—É—Ä–∞—Ç–Ω–æ —É–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ö–≤–æ—Å—Ç—ã –≤—Ä–æ–¥–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    ¬´–ß–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç: ‚Ä¶¬ª, –Ω–æ –Ω–µ –¥–æ–ø—É—Å–∫–∞–µ–º, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç
    –æ–±–Ω—É–ª–∏–ª—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é ‚Äî –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
    """
    if not s:
        return s

    original = s

    # –≤—ã—Ä–µ–∑–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –∞–±–∑–∞—Ü(—ã) –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—É—Å—Ç–æ–≥–æ —Ä–∞–∑—Ä—ã–≤–∞
    pat = re.compile(r"(?mis)^\s*(?:—á–µ–≥–æ|—á—Ç–æ)\s+–Ω–µ\s+—Ö–≤–∞—Ç–∞–µ—Ç\s*:.*?(?:\n\s*\n|\Z)")
    s = pat.sub("", s)
    # –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏-–º–µ—Ç–∫–∏
    s = re.sub(r"(?mi)^\s*–Ω–µ\s+—Ö–≤–∞—Ç–∞–µ—Ç\s*:.*$", "", s)

    s = s.strip()
    # –µ—Å–ª–∏ –ø–æ—Å–ª–µ –∑–∞—á–∏—Å—Ç–∫–∏ –≤—Å—ë –∏—Å—á–µ–∑–ª–æ ‚Äî –ª—É—á—à–µ –≤–µ—Ä–Ω—É—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç,
    # —á–µ–º –æ—Ç–¥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –ø—É—Å—Ç–æ—Ç—É
    return s or original.strip()


# ------------------------------ FULLREAD: –º–æ–¥–µ–ª—å —á–∏—Ç–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª ------------------------------

def _full_document_text(owner_id: int, doc_id: int, *, limit_chars: int | None = None) -> str:
    """–°–∫–ª–µ–∏–≤–∞–µ–º –í–ï–°–¨ —Ç–µ–∫—Å—Ç –∏–∑ chunks (page ASC, id ASC)."""
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT text FROM chunks WHERE owner_id=? AND doc_id=? ORDER BY page ASC, id ASC",
        (owner_id, doc_id),
    )
    rows = cur.fetchall() or []
    con.close()

    parts = []
    total = 0
    for r in rows:
        t = (r["text"] or "").strip()
        if not t:
            continue
        if limit_chars is not None and total + len(t) > limit_chars:
            remaining = max(0, limit_chars - total)
            if remaining > 0:
                parts.append(t[:remaining])
                total += remaining
            break
        parts.append(t)
        total += len(t)
    return "\n\n".join(parts)

def _fullread_try_answer(uid: int, doc_id: int, q_text: str) -> str | None:
    """
    DIRECT: –æ—Ç–¥–∞—ë–º –º–æ–¥–µ–ª–∏ —Ü–µ–ª–∏–∫–æ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–∞–∫ –µ–¥–∏–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None (—É–π–¥—ë–º –≤ –∏–Ω–æ–π —Ä–µ–∂–∏–º).
    """
    if getattr(Cfg, "FULLREAD_MODE", "off") != "direct":
        return None

    _limit = int(getattr(Cfg, "DIRECT_MAX_CHARS", 80000))
    full_text = _full_document_text(uid, doc_id, limit_chars=_limit + 1)
    if not full_text.strip():
        return None

    if len(full_text) > _limit:
        return None

    system_prompt = (
        "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –¢–µ–±–µ –¥–∞–Ω –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç –í–ö–†/–¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
        "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ–≤. –ù–µ –¥–æ–±–∞–≤–ª—è–π —Ä–∞–∑–¥–µ–ª–æ–≤ –≤–∏–¥–∞ "
        "¬´–ß–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç¬ª –∏ –Ω–µ –ø—Ä–æ—Å–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n"
        "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–¥–ø–∏—Å–∏ –∏ –±–ª–∏–∂–∞–π—à–∏–π —Ç–µ–∫—Å—Ç; –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞/–∑–Ω–∞—á–µ–Ω–∏—è.\n"
        "–ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞/—Ç–∞–±–ª–∏—Ü—ã –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ ‚Äî –æ—Ç–≤–µ—Ç—å: ¬´–¥–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ¬ª.\n"
        "–ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç –µ—Å—Ç—å, –Ω–æ –æ–Ω –≤ –ø–ª–æ—Ö–æ–º –∫–∞—á–µ—Å—Ç–≤–µ/–Ω–µ—á–∏—Ç–∞–µ–º ‚Äî –æ—Ç–≤–µ—Ç—å: ¬´–†–∏—Å—É–Ω–æ–∫ –ø–ª–æ—Ö–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–µ –º–æ–≥—É –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å¬ª, "
        "–∏ –¥–æ–±–∞–≤—å –∫—Ä–∞—Ç–∫—É—é –ø–æ–¥–ø–∏—Å—å/–∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞. –¶–∏—Ç–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–æ, –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
    )

    verbosity = _detect_verbosity(q_text)
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": f"[–î–æ–∫—É–º–µ–Ω—Ç ‚Äî –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç]\n{full_text}"},
        {"role": "user", "content": f"{q_text}\n\n{_verbosity_addendum(verbosity)}"},
    ]

    try:
        answer = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
        return (answer or "").strip() or None
    except Exception as e:
        logging.exception("fullread direct failed: %s", e)
        return None


def _fullread_collect_sections(uid: int, doc_id: int, *, max_sections: int = 800) -> List[str]:
    """
    –°–µ–∫—Ü–∏–∏ –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞: —Å–æ–±–∏—Ä–∞–µ–º –±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ section_path –≤ –ø–æ—Ä—è–¥–∫–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.
    """
    con = get_conn()
    cur = con.cursor()
    cur.execute(
        "SELECT section_path, text FROM chunks WHERE owner_id=? AND doc_id=? ORDER BY page ASC, id ASC",
        (uid, doc_id)
    )
    rows = cur.fetchall() or []
    con.close()

    out: List[str] = []
    cur_sec = None
    buf: List[str] = []

    def _flush():
        if buf:
            text = "\n".join([t for t in buf if t.strip()]).strip()
            if text:
                title = f"[{cur_sec}]" if cur_sec else ""
                out.append(f"{title}\n{text}" if title else text)
        buf.clear()


    for r in rows:
        sec = r["section_path"] or ""
        t = (r["text"] or "").strip()
        if not t:
            continue
        if cur_sec is None:
            cur_sec = sec
        if sec != cur_sec:
            _flush()
            cur_sec = sec
        buf.append(t)
        if len(out) >= max_sections:
            break
    _flush()
    return out[:max_sections]


def _group_for_steps(sections: Iterable[str], per_step_chars: int, max_steps: int) -> List[str]:
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–µ–∫—Ü–∏–∏ –≤ –±–∞—Ç—á–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞–º (–¥–ª—è map-—à–∞–≥–∞)."""
    batches: List[str] = []
    cur: List[str] = []
    cur_len = 0
    for s in sections:
        if cur_len + len(s) + 1 > per_step_chars and cur:
            batches.append("\n\n".join(cur))
            cur, cur_len = [], 0
            if len(batches) >= max_steps:
                break
        cur.append(s)
        cur_len += len(s) + 1
    if cur and len(batches) < max_steps:
        batches.append("\n\n".join(cur))
    return batches[:max_steps]

def _map_extract(uid: int, doc_id: int, question: str, chunk_text: str, *, map_tokens: int) -> str:
    """–û–¥–∏–Ω map-–≤—ã–∑–æ–≤: –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã/—Ü–∏—Ç–∞—Ç—ã –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞."""
    sys_map = (
        "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä. –¢–µ–±–µ –¥–∞–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–∏–ø–ª–æ–º–∞ –∏ –≤–æ–ø—Ä–æ—Å. "
        "–ò–∑–≤–ª–µ–∫–∏ –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏ –º–∏–Ω–∏-—Ü–∏—Ç–∞—Ç—ã, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ –≤–æ–ø—Ä–æ—Å—É. "
        "–ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ç–∞–±–ª–∏—Ü—ã ‚Äî –≤–∫–ª—é—á–∞–π –∏—Ö –Ω–∞–∑–≤–∞–Ω–∏—è –∏ 1‚Äì2 –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å —á–∏—Å–ª–∞–º–∏ "
        "(—Å–æ—Ö—Ä–∞–Ω—è–π –ø–æ—Ä—è–¥–æ–∫ –∏ –∑–Ω–∞—á–µ–Ω–∏—è). –§–æ—Ä–º–∞—Ç: –±—É–ª–ª–µ—Ç—ã."
    )
    return chat_with_gpt(
        [
            {"role": "system", "content": sys_map},
            {"role": "assistant", "content": f"[–§—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞]\n{chunk_text}"},
            {"role": "user", "content": f"–í–æ–ø—Ä–æ—Å: {question}\n–°–¥–µ–ª–∞–π –∫–æ—Ä–æ—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É (–±—É–ª–ª–µ—Ç—ã)."},
        ],
        temperature=0.1,
        max_tokens=max(120, int(map_tokens)),
    )

def _iterative_fullread_build_messages(uid: int, doc_id: int, question: str) -> Tuple[Optional[list], Optional[str]]:
    """
    –°–æ–±–∏—Ä–∞–µ–º map-–≤—ã–∂–∏–º–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º reduce-—Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Å—Ç—Ä–∏–º–∞
    –ò–õ–ò –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç (–µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫).
    """
    per_step = int(getattr(Cfg, "FULLREAD_STEP_CHARS", 14000))
    max_steps = int(getattr(Cfg, "FULLREAD_MAX_STEPS", 2))
    map_tokens = int(getattr(Cfg, "DIGEST_TOKENS_PER_SECTION", 300))

    sections = _fullread_collect_sections(uid, doc_id)
    if not sections:
        return None, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç —Å–µ–∫—Ü–∏—è–º–∏."

    batches = _group_for_steps(sections, per_step_chars=per_step, max_steps=max_steps)
    if not batches:
        return None, "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —à–∞–≥–∏ —á—Ç–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞."

    digests: List[str] = []
    for b in batches:
        try:
            digests.append(_map_extract(uid, doc_id, question, b, map_tokens=map_tokens))
        except Exception as e:
            logging.exception("map extract failed: %s", e)
            digests.append(b[:800])

    # –≤–º–µ—Å—Ç–æ —Ç–µ—Ö–Ω–∏—á–Ω—ã—Ö [MAP 1] –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏
    joined = "\n\n".join([f"[–§—Ä–∞–≥–º–µ–Ω—Ç {i+1}]\n{d}" for i, d in enumerate(digests)])
    ctx = joined[: int(getattr(Cfg, "FULLREAD_CONTEXT_CHARS", 9000))]

    sys_reduce = (
        "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –í–ö–†. –ù–∏–∂–µ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞. "
        "–°–æ–±–µ—Ä–∏ –∏–∑ –Ω–∏—Ö —Å–≤—è–∑–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã—Ö —Ü–∏—Ñ—Ä/—Ç–∞–±–ª–∏—Ü –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–π —Ä–∞–∑–¥–µ–ª–æ–≤ "
        "–ø—Ä–æ ¬´—á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç¬ª. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ –∏–º–µ—é—â–∏–º—Å—è –¥–∞–Ω–Ω—ã–º. "
        "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∏ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç—ã, –≤—ã—Ä—É—á–∫–∞, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø.), "
        "–µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ñ–∞–∫—Ç–∞—Ö/—Ü–∏—Ç–∞—Ç–∞—Ö.\n"
        "–ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞/—Ç–∞–±–ª–∏—Ü—ã –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ ‚Äî —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∫—Ä–∞—Ç–∫–æ: ¬´–¥–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ¬ª. "
        "–ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç –µ—Å—Ç—å, –Ω–æ –æ–Ω –Ω–µ—á–∏—Ç–∞–µ–º, –¥–∞–π: ¬´–†–∏—Å—É–Ω–æ–∫ –ø–ª–æ—Ö–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–µ –º–æ–≥—É –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å¬ª, "
        "–∏ –¥–æ–±–∞–≤—å –ø–æ–¥–ø–∏—Å—å/–∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞. "
        "–í —Å–≤–æ—ë–º –æ—Ç–≤–µ—Ç–µ –Ω–µ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∫–∏ –≤—Ä–æ–¥–µ ¬´—Ñ—Ä–∞–≥–º–µ–Ω—Ç 1¬ª –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–≤–æ ¬´–≤—ã–∂–∏–º–∫–∞¬ª."
    )



    verbosity = _detect_verbosity(question)
    messages = [
        {"role": "system", "content": sys_reduce},
        {"role": "assistant", "content": f"–°–≤–æ–¥–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{ctx}"},
        {"role": "user", "content": f"{question}\n\n{_verbosity_addendum(verbosity)}"},
    ]
    return messages, None


# ------------------------------ –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ ------------------------------

@dp.message(F.document)
async def handle_doc(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc = m.document

    # 0) FSM: —Ñ–∏–∫—Å–∏—Ä—É–µ–º, —á—Ç–æ –Ω–∞—á–∞–ª–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
    start_downloading(uid)
    await _send(m, Cfg.MSG_ACK_DOWNLOADING)

    # 1) —Å–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª —Ü–µ–ª–∏–∫–æ–º (–±–µ–∑ –æ–±—Ä–µ–∑–∫–∏)
    from io import BytesIO

    file = await bot.get_file(doc.file_id)
    buf = BytesIO()
    await bot.download_file(file.file_path, destination=buf)
    buf.seek(0)
    data = buf.read()  # –∑–¥–µ—Å—å –≤—Å–µ –±–∞–π—Ç—ã —Ñ–∞–π–ª–∞ –∫–∞–∫ –µ—Å—Ç—å

    # 2) —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫ (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞)
    filename = safe_filename(f"{m.from_user.id}_{doc.file_name}")
    path = save_upload(data, filename, Cfg.UPLOAD_DIR)
    await _send(m, Cfg.MSG_ACK_INDEXING)

    # 3) –æ–±—ë—Ä—Ç–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞ –ø–æ–¥ —Å–∏–≥–Ω–∞—Ç—É—Ä—É –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞ (–∑–∞–º—ã–∫–∞–µ–º uid)
    def _indexer_fn(doc_id: int, file_path: str, kind: str) -> dict:
        sections = _parse_by_ext(file_path)
        sections = enrich_sections(sections, doc_kind=os.path.splitext(file_path)[1].lower().strip("."))
        # sanity-check –Ω–∞ ¬´–ø—É—Å—Ç—ã–µ¬ª —Ñ–∞–π–ª—ã
        if sum(len(s.get("text") or "") for s in sections) < 500 and not any(
            s.get("element_type") in ("table", "table_row", "figure") for s in sections
        ):
            raise RuntimeError("–ü–æ—Ö–æ–∂–µ, —Ñ–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç ¬´–∂–∏–≤–æ–≥–æ¬ª —Ç–µ–∫—Å—Ç–∞/—Å—Ç—Ä—É–∫—Ç—É—Ä.")
        # –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è ¬´–∫–∞–∫ —Ä–∞–Ω—å—à–µ¬ª
        delete_document_chunks(doc_id, uid)
        index_document(uid, doc_id, sections)
        invalidate_cache(uid, doc_id)
        update_document_meta(doc_id, layout_profile=_current_embedding_profile())
        return {"sections_count": len(sections)}

    # 4) –∑–∞–ø—É—Å–∫–∞–µ–º –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä (–æ–Ω —Å–∞–º: –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å, INDEXING, READY/IDLE, –≤–µ—Ä—Å–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞)
    try:
        result = ingest_document(
            user_id=uid,
            file_path=path,
            kind=infer_doc_kind(doc.file_name),
            file_uid=getattr(doc, "file_unique_id", None),
            content_sha256=sha256_bytes(data),
            indexer_fn=_indexer_fn,
        )
    except Exception as e:
        logging.exception("ingest failed: %s", e)
        await _send(m, Cfg.MSG_INDEX_FAILED + f" –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏: {e}")
        return

    doc_id = int(result["doc_id"])
    ACTIVE_DOC[uid] = doc_id
    set_user_active_doc(uid, doc_id)

    # NEW: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Ä–∏—Å—É–Ω–∫–æ–≤ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞—Ç—å (—Å—Ç–∞—Ä—ã–π –ø—É—Ç—å)
    try:
        if fig_index_document is not None:
            FIG_INDEX[doc_id] = fig_index_document(path)
    except Exception as e:
        logging.exception("figures indexing failed: %s", e)


        # NEW: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ï–î–ò–ù–´–ô OOXML-–∏–Ω–¥–µ–∫—Å (–≥–ª–∞–≤—ã/—Ä–∏—Å—É–Ω–∫–∏/—Ç–∞–±–ª–∏—Ü—ã/–∏—Å—Ç–æ—á–Ω–∏–∫–∏) –±–µ–∑ LibreOffice
    # NEW: –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ï–î–ò–ù–´–ô OOXML-–∏–Ω–¥–µ–∫—Å (–≥–ª–∞–≤—ã/—Ä–∏—Å—É–Ω–∫–∏/—Ç–∞–±–ª–∏—Ü—ã/–∏—Å—Ç–æ—á–Ω–∏–∫–∏) –±–µ–∑ LibreOffice
    try:
        idx_oox = oox_build_index(path)
        OOXML_INDEX[doc_id] = idx_oox
        #persist –ø–æ–¥ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –ë–î ‚Äî —á—Ç–æ–±—ã _ooxml_get_index —Ä–∞–±–æ—Ç–∞–ª –ø–æ—Å–ª–µ —Ä–µ—Å—Ç–∞—Ä—Ç–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞
        try:
            os.makedirs(os.path.join("runtime", "indexes"), exist_ok=True)
            with open(os.path.join("runtime", "indexes", f"{doc_id}.json"), "w", encoding="utf-8") as f:
                json.dump(idx_oox, f, ensure_ascii=False, indent=2)
        except Exception:
            pass
    except Exception as e:
        logging.exception("ooxml build_index failed: %s", e)


    # 5) READY: —Å–æ–æ–±—â–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º ...
    await _send(
        m,
        (f"–≠—Ç–æ—Ç —Ñ–∞–π–ª —É–∂–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç #{doc_id}. " if result.get("reused") else "") + Cfg.MSG_READY
    )

    caption = (m.caption or "").strip()
    if caption:
        await respond_with_answer(m, uid, doc_id, caption)

    # –∞–≤—Ç–æ-–¥—Ä–µ–Ω–∞–∂ –æ—á–µ—Ä–µ–¥–∏ –æ–∂–∏–¥–∞–Ω–∏—è (–±–µ–∑ –¥—É–±–ª–µ–π –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤)
    try:
        queued = dequeue_all_pending_queries(uid)
        for item in queued:
            q = (item.get("text") or "").strip()
            if not q:
                continue
            # –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –∏–∑ –æ—á–µ—Ä–µ–¥–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø–æ–¥–ø–∏—Å—å—é –∫ —Ñ–∞–π–ª—É ‚Äî –Ω–µ –æ—Ç–≤–µ—á–∞–µ–º –≤—Ç–æ—Ä–æ–π —Ä–∞–∑
            if caption and q.strip() == caption:
                continue
            await respond_with_answer(m, uid, doc_id, q)
            await asyncio.sleep(0)  # –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º —Ü–∏–∫–ª
    except Exception as e:
        logging.exception("drain pending queue failed: %s", e)



# ------------------------------ –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç—á–∏–∫ ------------------------------

async def _answer_with_model_extra(m: types.Message, uid: int, base_question: str) -> None:
    """
    –û—Ç–≤–µ—Ç –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É ‚Äî –æ–±—â–∏–π —Å–æ–≤–µ—Ç –æ—Ç [–º–æ–¥–µ–ª—å].

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –≤ —Ç–µ–∫—Å—Ç–µ —Ä–∞–±–æ—Ç—ã –Ω–µ –Ω–∞—à–ª–æ—Å—å —Ñ–∞–∫—Ç–æ–≤ –ø–æ –≤–æ–ø—Ä–æ—Å—É
    –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª, —á—Ç–æ —Ö–æ—á–µ—Ç —Ç–∞–∫–æ–π –æ—Ç–≤–µ—Ç.
    """
    if not (chat_with_gpt or chat_with_gpt_stream):
        await _send(
            m,
            "–°–µ–π—á–∞—Å –º–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ —Ç–µ–∫—Å—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Ä–µ–∂–∏–º [–º–æ–¥–µ–ª—å] –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
        )
        return

    base_question = (base_question or "").strip()
    if not base_question:
        await _send(m, "–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –µ–≥–æ –µ—â—ë —Ä–∞–∑, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞.")
        return

    system_prompt = (
        "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —É—á—ë–±–µ. –í –≠–¢–û–ú –æ—Ç–≤–µ—Ç–µ —Ç—ã –Ω–µ –æ–ø–∏—Ä–∞–µ—à—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç –¥–∏–ø–ª–æ–º–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, "
        "–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å —Ç–æ–ª—å–∫–æ —Å–≤–æ–∏ –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –∏ –∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª. "
        "–°—Ä–∞–∑—É –≤ –Ω–∞—á–∞–ª–µ –æ—Ç–≤–µ—Ç–∞ —É–∫–∞–∂–∏ —Ç–µ–≥ '[–º–æ–¥–µ–ª—å] ' –∏ –¥–∞–ª—å—à–µ –æ—Ç–≤–µ—á–∞–π –ø—Ä–æ—Å—Ç—ã–º, –ø–æ–Ω—è—Ç–Ω—ã–º —è–∑—ã–∫–æ–º."
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": base_question},
    ]

    try:
        if STREAM_ENABLED and chat_with_gpt_stream is not None:
            stream = chat_with_gpt_stream(messages, temperature=0.3, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
            await _stream_to_telegram(m, stream)
            return

        answer = chat_with_gpt(messages, temperature=0.3, max_tokens=FINAL_MAX_TOKENS)
    except Exception as e:
        logging.exception("model-extra answer failed: %s", e)
        await _send(
            m,
            "–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç [–º–æ–¥–µ–ª—å]. –ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
        )
        return

    answer = (answer or "").strip()
    if not answer:
        await _send(
            m,
            "–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç [–º–æ–¥–µ–ª—å]. –ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
        )
        return

    if not answer.startswith("[–º–æ–¥–µ–ª—å]"):
        answer = "[–º–æ–¥–µ–ª—å] " + answer

    await _send(m, answer)

async def _answer_with_model_extra_table(
    m: types.Message,
    uid: int,
    doc_id: int,
    base_question: str,
    ctx_tables: str,
    nums: list[str],
) -> None:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç [–º–æ–¥–µ–ª—å] –ø–æ —Ç–∞–±–ª–∏—Ü–µ(—Ç–∞–±–ª–∏—Ü–∞–º):
    –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü –∏–∑ OOXML –∏ –º–æ–∂–µ—Ç –Ω–∞ –Ω–∏—Ö –æ–ø–∏—Ä–∞—Ç—å—Å—è,
    –¥–æ–±–∞–≤–ª—è—è –æ–±—â—É—é —Ç–µ–æ—Ä–∏—é, –Ω–æ –ù–ï –º–µ–Ω—è—è —Å–∞–º–∏ —á–∏—Å–ª–∞.
    """
    if not (chat_with_gpt or chat_with_gpt_stream):
        await _send(
            m,
            "–°–µ–π—á–∞—Å –º–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ —Ç–µ–∫—Å—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Ä–µ–∂–∏–º [–º–æ–¥–µ–ª—å] –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
        )
        return

    ctx_tables = (ctx_tables or "").strip()
    if not ctx_tables:
        # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π ‚Äî —Ñ–æ–ª–±—ç–∫ –≤ –æ–±—â–∏–π —Ä–µ–∂–∏–º
        await _answer_with_model_extra(m, uid, base_question)
        return

    nums = [str(n).strip() for n in (nums or []) if str(n).strip()]
    nums_str = ", ".join(nums) if nums else "—ç—Ç–∏–º —Ç–∞–±–ª–∏—Ü–∞–º"

    base_question = (base_question or "").strip()
    if not base_question:
        base_question = f"–ü–æ–¥—Ä–æ–±–Ω–æ –æ–±—ä—è—Å–Ω–∏ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–∞–±–ª–∏—Ü–µ(—Ç–∞–±–ª–∏—Ü–∞–º) {nums_str}."

    system_prompt = (
        "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —É—á—ë–±–µ. –í –≠–¢–û–ú –æ—Ç–≤–µ—Ç–µ —Ç—ã –æ–ø–∏—Ä–∞–µ—à—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü –∏–∑ –¥–∏–ø–ª–æ–º–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è "
        "(–æ–Ω–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã –Ω–∏–∂–µ –≤ –º–∞—à–∏–Ω–Ω–æ-—á–∏—Ç–∞–µ–º–æ–º –≤–∏–¥–µ). "
        "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ —á–∏—Å–ª–∞ –∫–∞–∫ –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã: –Ω–µ –º–µ–Ω—è–π –∏—Ö –∏ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –¥—Ä—É–≥–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è. "
        "–ü—Ä–∏ —ç—Ç–æ–º –º–æ–∂–µ—à—å –¥–æ–ø–æ–ª–Ω—è—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –æ–±—â–∏–º–∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å–≤–µ–¥–µ–Ω–∏—è–º–∏ –ø–æ —Ç–µ–º–µ. "
        "–°—Ä–∞–∑—É –≤ –Ω–∞—á–∞–ª–µ –æ—Ç–≤–µ—Ç–∞ —É–∫–∞–∂–∏ —Ç–µ–≥ '[–º–æ–¥–µ–ª—å] '."
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": f"[–î–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü –∏–∑ –¥–∏–ø–ª–æ–º–∞]\n{ctx_tables}"},
        {"role": "user", "content": base_question},
    ]

    try:
        if STREAM_ENABLED and chat_with_gpt_stream is not None:
            stream = chat_with_gpt_stream(
                messages,
                temperature=0.3,
                max_tokens=FINAL_MAX_TOKENS,
            )  # type: ignore
            await _stream_to_telegram(m, stream)
            return

        answer = chat_with_gpt(
            messages,
            temperature=0.3,
            max_tokens=FINAL_MAX_TOKENS,
        )
    except Exception as e:
        logging.exception("model-extra-table answer failed: %s", e)
        await _send(
            m,
            "–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ —Ç–∞–±–ª–∏—Ü–µ. –ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
        )
        return

    answer = (answer or "").strip()
    if not answer:
        await _send(
            m,
            "–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ —Ç–∞–±–ª–∏—Ü–µ. –ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
        )
        return

    if not answer.startswith("[–º–æ–¥–µ–ª—å]"):
        answer = "[–º–æ–¥–µ–ª—å] " + answer

    await _send(m, answer)

async def respond_with_answer(m: types.Message, uid: int, doc_id: int, q_text: str):
    q_text = (q_text or "").strip()
    orig_q_text = q_text  # –∑–∞–ø–æ–º–Ω–∏–º –∏—Å—Ö–æ–¥–Ω—É—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –¥–æ –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–æ–∫
    logger.info(
        "ANSWER: new question (uid=%s, doc_id=%s, len=%d): %r",
        uid,
        doc_id,
        len(q_text or ""),
        q_text,
    )
    if not q_text:
        logger.warning(
            "ANSWER: empty question from uid=%s, doc_id=%s",
            uid,
            doc_id,
        )
        await _send(m, "–í–æ–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π. –ù–∞–ø–∏—à–∏—Ç–µ, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –ø–æ –í–ö–†.")
        return

    viol = safety_check(q_text)
    if viol:
        logger.warning(
            "ANSWER: safety_check blocked question (uid=%s, doc_id=%s): %s",
            uid,
            doc_id,
            viol,
        )
        await _send(m, viol + " –ó–∞–¥–∞–π—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ –í–ö–†.")
        return

    logger.debug(
        "ANSWER: before GOST check (uid=%s, doc_id=%s)",
        uid,
        doc_id,
    )
    if await _maybe_run_gost(m, uid, doc_id, q_text):
        logger.info(
            "ANSWER: handled by GOST validator (uid=%s, doc_id=%s)",
            uid,
            doc_id,
        )
        return

    # –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ–ø–ª–∏–∫ –≤–∏–¥–∞ ¬´–æ–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ¬ª, ¬´—Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –Ω–µ–≥–æ¬ª
    q_text = _expand_with_last_referent(uid, q_text)

        # –ü—Ä–∏–º–µ—Ä—ã: "–æ–ø–∏—à–∏ —Ç–∞–±–ª–∏—Ü—É 4", "—á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü–∞ 2.3", "—Å–¥–µ–ª–∞–π –≤—ã–≤–æ–¥—ã –ø–æ —Ç–∞–±–ª–∏—Ü–µ 4"
    if _is_pure_table_request(q_text):
        verbosity = _detect_verbosity(q_text)
        base_text = (orig_q_text or "")
        mode = "more" if _FOLLOWUP_MORE_RE.search(base_text) else "normal"
        logger.info(
            "ANSWER: pure table request detected (uid=%s, doc_id=%s, mode=%s)",
            uid,
            doc_id,
            mode,
        )
        handled = await _answer_table_query(
            m, uid, doc_id, q_text, verbosity=verbosity, mode=mode
        )
        logger.info(
            "ANSWER: _answer_table_query finished (uid=%s, doc_id=%s, handled=%s)",
            uid,
            doc_id,
            handled,
        )
        if handled:
            return
        else:
            logger.info(
                "ANSWER: table pipeline did not handle request, falling back to general pipeline "
                "(uid=%s, doc_id=%s)",
                uid,
                doc_id,
            )


        # –µ—Å–ª–∏ _answer_table_query –Ω–µ —Å–º–æ–≥ –æ—Ç–≤–µ—Ç–∏—Ç—å (—Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞—à–ª–∞—Å—å –≤ OOXML/–∫–∞—Ä—Ç–∏–Ω–∫–µ),


    # –±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ —Ä–∏—Å—É–Ω–∫–∏
    if _is_pure_figure_request(q_text):
        verbosity = _detect_verbosity(q_text)
        logger.info(
            "ANSWER: pure figure request detected (uid=%s, doc_id=%s)",
            uid,
            doc_id,
        )
        handled = await _answer_figure_query(
            m,
            uid,
            doc_id,
            q_text,
            verbosity=verbosity,
        )
        logger.info(
            "ANSWER: _answer_figure_query finished (uid=%s, doc_id=%s, handled=%s)",
            uid,
            doc_id,
            handled,
        )
        if handled:
            return

    # –ï—Å–ª–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≥–ª–∞–≤—ã/—Ç–∞–±–ª–∏—Ü—ã ‚Äî –¥–∞—ë–º —ç—Ç–æ
    # –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –º—É–ª—å—Ç–∏–∏ÃÜ–Ω—Ç–µ–Ω—Ç–Ω–æ–º—É –ø–∞–π–ø–ª–∞–π–Ω—É –Ω–∏–∂–µ.
    if (
        _ALL_FIGS_HINT.search(q_text or "")
        and not _SECTION_NUM_RE.search(q_text or "")
        and not _TABLE_ANY.search(q_text or "")
    ):
        meta = _list_figures_db(uid, doc_id, limit=999999)
        total = int(meta["count"])
        if total == 0:
            await _send(m, "–í —Ä–∞–±–æ—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞.")
            return
        # –ø–∞—Ä—Ç–∏—è–º–∏ –ø–æ 8‚Äì12 –Ω–æ–º–µ—Ä–æ–≤
        nums = []
        for disp in meta["list"]:
            # –∏–∑ "–†–∏—Å—É–Ω–æ–∫ 2.1 ‚Äî ..." –≤—ã—Ç–∞—â–∏–º "2.1" (–µ—Å–ª–∏ –µ—Å—Ç—å)
            mnum = re.search(r"(?i)\b—Ä–∏—Å—É–Ω–æ–∫\s+([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*)\b", disp)
            if mnum:
                nums.append(mnum.group(1).replace(" ", "").replace(",", "."))
        batch = nums[:8] or nums[:12]
        # –∫–∞—Ä—Ç–æ—á–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—Å—Ç–∞; –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ —á–∞—Ç –±–æ–ª—å—à–µ –Ω–µ —à–ª—ë–º)
        cards = []
        try:
            cards = describe_figures_by_numbers(uid, doc_id, batch, sample_chunks=1, use_vision=False, lang="ru") or []
        except Exception:
            cards = []

        # –∑–∞—Ç–µ–º ‚Äî —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ –∫–∞–∂–¥–æ–º—É —Ä–∏—Å—É–Ω–∫—É: prefer vision_analyzer
        lines = []

        if va_analyze_figure and cards:
            for c in cards:
                disp = c.get("display") or f"–†–∏—Å—É–Ω–æ–∫ {c.get('num') or ''}".strip()
                imgs = c.get("images") or []
                hint = (c.get("highlights") or [None])[0]
                if not imgs:
                    continue
                try:
                    res = va_analyze_figure(imgs[0], caption_hint=hint, lang="ru")
                    if isinstance(res, dict):
                        text_block = (res.get("text") or "").strip() or _pairs_to_bullets(res.get("data") or [])
                    else:
                        text_block = (str(res) or "").strip()
                except Exception:
                    text_block = ""
                if not text_block:
                    # —Ñ–æ–ª–±—ç–∫ ‚Äî —Å—Ç–∞—Ä—ã–π summarizer –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º
                    text_block = ""
                if text_block:
                    # —ç—Ç–æ —Ç–µ–∫—Å—Ç –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (OCR/–æ–ø–∏—Å–∞–Ω–∏–µ)
                    lines.append(f"[Text on image] **{disp}**\n\n{text_block}")
                else:
                    # —Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º ‚Äî —è–≤–Ω–æ –≥–æ–≤–æ—Ä–∏–º, —á—Ç–æ —Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç
                    if FIG_STRICT:
                        lines.append(f"[No precise data] **{disp}**")

        suffix = (f"\n\n–ü–æ–∫–∞–∑–∞–Ω–∞ –ø–µ—Ä–≤–∞—è –ø–∞—Ä—Ç–∏—è –∏–∑ {len(batch)} / {total}." if total > len(batch) else "")
        if lines:
            await _send(m, "\n\n".join(lines) + suffix)
        else:
            # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–ª–±—ç–∫, –µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            await _send(m, "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø–∏—Å–∞—Ç—å —Ä–∏—Å—É–Ω–∫–∏." + suffix)
        return


    # NEW: –µ—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω —Ä–∞–∑–¥–µ–ª/–ø—É–Ω–∫—Ç ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –µ–≥–æ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π
    m_area = _SECTION_NUM_RE.search(q_text)
    if m_area:
        try:
            area = (m_area.group(1) or "").replace(" ", "").replace(",", ".")
            LAST_REF.setdefault(uid, {})["area"] = area
        except Exception:
            pass

        # --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç—ã –∑–∞—Ä–∞–Ω–µ–µ
    intents = detect_intents(q_text)

    # –ß–∏—Å—Ç—ã–π –∑–∞–ø—Ä–æ—Å –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∏—Å—É–Ω–∫–∏ (–Ω–µ—Ç —Å–µ–∫—Ü–∏–π/—Ç–∞–±–ª–∏—Ü/–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤/–æ–±—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞)
    pure_figs = intents["figures"]["want"] and not (
        intents["tables"]["want"] or intents["sources"]["want"] or
        intents.get("summary") or intents.get("general_question") or
        _SECTION_NUM_RE.search(q_text)
    )

    # NEW: —è–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ¬´–ø–æ –ø—É–Ω–∫—Ç—É/—Ä–∞–∑–¥–µ–ª—É/–≥–ª–∞–≤–µ X.Y¬ª (–Ω–æ —Ç–æ–ª—å–∫–æ –¥–ª—è –ß–ò–°–¢–´–• –∑–∞–ø—Ä–æ—Å–æ–≤)
    m_sec = _SECTION_NUM_RE.search(q_text)
    sec = None
    if m_sec:
        raw_sec = (m_sec.group(1) or "").strip()
        raw_sec = re.sub(r"^[A-Za-z–ê-–Ø–∞-—è]\s+(?=\d)", "", raw_sec)
        sec = raw_sec.replace(" ", "").replace(",", ".")

    # –°—Ç—Ä–æ–≥–∏–π —Å–µ–∫—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ —Å–º–µ—à–∞–Ω–Ω—ã–π
    if sec and _is_pure_section_request(q_text, intents):
        verbosity = _detect_verbosity(q_text)
        ctx = _section_context(uid, doc_id, sec, max_chars=9000)
        if ctx:
            base_sys = (
                "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –í–ö–†. –ù–∏–∂–µ ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç –¢–û–õ–¨–ö–û –ø–æ –æ–¥–Ω–æ–º—É –ø—É–Ω–∫—Ç—É/–≥–ª–∞–≤–µ –¥–∏–ø–ª–æ–º–∞.\n"
                "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É: –Ω–µ –¥–æ–±–∞–≤–ª—è–π –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ–≤, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã—Ö –ø–æ–ª–æ–∂–µ–Ω–∏–π "
                "–∏ –Ω–µ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–π —Ç–æ, —á–µ–≥–æ –≤ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ –Ω–µ—Ç. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ –Ω–∞–ø–∏—à–∏, "
                "—á—Ç–æ –¥–∞–Ω–Ω—ã—Ö –≤ —ç—Ç–æ–º –ø—É–Ω–∫—Ç–µ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."
            )
            if verbosity == "brief":
                sys_prompt = base_sys + " –ù—É–∂–Ω–∞ –ö–†–ê–¢–ö–ê–Ø –≤—ã–∂–∏–º–∫–∞."
                user_prompt = (
                    f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {q_text}\n\n"
                    f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –ø–æ –ø—É–Ω–∫—Ç—É {sec}. {_verbosity_addendum('brief')}"
                )
            elif verbosity == "detailed":
                sys_prompt = base_sys + " –ù—É–∂–µ–Ω –ü–û–î–†–û–ë–ù–´–ô —Ä–∞–∑–±–æ—Ä."
                user_prompt = (
                    f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {q_text}\n\n"
                    f"–°–¥–µ–ª–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ä–∞–∑–±–æ—Ä –ø–æ –ø—É–Ω–∫—Ç—É {sec}. {_verbosity_addendum('detailed')}"
                )
            else:
                sys_prompt = base_sys + " –û—Ç–≤–µ—Ç—å –ø–æ –¥–µ–ª—É, –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
                user_prompt = (
                    f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {q_text}\n\n"
                    f"–û—Ç–≤–µ—Ç—å –ø–æ –ø—É–Ω–∫—Ç—É {sec}. {_verbosity_addendum('normal')}"
                )

            messages = [
                {"role": "system", "content": sys_prompt},
                {"role": "assistant", "content": f"[–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –ø—É–Ω–∫—Ç—É {sec}]\n{ctx}"},
                {"role": "user", "content": user_prompt},
            ]

            if STREAM_ENABLED and chat_with_gpt_stream is not None:
                try:
                    stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                    await _stream_to_telegram(m, stream)
                    return
                except Exception as e:
                    logging.exception("section summary stream failed: %s", e)
            try:
                ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                if ans:
                    await _send(m, _strip_unwanted_sections(ans))
                    return
            except Exception as e:
                logging.exception("section summary non-stream failed: %s", e)
                # –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è ‚Äî –ø—É—Å—Ç—å –ø–æ–π–¥—ë—Ç –æ–±—ã—á–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –Ω–∏–∂–µ, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ —Å–ª–æ–º–∞–ª–æ—Å—å
        else:
            await _send(m, f"–ü—É–Ω–∫—Ç {sec} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∏–Ω–¥–µ–∫—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞.")
            return



    # ====== FULLREAD: auto ======
    fr_mode = getattr(Cfg, "FULLREAD_MODE", "off")
    # FULLREAD(auto) –≤–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é,
    # —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–±–∏–≤–∞—Ç—å —Å–ø–µ—Ü-–ª–æ–≥–∏–∫–∏ –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º/—Ä–∏—Å—É–Ω–∫–∞–º/–∏—Å—Ç–æ—á–Ω–∏–∫–∞–º.
    if (
        fr_mode == "auto"
        and intents.get("general_question")
        and not intents["tables"]["want"]
        and not intents["figures"]["want"]
        and not intents["sources"]["want"]
    ):
        logger.info(
            "ANSWER: FULLREAD(auto) mode, uid=%s, doc_id=%s",
            uid,
            doc_id,
        )
        _limit = int(getattr(Cfg, "DIRECT_MAX_CHARS", 80000))
        full_text = _full_document_text(uid, doc_id, limit_chars=_limit + 1)
        full_len = len(full_text or "")
        logger.debug(
            "ANSWER: FULLREAD(auto) full_text_len=%d (limit=%d)",
            full_len,
            _limit,
        )

        # 1) –≤–æ–æ–±—â–µ –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç ‚Üí —á–µ—Å—Ç–Ω–æ –ø–∞–¥–∞–µ–º –≤ –æ–±—ã—á–Ω—ã–π RAG-–ø–∞–π–ø–ª–∞–π–Ω –Ω–∏–∂–µ
        if not full_text.strip():
            logger.warning(
                "ANSWER: FULLREAD(auto) got empty full_text, falling back to RAG (uid=%s, doc_id=%s)",
                uid,
                doc_id,
            )
        # 2) –¥–æ–∫—É–º–µ–Ω—Ç —Ü–µ–ª–∏–∫–æ–º –≤–ª–µ–∑–∞–µ—Ç –≤ –ª–∏–º–∏—Ç ‚Üí –ø—Ä—è–º–æ–π FULLREAD
        elif full_len <= _limit:
            system_prompt = (
                "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –¥–∏–ø–ª–æ–º–Ω—ã–º —Ä–∞–±–æ—Ç–∞–º. –¢–µ–±–µ –¥–∞–Ω –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç –í–ö–†/–¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
                "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ–≤. –ù–µ –¥–æ–±–∞–≤–ª—è–π —Ä–∞–∑–¥–µ–ª–æ–≤ –≤–∏–¥–∞ "
                "¬´–ß–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç¬ª –∏ –Ω–µ –ø—Ä–æ—Å–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n"
                "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –ø–æ–¥–ø–∏—Å–∏ –∏ –±–ª–∏–∂–∞–π—à–∏–π —Ç–µ–∫—Å—Ç; –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞/–∑–Ω–∞—á–µ–Ω–∏—è. "
                "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∏ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç—ã, –≤—ã—Ä—É—á–∫–∞, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø.), "
                "–µ—Å–ª–∏ –æ–Ω–∏ –ø—Ä—è–º–æ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ.\n"
                "–ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞/—Ç–∞–±–ª–∏—Ü—ã –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ ‚Äî –æ—Ç–≤–µ—Ç—å: ¬´–¥–∞–Ω–Ω–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç –≤ —Ä–∞–±–æ—Ç–µ¬ª.\n"
                "–ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç –µ—Å—Ç—å, –Ω–æ –æ–Ω –≤ –ø–ª–æ—Ö–æ–º –∫–∞—á–µ—Å—Ç–≤–µ/–Ω–µ—á–∏—Ç–∞–µ–º ‚Äî –æ—Ç–≤–µ—Ç—å: ¬´–†–∏—Å—É–Ω–æ–∫ –ø–ª–æ—Ö–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–µ –º–æ–≥—É –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å¬ª, "
                "–∏ –¥–æ–±–∞–≤—å –∫—Ä–∞—Ç–∫—É—é –ø–æ–¥–ø–∏—Å—å/–∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞. –¶–∏—Ç–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–æ, –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."
            )

            verbosity = _detect_verbosity(q_text)
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "assistant", "content": f"[–î–æ–∫—É–º–µ–Ω—Ç ‚Äî –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç]\n{full_text}"},
                {"role": "user", "content": f"{q_text}\n\n{_verbosity_addendum(verbosity)}"},
            ]

            if STREAM_ENABLED and chat_with_gpt_stream is not None:
                try:
                    stream = chat_with_gpt_stream(
                        messages,
                        temperature=0.2,
                        max_tokens=FINAL_MAX_TOKENS,
                    )  # type: ignore
                    await _stream_to_telegram(m, stream)
                    return
                except Exception as e:
                    logging.exception("auto fullread stream failed: %s", e)

            try:
                ans = chat_with_gpt(
                    messages,
                    temperature=0.2,
                    max_tokens=FINAL_MAX_TOKENS,
                )
                if ans:
                    await _send(m, _strip_unwanted_sections(ans))
                    return
            except Exception as e:
                logging.exception("auto fullread non-stream failed: %s", e)
        # 3) –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª–∏–Ω–Ω—ã–π ‚Üí –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —á—Ç–µ–Ω–∏–µ (map‚Üíreduce)
        else:
            # –¥–æ–∫—É–º–µ–Ω—Ç –±–æ–ª—å—à–æ–π ‚Üí –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —á—Ç–µ–Ω–∏–µ (map‚Üíreduce)
            messages, err = _iterative_fullread_build_messages(uid, doc_id, q_text)
            if messages:
                if STREAM_ENABLED and chat_with_gpt_stream is not None:
                    try:
                        stream = chat_with_gpt_stream(
                            messages,
                            temperature=0.2,
                            max_tokens=FINAL_MAX_TOKENS,
                        )  # type: ignore
                        await _stream_to_telegram(m, stream)
                        return
                    except Exception as e:
                        logging.exception("auto iterative stream failed: %s", e)
                try:
                    ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                    if ans:
                        await _send(m, _strip_unwanted_sections(ans))
                        return
                except Exception as e:
                    logging.exception("auto iterative non-stream failed: %s", e)
            elif err:
                # –í auto-—Ä–µ–∂–∏–º–µ –Ω–µ —Ä–≤—ë–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω, –∞ —Ç–∏—Ö–æ –ª–æ–≥–∏—Ä—É–µ–º
                logging.warning(
                    "ANSWER: FULLREAD(auto) iterative build failed, "
                    "falling back to RAG (uid=%s, doc_id=%s): %s",
                    uid,
                    doc_id,
                    err,
                )
                # –±–µ–∑ return ‚Äî –Ω–∏–∂–µ —Å–ø–æ–∫–æ–π–Ω–æ –æ—Ç—Ä–∞–±–æ—Ç–∞–µ—Ç –æ–±—ã—á–Ω—ã–π RAG-–æ—Ç–≤–µ—Ç

    # ====== FULLREAD: iterative/digest ======
    if fr_mode in {"iterative", "digest"} and not pure_figs:
        messages, err = _iterative_fullread_build_messages(uid, doc_id, q_text)
        if messages:
            if STREAM_ENABLED and chat_with_gpt_stream is not None:
                try:
                    stream = chat_with_gpt_stream(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)  # type: ignore
                    await _stream_to_telegram(m, stream)
                    return
                except Exception as e:
                    logging.exception("iterative fullread stream failed: %s", e)
            try:
                ans = chat_with_gpt(messages, temperature=0.2, max_tokens=FINAL_MAX_TOKENS)
                if ans:
                    await _send(m, _strip_unwanted_sections(ans))
                    return
            except Exception as e:
                logging.exception("iterative fullread non-stream failed: %s", e)
        else:
            if err:
                await _send(m, err)
                return
        # –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ –≤—ã—à–ª–æ ‚Äî –ø—Ä–æ–≤–∞–ª–∏–≤–∞–µ–º—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º –Ω–∏–∂–µ

    # ====== –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º—É–ª—å—Ç–∏-–∏–Ω—Ç–µ–Ω—Ç –ø–∞–π–ø–ª–∞–π–Ω (RAG) ======
    await _ensure_modalities_indexed(m, uid, doc_id, intents)
    facts = _gather_facts(uid, doc_id, intents)
    logger.info(
        "ANSWER: RAG facts gathered (uid=%s, doc_id=%s, keys=%s)",
        uid,
        doc_id,
        list(facts.keys()) if isinstance(facts, dict) else type(facts),
    )

    # NEW: –µ—Å–ª–∏ –ø–æ –æ–±—â–µ–º—É –≤–æ–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å –∏–º–µ–Ω–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ —Ä–∞–±–æ—Ç—ã ‚Äî
    # —Å–Ω–∞—á–∞–ª–∞ —Å–ø—Ä–∞—à–∏–≤–∞–µ–º, –º–æ–∂–Ω–æ –ª–∏ –æ—Ç–≤–µ—Ç–∏—Ç—å –≤ –æ–±—â–µ–º –≤–∏–¥–µ –∫–∞–∫ [–º–æ–¥–µ–ª—å].
    if intents.get("general_question") and not facts.get("general_ctx") and not facts.get("summary_text"):
        MODEL_EXTRA_PENDING[uid] = {
            "kind": "generic",
            "question": intents["general_question"] or q_text,
        }
        await _send(
            m,
            "–ü–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É —è –Ω–µ –Ω–∞—à—ë–ª —è–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–∞–º–æ–º —Ç–µ–∫—Å—Ç–µ —Ä–∞–±–æ—Ç—ã. "
            "–ú–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –≤ –æ–±—â–µ–º –≤–∏–¥–µ –∫–∞–∫ [–º–æ–¥–µ–ª—å] (—ç—Ç–æ —É–∂–µ –Ω–µ –±—É–¥–µ—Ç –æ–ø–∏—Ä–∞—Ç—å—Å—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç). "
            "–ù–∞–ø–∏—à–∏ ¬´–¥–∞¬ª –∏–ª–∏ ¬´–Ω–µ—Ç¬ª."
        )
        return

    # ‚Üì –ù–û–í–û–ï: –µ—Å–ª–∏ –µ—Å—Ç—å –ø–ª–∞–Ω –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ ‚Äî –≤–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—É—é –ø–æ–¥–∞—á—É
        # ‚Üì –ù–û–í–û–ï: –µ—Å–ª–∏ –µ—Å—Ç—å –ø–ª–∞–Ω –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ ‚Äî –≤–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—É—é –ø–æ–¥–∞—á—É,
    # –Ω–æ —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –æ–ø—Ä–∞–≤–¥–∞–Ω–æ (–µ—Å—Ç—å –ø–æ–¥–ø—É–Ω–∫—Ç—ã –∏ –≤–æ–ø—Ä–æ—Å –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π).
    discovered_items: list[dict] | None = None
    if isinstance(facts, dict):
        discovered_items = (
            (facts.get("coverage") or {}).get("items")
            or facts.get("general_subitems")
        )

    if _should_use_multistep(q_text, discovered_items):
        try:
            handled = await _run_multistep_answer(
                m,
                uid,
                doc_id,
                q_text,
                discovered_items=discovered_items,  # –æ—Ç–ø—Ä–∞–≤–∏—Ç A‚ÜíB‚Üí‚Ä¶ –∏ –≤–µ—Ä–Ω—ë—Ç True
            )
            if handled:
                return
        except Exception as e:
            logging.exception("multistep pipeline failed, fallback to normal: %s", e)
    # –µ—Å–ª–∏ –º—É–ª—å—Ç–∏—à–∞–≥ –Ω–µ –ø–æ–¥–æ—à—ë–ª ‚Äî –Ω–∏–∂–µ –∏–¥—ë–º –ø–æ –æ–±—ã—á–Ω–æ–º—É –ø–∞–π–ø–ª–∞–π–Ω—É



        # –æ–±—ã—á–Ω—ã–π –ø—É—Ç—å + —è–≤–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –≤–µ—Ä–±–æ–∑–Ω–æ—Å—Ç–∏
    verbosity = _detect_verbosity(q_text)
    SAFE_RULES = (
        "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–º —Ñ–∞–∫—Ç–∞–º –∏ —Ü–∏—Ç–∞—Ç–∞–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. "
        "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏, –±–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞/–∑–Ω–∞—á–µ–Ω–∏—è "
        "–∏ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∏ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç—ã, –≤—ã—Ä—É—á–∫–∞, "
        "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø.), –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ."
    )

    enriched_q = f"{SAFE_RULES}\n\n{q_text}\n\n{_verbosity_addendum(verbosity)}"

    # –µ—Å–ª–∏ —Ö–æ—á–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª—è—Ç—å ¬´–ø–æ—Å–ª–µ–¥–Ω–∏–π —É–ø–æ–º—è–Ω—É—Ç—ã–π —Ä–∏—Å—É–Ω–æ–∫¬ª ‚Äî –≤–æ–∑—å–º–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞
    figs_in_q = [_num_norm_fig(n) for n in _extract_fig_nums(q_text)]
    if figs_in_q:
        LAST_REF.setdefault(uid, {})["figure_nums"] = figs_in_q

        # NEW: –ø—Ä—è–º–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    # (–Ω–µ –ª–æ–º–∞–µ—Ç —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É: –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å/–Ω–µ—Ç –∫–∞—Ä—Ç–∏–Ω–æ–∫ ‚Äî –∏–¥—ë–º –≤ generate_answer)
    try:
        if intents.get("general_question") and getattr(Cfg, "vision_active", lambda: False)():
            # –ø–æ–¥—Ç—è–Ω–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫-—Ö–∏—Ç—ã –∏ –≤—ã–±–µ—Ä–µ–º 1‚Äì3 —Ñ–∞–π–ª–∞-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            hits_v = retrieve(uid, doc_id, intents["general_question"], top_k=10) or []
            img_paths = _pick_images_from_hits(hits_v, limit=getattr(Cfg, "VISION_MAX_IMAGES_PER_REQUEST", 3))
            if img_paths and (chat_with_gpt_stream_multimodal or chat_with_gpt_multimodal):
                # –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                ctx = (facts.get("general_ctx") or "").strip() if isinstance(facts, dict) else ""
                mm_system = (
                    "–¢—ã —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ –í–ö–†. –£ —Ç–µ–±—è –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å, –∫—Ä–∞—Ç–∫–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è "
                    "(—Ñ–æ—Ç–æ/—Å–∫–∞–Ω—ã/–¥–∏–∞–≥—Ä–∞–º–º—ã) –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –û—Ç–≤–µ—á–∞–π –ø–æ –¥–µ–ª—É, –∏—Å–ø–æ–ª—å–∑—É—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é. "
                    "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∑–Ω–∞—á–µ–Ω–∏—è –∏ –Ω–æ–º–µ—Ä–∞, –ø–∏—à–∏ —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –≤–∏–¥–Ω–æ –∏–ª–∏ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ. "
                    "–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∏ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–¥–∞–∂–∏, –∫–ª–∏–µ–Ω—Ç—ã, –≤—ã—Ä—É—á–∫–∞, "
                    "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Ç.–ø.), –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ –∏–ª–∏ –ø–æ–¥–ø–∏—Å—è—Ö –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º."
                )

                mm_prompt = (f"{q_text}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{ctx}" if ctx else q_text)

                if STREAM_ENABLED and chat_with_gpt_stream_multimodal is not None:
                    stream = chat_with_gpt_stream_multimodal(
                        mm_prompt,
                        image_paths=img_paths,
                        system=mm_system,
                        temperature=0.2,
                        max_tokens=FINAL_MAX_TOKENS,
                    )
                    await _stream_to_telegram(m, stream)
                    return
                elif chat_with_gpt_multimodal is not None:
                    ans_mm = chat_with_gpt_multimodal(
                        mm_prompt,
                        image_paths=img_paths,
                        system=mm_system,
                        temperature=0.2,
                        max_tokens=FINAL_MAX_TOKENS,
                    )
                    ans_mm = (ans_mm or "").strip()
                    if ans_mm:
                        await _send(m, _strip_unwanted_sections(ans_mm))
                        return
    except Exception as e:
        logging.exception("multimodal answer path failed, falling back: %s", e)

    # --- —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å RAG ‚Üí –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –ø–æ —Ñ–∞–∫—Ç–∞–º (answer_builder) ---

    # 1) –ø—Ä–æ–±—É–µ–º —Å—Ç—Ä–∏–º–æ–≤—É—é –≤–µ—Ä—Å–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
    if STREAM_ENABLED and generate_answer_stream is not None:
        try:
            stream = generate_answer_stream(
                enriched_q,
                facts,
                language=intents.get("language", "ru"),
            )
            await _stream_to_telegram(m, stream)
            return
        except Exception:
            logging.exception("generate_answer_stream failed, fallback to sync")

    # 2) –Ω–µ—Å—Ç—Ä–∏–º–æ–≤—ã–π —Ñ–æ–ª–±—ç–∫
    try:
        answer = generate_answer(
            enriched_q,
            facts,
            language=intents.get("language", "ru"),
        )
    except Exception as e:
        logging.exception("generate_answer failed: %s", e)
        answer = ""

    # 3) —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ –æ—Ç –ø—É—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: –≤—Å–µ–≥–¥–∞ —á—Ç–æ-—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º
    answer = (answer or "").strip()
    if not answer:
        answer = (
            "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ä–∞–±–æ—Ç—ã. "
            "–ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç—å, –∫–∞–∫–æ–π —Ä–∞–∑–¥–µ–ª, —Ç–∞–±–ª–∏—Ü—É –∏–ª–∏ —Ä–∏—Å—É–Ω–æ–∫ —Ç–µ–±—è –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç."
        )

    await _send(m, _strip_unwanted_sections(answer))


# ------------------------------ —ç–º–±–µ–¥–¥–∏–Ω–≥-–ø—Ä–æ—Ñ–∏–ª—å ------------------------------

def _current_embedding_profile() -> str:
    dim = probe_embedding_dim(None)
    if dim:
        return f"emb={Cfg.POLZA_EMB}|dim={dim}"
    return f"emb={Cfg.POLZA_EMB}"

def _needs_reindex_by_embeddings(con, doc_id: int) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ—Ä–∞ –ª–∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑-–∑–∞ —Å–º–µ–Ω—ã embedding-–º–æ–¥–µ–ª–∏
    –∏–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.

    –í layout_profile —Ö—Ä–∞–Ω–∏–º —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞:
      "emb=polza-emb-v1|dim=768"
    """
    if not _table_has_columns(con, "documents", ["layout_profile"]):
        # —Å—Ç–∞—Ä—ã–µ –±–∞–∑—ã –±–µ–∑ layout_profile ‚Äî –ª—É—á—à–µ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
        return True

    cur = con.cursor()
    cur.execute("SELECT layout_profile FROM documents WHERE id=?", (doc_id,))
    row = cur.fetchone()
    stored = (row["layout_profile"] or "") if row else ""
    if not stored:
        # –ø—Ä–æ—Ñ–∏–ª—è –Ω–µ—Ç ‚Äî —Ç–æ–∂–µ –ø–æ–≤–æ–¥ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
        return True

    cur_model = Cfg.POLZA_EMB.strip().lower()
    stored_model = ""
    stored_dim: int | None = None

    for part in stored.split("|"):
        part = (part or "").strip().lower()
        if part.startswith("emb="):
            # "emb=polza-emb-v1" ‚Üí "polza-emb-v1"
            stored_model = part[4:]
        elif part.startswith("dim="):
            # "dim=768" ‚Üí 768
            try:
                stored_dim = int(part[4:])
            except ValueError:
                stored_dim = None

    # –µ—Å–ª–∏ embedding-–º–æ–¥–µ–ª—å –ø–æ–º–µ–Ω—è–ª–∞—Å—å ‚Äî —Ç–æ—á–Ω–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
    if stored_model and stored_model != cur_model:
        return True

    # —Å–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω–∞
    try:
        cur_dim = probe_embedding_dim(None)
    except Exception:
        cur_dim = None

    if cur_dim and stored_dim and cur_dim != stored_dim:
        return True

    # –≤—Å—ë —Å–æ–≤–ø–∞–ª–æ ‚Äî –º–æ–∂–Ω–æ –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
    return False


# ------------------------------ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ------------------------------

@dp.message(F.text & ~F.via_bot & ~F.text.startswith("/"))
async def qa(m: types.Message):
    uid = ensure_user(str(m.from_user.id))
    doc_id = ACTIVE_DOC.get(uid)

    if not doc_id:
        persisted = get_user_active_doc(uid)
        if persisted:
            ACTIVE_DOC[uid] = persisted
            doc_id = persisted

    text = (m.text or "").strip()

    # NEW: –µ—Å–ª–∏ –∂–¥—ë–º –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ—Ç–≤–µ—Ç–∞ ¬´–¥–∞/–Ω–µ—Ç¬ª –ø—Ä–æ [–º–æ–¥–µ–ª—å] ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ
    pending = MODEL_EXTRA_PENDING.get(uid)
    if pending:
        low = text.lower()
        if low in ("–¥–∞", "–¥", "–∞–≥–∞", "–æ–∫", "—Ö–æ—Ä–æ—à–æ", "yes", "y"):
            info = MODEL_EXTRA_PENDING.pop(uid, None) or {}
            kind = (info.get("kind") or "generic").lower()

            if kind == "table_more":
                # doc_id –º–æ–≥–ª–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –º–æ–º–µ–Ω—Ç –≤–æ–ø—Ä–æ—Å–∞ ¬´–æ–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ¬ª
                doc_id_for_pending = info.get("doc_id") or doc_id

                # –µ—Å–ª–∏ –ø–æ—á–µ–º—É-—Ç–æ doc_id –Ω–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å ‚Äî –ø–∞–¥–∞–µ–º –≤ –æ–±—â–∏–π [–º–æ–¥–µ–ª—å]
                if not doc_id_for_pending:
                    await _answer_with_model_extra(
                        m,
                        uid,
                        info.get("question") or "",
                    )
                    return

                await _answer_with_model_extra_table(
                    m,
                    uid,
                    doc_id_for_pending,
                    info.get("question") or "",
                    info.get("ctx_tables") or "",
                    info.get("nums") or [],
                )
            else:
                await _answer_with_model_extra(
                    m,
                    uid,
                    info.get("question") or "",
                )
            return
        # –ª—é–±–∞—è –¥—Ä—É–≥–∞—è —Ä–µ–ø–ª–∏–∫–∞ —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç –æ–∂–∏–¥–∞–Ω–∏–µ –∏ –∏–¥—ë—Ç –ø–æ –æ–±—ã—á–Ω–æ–º—É –ø—É—Ç–∏
        MODEL_EXTRA_PENDING.pop(uid, None)

    # üëã –†–ê–ù–ù–ò–ô –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –±–µ–∑ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
    if _is_greeting(text):
        greet = getattr(
            Cfg, "MSG_GREET",
            "–ü—Ä–∏–≤–µ—Ç! –Ø —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ —Ç–≤–æ–µ–π –í–ö–†. –ü—Ä–∏—à–ª–∏ —Ñ–∞–π–ª –í–ö–† (.doc/.docx) ‚Äî –∏ —è –ø–æ–º–æ–≥—É –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é."
        )
        await _send(m, greet)
        return

    if not doc_id:
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–ø—Ä–æ—Å, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –ø–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞
        if text:
            enqueue_pending_query(uid, text, meta={"source": "chat", "reason": "no_active_doc"})
        await _send(m, Cfg.MSG_NEED_FILE_QUEUED)
        return

    await respond_with_answer(m, uid, doc_id, text)
