# app/retrieval.py
import re
import json
import numpy as np
from typing import Optional, List, Dict, Tuple, Any
from .db import get_conn, get_figures_for_doc
from .polza_client import embeddings, vision_describe  # —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ + vision
import logging
# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ö–µ–ª–ø–µ—Ä—ã –ø–æ –Ω–æ–º–µ—Ä–∞–º —Ç–∞–±–ª–∏—Ü/—Ä–∏—Å—É–Ω–∫–æ–≤ –∏ ¬´–≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª
try:
    from .intents import extract_table_numbers, extract_figure_numbers, TABLE_ALL_VALUES_RE  # type: ignore
except Exception:
    extract_table_numbers = None      # type: ignore
    extract_figure_numbers = None     # type: ignore
    TABLE_ALL_VALUES_RE = re.compile(  # type: ignore
        r"(?i)\b(–≤—Å–µ|–≤—Å—é|—Ü–µ–ª–∏–∫–æ–º|–ø–æ–ª–Ω–æ—Å—Ç—å—é|–ø–æ–ª–Ω–∞—è)\b.*\b(—Ç–∞–±–ª–∏—Ü\w*|—Ç–∞–±–ª–∏—Ü–∞|–∑–Ω–∞—á–µ–Ω–∏\w*|–¥–∞–Ω–Ω\w*|—Å—Ç—Ä–æ–∫\w*|–∫–æ–ª–æ–Ω\w*)\b"
        r"|(?:\ball\b.*\b(table|values|rows|columns)\b|\bfull\s+(table|values)\b|\bentire\s+table\b)"
    )

logger = logging.getLogger(__name__)
# –ö—ç—à –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å: (owner_id, doc_id) -> {"mat": np.ndarray [N,D], "meta": list[dict]}
_DOC_CACHE: dict[tuple[int, int], dict] = {}

# ---------------------------
# –£—Ç–∏–ª–∏—Ç—ã —Å—Ö–µ–º—ã
# ---------------------------

def _table_has_columns(con, table: str, cols: List[str]) -> bool:
    cur = con.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    have = {row[1] for row in cur.fetchall()}
    return all(c in have for c in cols)

# ---------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
# ---------------------------

def _load_doc(owner_id: int, doc_id: int) -> dict:
    """
    –ì—Ä—É–∑–∏—Ç –∏–∑ SQLite –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ. –†–µ–∑—É–ª—å—Ç–∞—Ç –∫—ç—à–∏—Ä—É–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞.
    """
    key = (owner_id, doc_id)
    if key in _DOC_CACHE:
        return _DOC_CACHE[key]

    con = get_conn()
    cur = con.cursor()

    has_ext = _table_has_columns(con, "chunks", ["element_type", "attrs"])

    if has_ext:
        cur.execute(
            "SELECT id, page, section_path, text, element_type, attrs, embedding "
            "FROM chunks WHERE owner_id=? AND doc_id=? "
            "ORDER BY id ASC",
            (owner_id, doc_id),
        )
    else:
        cur.execute(
            "SELECT id, page, section_path, text, embedding "
            "FROM chunks WHERE owner_id=? AND doc_id=? "
            "ORDER BY id ASC",
            (owner_id, doc_id),
        )

    rows = cur.fetchall()
    con.close()

    if not rows:
        pack = {"mat": np.zeros((0, 1), np.float32), "meta": []}
        _DOC_CACHE[key] = pack
        return pack

    def _parse_attrs(x):
        # attrs –º–æ–∂–µ—Ç –±—ã—Ç—å: None | dict | str(json) | bytes
        if x is None:
            return {}
        if isinstance(x, dict):
            return x
        if isinstance(x, (bytes, bytearray)):
            try:
                x = x.decode("utf-8", errors="ignore")
            except Exception:
                return {}
        if isinstance(x, str) and x.strip():
            try:
                v = json.loads(x)
                return v if isinstance(v, dict) else {}
            except Exception:
                return {}
        return {}

    meta: List[Dict[str, Any]] = []
    vecs: List[np.ndarray] = []
    dim: Optional[int] = None

    for r in rows:
        emb = r["embedding"]
        if emb is None:
            continue

        v = np.frombuffer(emb, dtype=np.float32)

        if dim is None:
            dim = v.size
        elif v.size != dim:
            continue

        if has_ext:
            et = (r["element_type"] or "").lower()
            attrs = _parse_attrs(r["attrs"])
        else:
            et = ""
            attrs = {}

        meta.append(
            {
                "id": r["id"],
                "page": r["page"],
                "section_path": r["section_path"] or "",
                "text": r["text"] or "",
                "element_type": et,
                "attrs": attrs,
            }
        )
        vecs.append(v)

    if not vecs:
        pack = {"mat": np.zeros((0, 1), np.float32), "meta": []}
        _DOC_CACHE[key] = pack
        return pack

    mat = np.vstack(vecs).astype(np.float32, copy=False)
    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-9
    mat = (mat / norms).astype(np.float32, copy=False)

    pack = {"mat": mat, "meta": meta}
    _DOC_CACHE[key] = pack
    return pack

# ---------------------------
# –í—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ —Å–∏–≥–Ω–∞–ª—ã/–∫–ª–∞—Å—Å—ã
# ---------------------------

_NUM_RE = re.compile(r"\b\d[\d\s.,]*%?\b")

_CLEAN_MARKERS_RE = re.compile(
    r"^\s*\[(?:—Ç–∞–±–ª–∏—Ü–∞|—Ä–∏—Å—É–Ω–æ–∫|–∑–∞–≥–æ–ª–æ–≤–æ–∫|—Å—Ç—Ä–∞–Ω–∏—Ü–∞)\]\s*|"
    r"\s*\[\s*row\s*\d+\s*\]\s*|"
    r"\s*\(\d+\s*[√óx]\s*\d+\)\s*$",
    re.IGNORECASE
)

def _clean_for_ctx(s: str) -> str:
    if not s:
        return ""
    t = _CLEAN_MARKERS_RE.sub("", s)
    t = t.replace("\u00A0", " ")                # NBSP -> –ø—Ä–æ–±–µ–ª
    t = t.replace("‚Äì", "-").replace("‚Äî", "-")   # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏—Ä–µ
    t = re.sub(r"\s+\|\s+", " ‚Äî ", t)           # "a | b | c" -> "a ‚Äî b ‚Äî c"
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _extract_numbers(s: str) -> set[str]:
    return set(_NUM_RE.findall(s or ""))

def _classify_by_prefix(text: str) -> str:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º –∏–∑ indexing.py, –µ—Å–ª–∏ –Ω–µ—Ç element_type –≤ –ë–î."""
    t = (text or "").lower()
    if t.startswith("[—Ç–∞–±–ª–∏—Ü–∞]"):
        return "table"
    if t.startswith("[—Ä–∏—Å—É–Ω–æ–∫]"):
        return "figure"
    if t.startswith("[–∑–∞–≥–æ–ª–æ–≤–æ–∫]"):
        return "heading"
    if t.startswith("[—Å—Ç—Ä–∞–Ω–∏—Ü–∞]"):
        return "page"
    return "text"

def _chunk_type(meta: Dict) -> str:
    """
    –ß–∏—Ç–∞–µ–º element_type –∏–∑ –ë–î. –ï—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî:
    1) –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ä–∞–∑–¥–µ–ª–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º 'reference' (–ò—Å—Ç–æ—á–Ω–∏–∫–∏/–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã/–ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è/References);
    2) –∏–Ω–∞—á–µ –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É —Ç–µ–∫—Å—Ç–∞.
    """
    et = (meta.get("element_type") or "").lower()
    if et:
        return et
    sec = (meta.get("section_path") or "").lower()
    if ("–∏—Å—Ç–æ—á–Ω–∏–∫" in sec) or ("–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä" in sec) or ("–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ" in sec) or ("reference" in sec):
        return "reference"
    return _classify_by_prefix(meta.get("text") or "")

def _query_signals(q: str) -> Dict[str, bool]:
    ql = (q or "").lower()
    return {
        "ask_table":   bool(re.search(r"\b(—Ç–∞–±–ª|—Ç–∞–±–ª–∏—Ü–∞|table)\b", ql)),
        "ask_figure":  bool(
            re.search(r"\b(—Ä–∏—Å\.?|—Ä–∏—Å—É–Ω–∫\w*|figure|fig\.?|–¥–∏–∞–≥—Ä–∞–º\w*|diagram)\b", ql)
            or re.search(r"\b–Ω–∞\s+—Ä–∏—Å—É–Ω–∫\w*\b", ql)
        ),
        "ask_heading": bool(re.search(r"\b(–≥–ª–∞–≤–∞|—Ä–∞–∑–¥–µ–ª|–≤–≤–µ–¥–µ–Ω–∏–µ|–∑–∞–∫–ª—é—á–µ–Ω–∏–µ|heading|chapter|section)\b", ql)),
        "ask_sources": bool(re.search(r"\b–∏—Å—Ç–æ—á–Ω–∏–∫(?:–∏|–æ–≤)?\b|\b—Å–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã\b|\b–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ", ql) or
                            re.search(r"\breferences?\b|bibliograph\w*", ql)),
    }

# ---------------------------
# –ë–∞–∑–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
# ---------------------------

def _embed_query(query: str) -> Optional[np.ndarray]:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å–∞.
    –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ –≤–µ—Ä–Ω—ë–º None.
    """
    q_str = (query or "").strip()
    if not q_str:
        return None
    try:
        vec = embeddings([q_str])[0]
        q = np.asarray(vec, dtype=np.float32)
        norm = float(np.linalg.norm(q))
        if not np.isfinite(norm) or norm == 0.0:
            return None
        q /= norm
        return q.astype(np.float32, copy=False)
    except Exception:
        return None

# ---------------------------
# –†–µ–≥–µ–∫—Å—ã-–ø–æ–º–æ—â–Ω–∏–∫–∏ –¥–ª—è ID-aware
# ---------------------------

def _mk_table_pattern(q: str) -> Optional[str]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –≤—ã—Ç–∞—â–∏—Ç—å –Ω–æ–º–µ—Ä —Ç–∞–±–ª–∏—Ü—ã –∏–∑ –≤–æ–ø—Ä–æ—Å–∞.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ñ–æ—Ä–º—ã:
      - '—Ç–∞–±–ª–∏—Ü–∞ 3.1', '—Ç–∞–±–ª. 3,1'
      - 'table 2.4'
      - –¥–æ–ø—É—Å–∫–∞–µ–º '‚Ññ'
      - –¥–æ–ø—É—Å–∫–∞–µ–º –±—É–∫–≤–µ–Ω–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å '–ê.1' / '–ü1.2'
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω regex –∏–ª–∏ None.
    """
    ql = (q or "")
    m = re.search(r"(—Ç–∞–±–ª(?:–∏—Ü–∞)?|table)\s*(?:‚Ññ\s*|no\.?\s*|–Ω–æ–º–µ—Ä\s*)?([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*)", ql, re.IGNORECASE)
    if not m:
        return None
    raw = m.group(2).strip()
    raw = re.sub(r"\s+", " ", raw).replace(" ", "")
    # –ë—É–∫–≤–µ–Ω–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å?
    if re.match(r"^[A-Za-z–ê-–Ø–∞-—è]\d", raw):
        letter = raw[0]
        rest = raw[1:]
        # —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º, –∞ –∑–∞—Ç–µ–º –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º '\.' –≤ '[.,]'
        rest_esc = re.escape(rest)
        rest_pat = re.sub(r"\\\.", "[.,]", rest_esc)
        pat_num = rf"{re.escape(letter)}\.?\s*{rest_pat}"
    else:
        pat_num = re.sub(r"\\\.", "[.,]", re.escape(raw))
    return rf"(?:—Ç–∞–±–ª(?:–∏—Ü–∞)?|table)\s*\.?\s*(?:‚Ññ\s*|no\.?\s*|–Ω–æ–º–µ—Ä\s*)?{pat_num}"

def _mk_figure_pattern(q: str) -> Optional[str]:
    """
    –ü–æ–∏—Å–∫ '—Ä–∏—Å—É–Ω–æ–∫ 2.4' / '—Ä–∏—Å. 2,4' / 'figure 3' / 'fig. 1' / '—Ä–∏—Å. –ê.1'.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω regex –∏–ª–∏ None.
    """
    ql = (q or "")
    m = re.search(r"(—Ä–∏—Å(?:—É–Ω–æ–∫)?|fig(?:ure)?\.?)\s*(?:‚Ññ\s*|no\.?\s*|–Ω–æ–º–µ—Ä\s*)?([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*)", ql, re.IGNORECASE)
    if not m:
        return None
    raw = m.group(2).strip()
    raw = re.sub(r"\s+", " ", raw).replace(" ", "")
    if re.match(r"^[A-Za-z–ê-–Ø–∞-—è]\d", raw):
        letter = raw[0]
        rest = raw[1:]
        rest_esc = re.escape(rest)
        rest_pat = re.sub(r"\\\.", "[.,]", rest_esc)
        pat_num = rf"{re.escape(letter)}\.?\s*{rest_pat}"
    else:
        pat_num = re.sub(r"\\\.", "[.,]", re.escape(raw))
    return rf"(?:—Ä–∏—Å(?:—É–Ω–æ–∫)?|fig(?:ure)?\.?)\s*\.?\s*(?:‚Ññ\s*|no\.?\s*|–Ω–æ–º–µ—Ä\s*)?{pat_num}"

# ---------------------------
# –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å–∫–æ—Ä–µ—Ä/—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (ID-aware + –æ–±–ª–∞—Å—Ç—å)
# ---------------------------

def _score_and_rank(
    pack: dict,
    query: str,
    *,
    prelim_k: int = 48,
    section_prefix: Optional[str] = None,
    element_types: Optional[List[str]] = None,
) -> List[Tuple[int, float]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —á–∞–Ω–∫–æ–≤ –∏ –∏—Ö —Å–∫–æ—Ä–∏–Ω–≥–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é.
    –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤ retrieve() –∏ coverage-–≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö.
    –ú–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ–±–ª–∞—Å—Ç—å—é –¥–æ–∫—É–º–µ–Ω—Ç–∞ (section_prefix) –∏/–∏–ª–∏ —Ç–∏–ø–∞–º–∏ —á–∞–Ω–∫–æ–≤.
    """
    if pack["mat"].shape[0] == 0:
        return []

    q = _embed_query(query)
    if q is None:
        return []

    sims = pack["mat"] @ q  # –∫–æ—Å–∏–Ω—É—Å
    N = sims.shape[0]
    if N == 0:
        return []

    # --- –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –ø–æ –æ–±–ª–∞—Å—Ç–∏/—Ç–∏–ø–∞–º ---
    cand_idx = list(range(N))
    if section_prefix or element_types:
        spfx = section_prefix or ""
        want_types = {t.lower() for t in (element_types or [])}
        cand_idx = []
        for i, m in enumerate(pack["meta"]):
            ok = True
            if spfx and not str(m.get("section_path") or "").startswith(spfx):
                ok = False
            if ok and want_types and (_chunk_type(m) not in want_types):
                ok = False
            if ok:
                cand_idx.append(i)
        if not cand_idx:
            return []

    cand_idx = np.asarray(cand_idx, dtype=np.int64)
    sims_sub = sims[cand_idx]

    prelim_k = max(min(prelim_k, sims_sub.shape[0]), 1)
    part_idx_local = np.argpartition(sims_sub, -prelim_k)[-prelim_k:]
    order_local = part_idx_local[np.argsort(-sims_sub[part_idx_local])]
    idx = cand_idx[order_local]

    sig = _query_signals(query)
    q_nums = _extract_numbers(query)
    tab_pat = _mk_table_pattern(query)
    fig_pat = _mk_figure_pattern(query)
    tab_rgx = re.compile(tab_pat, re.IGNORECASE) if tab_pat else None
    fig_rgx = re.compile(fig_pat, re.IGNORECASE) if fig_pat else None

    rescored: List[Tuple[int, float]] = []
    for i in idx:
        m = pack["meta"][int(i)]
        text = (m["text"] or "")
        score = float(sims[int(i)])

        ctype = _chunk_type(m)

        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –±—É—Å—Ç—ã –ø–æ —Ç–∏–ø—É —á–∞–Ω–∫–∞
        if sig["ask_table"] and ctype in {"table", "table_row"}:
            score += 0.12
        if sig["ask_figure"] and ctype == "figure":
            score += 0.14
        if sig["ask_heading"] and ctype == "heading":
            score += 0.06
        if sig["ask_sources"] and ctype == "reference":
            score += 0.35
        if (not sig["ask_sources"]) and ctype == "reference":
            score -= 0.12

        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∞
        if tab_rgx and tab_rgx.search(text):
            score += 0.24
        if fig_rgx and fig_rgx.search(text):
            score += 0.26

        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —á–∏—Å–µ–ª (–¥–æ +0.12 —Å—É–º–º–∞—Ä–Ω–æ)
        if q_nums:
            c_nums = _extract_numbers(text)
            inter = len(q_nums & c_nums)
            if inter:
                score += min(0.025 * inter, 0.12)

        # –õ—ë–≥–∫–∏–π —à—Ç—Ä–∞—Ñ –¥–ª—è ¬´—Å—Ç—Ä–∞–Ω–∏—Ü¬ª, –µ—Å–ª–∏ –Ω–µ —Å–ø—Ä–∞—à–∏–≤–∞–ª–∏ –ø—Ä–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        if ctype == "page" and not (sig["ask_table"] or sig["ask_figure"] or sig["ask_heading"] or sig["ask_sources"]):
            score -= 0.03

        # –ü—É—Å—Ç—ã–µ/–æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –¢–ï–ö–°–¢–û–í–´–ï —á–∞–Ω–∫–∏ ‚Äî –º—è–≥–∫–∏–π —à—Ç—Ä–∞—Ñ
        # (—Ç–∞–±–ª–∏—á–Ω—ã–µ/—Ä–∏—Å—É–Ω–∫–∏ –Ω–µ —Ä–µ–∂–µ–º, —É –Ω–∏—Ö —Å—Ç—Ä–æ–∫–∏ —á–∞—Å—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ –ø—Ä–∏—Ä–æ–¥–µ)
        if ctype in {"text", "heading", "page"} and len((text or "").strip()) < 30:
            score -= 0.04

        rescored.append((int(i), score))

    rescored.sort(key=lambda x: -x[1])
    return rescored

# ---------------------------
# –û—Å–Ω–æ–≤–Ω–æ–π RAG-–ø–æ–∏—Å–∫ (ID-aware + –æ–±–ª–∞—Å—Ç—å)
# ---------------------------

def retrieve(
    owner_id: int,
    doc_id: int,
    query: str,
    top_k: int = 8,
    *,
    section_prefix: Optional[str] = None,
    element_types: Optional[List[str]] = None,
    role: Optional[str] = None,   # ‚úÖ NEW
) -> List[Dict]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç top-k —á–∞–Ω–∫–æ–≤ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É —Å –≤–æ–ø—Ä–æ—Å–æ–º (—Å –ª—ë–≥–∫–∏–º –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º).
    –ú–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å –æ–±–ª–∞—Å—Ç—å—é (section_prefix) –∏/–∏–ª–∏ —Ç–∏–ø–∞–º–∏ —á–∞–Ω–∫–æ–≤ (element_types),
    –∞ —Ç–∞–∫–∂–µ —Ä–æ–ª—å—é —Å–µ–∫—Ü–∏–∏ (attrs.role), –µ—Å–ª–∏ —Ä–æ–ª–∏ –ø—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–∞ —ç—Ç–∞–ø–µ ingest/index.

    –ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç: {id, page, section_path, text, score}.

    NEW:
      - –ø–µ—Ä–µ–¥ –æ–±—ã—á–Ω—ã–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º –ø—Ä–æ–±—É–µ–º –ø–æ–¥—Ç—è–Ω—É—Ç—å ¬´—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ¬ª –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        (—Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏) —á–µ—Ä–µ–∑ _inject_special_sources_for_item();
      - –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º –ø–æ id (int/str) –±–µ–∑–æ–ø–∞—Å–Ω–æ.
    """

    def _norm_id(x: Any) -> Optional[Any]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º id –¥–ª—è used_ids: int –µ—Å–ª–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ —á–∏—Å–ª–æ, –∏–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å."""
        if x is None:
            return None
        # –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ int ‚Äî –æ—Ç–ª–∏—á–Ω–æ
        if isinstance(x, int):
            return x
        # –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞/—á–∏—Å–ª–æ-–ø–æ–¥–æ–±–Ω–æ–µ ‚Äî –ø—Ä–æ–±—É–µ–º int
        try:
            s = str(x).strip()
            if s == "":
                return None
            return int(s)
        except Exception:
            # —Å—Ç—Ä–æ–∫–∞ —Ç–æ–∂–µ hashable
            return x

    def _attrs_as_dict(m: Dict[str, Any]) -> Dict[str, Any]:
        """attrs –º–æ–≥—É—Ç –±—ã—Ç—å dict –∏–ª–∏ JSON-—Å—Ç—Ä–æ–∫–æ–π; –ø—Ä–∏–≤–æ–¥–∏–º –∫ dict."""
        a = m.get("attrs")
        if isinstance(a, dict):
            return a
        if isinstance(a, str) and a.strip():
            try:
                return json.loads(a)
            except Exception:
                return {}
        return {}

    def _role_ok(meta_or_item: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ role, –µ—Å–ª–∏ role –∑–∞–¥–∞–Ω."""
        if not role:
            return True
        a = _attrs_as_dict(meta_or_item)
        # –µ—Å–ª–∏ attrs –ø—É—Å—Ç–æ–π ‚Äî —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —Ä–æ–ª—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–µ–ª—å–∑—è ‚Üí –ù–ï –±–µ—Ä—ë–º –ø—Ä–∏ role-—Ñ–∏–ª—å—Ç—Ä–µ
        if not a:
            return False
        return (a.get("role") or "") == role

    # üîß –ê–≤—Ç–æ-—É–≤–µ–ª–∏—á–µ–Ω–∏–µ top_k –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö/—Å–æ—Å—Ç–∞–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    q_norm = (query or "").strip()
    if q_norm:
        long_query = len(q_norm) > 200
        many_parts = (q_norm.count(" –∏ ") + q_norm.count(",") + q_norm.count(";")) > 3
        multi_questions = q_norm.count("?") > 1

        if long_query or many_parts or multi_questions:
            top_k = max(top_k, 12)
        elif len(q_norm) > 100:
            top_k = max(top_k, 10)

    pack = _load_doc(owner_id, doc_id)

    used_ids: set[Any] = set()

    # 1) –°–ø–µ—Ü-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏‚Ä¶), –µ—Å–ª–∏ —è–≤–Ω–æ —É–ø–æ–º—è–Ω—É—Ç—ã
    special: List[Dict[str, Any]] = _inject_special_sources_for_item(
        pack,
        query,
        used_ids,     # ‚úÖ –¥–∞—ë–º –æ–±—â–∏–π used_ids
        doc_id=doc_id,
    ) or []

    for sp in special:
        sp["for_item"] = None

    # 2) –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
    rescored = _score_and_rank(
        pack,
        query,
        prelim_k=max(top_k * 3, 16),
        section_prefix=section_prefix,
        element_types=element_types,
    )

    if not rescored and not special:
        return []

    # 3) –ü—Ä–µ–¥-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ (–∏—Å—Ç–æ—á–Ω–∏–∫–∏/—Ä–æ–ª—å)
    sig = _query_signals(query)
    filtered: List[Tuple[int, float]] = []

    for i, sc in rescored:
        if len(filtered) >= top_k * 3:
            break

        mi = pack["meta"][int(i)]

        # —Å–∫—Ä—ã–≤–∞–µ–º references, –µ—Å–ª–∏ –∏—Ö –Ω–µ –ø—Ä–æ—Å–∏–ª–∏
        if (not sig["ask_sources"]) and _chunk_type(mi) == "reference":
            continue

        # —Ñ–∏–ª—å—Ç—Ä –ø–æ —Ä–æ–ª–∏
        if role and not _role_ok(mi):
            continue

        filtered.append((int(i), float(sc)))

    best = (filtered or [(int(i), float(sc)) for i, sc in rescored])[: max(top_k, 1)]

    out: List[Dict] = []

    # 4) –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º special (—Å —Ä–æ–ª—å-—Ñ–∏–ª—å—Ç—Ä–æ–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    for sp in special:
        if role and not _role_ok(sp):
            continue

        sp_id_norm = _norm_id(sp.get("id"))
        if sp_id_norm is not None and sp_id_norm in used_ids:
            continue

        out.append(sp)
        if sp_id_norm is not None:
            used_ids.add(sp_id_norm)

        if len(out) >= top_k:
            return out

    # 5) –°–∏–Ω—Ç–µ—Ç–∏–∫–∞ –ø–æ —Ä–∏—Å—É–Ω–∫–∞–º ‚Äî –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏ role-—Ñ–∏–ª—å—Ç—Ä–µ (–∏–Ω–∞—á–µ –ª–æ–º–∞–µ—Ç ‚Äú—Å—Ç—Ä–æ–≥–æ –≤–≤–µ–¥–µ–Ω–∏–µ‚Äù)
    if not role:
        fig_snips = _figure_context_snippets_for_query(doc_id, query, max_items=2)
        for fs in fig_snips:
            fid_norm = _norm_id(fs.get("id"))
            if fid_norm is not None and fid_norm in used_ids:
                continue
            out.append(fs)
            if fid_norm is not None:
                used_ids.add(fid_norm)
            if len(out) >= top_k:
                return out

    # 6) –ó–∞—Ç–µ–º –¥–æ–±–∏—Ä–∞–µ–º –æ–±—ã—á–Ω—ã–µ —á–∞–Ω–∫–∏
    for i, sc in best:
        if len(out) >= top_k:
            break

        m = pack["meta"][int(i)]
        mid_norm = _norm_id(m.get("id"))
        if mid_norm is not None and mid_norm in used_ids:
            continue

        # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –µ—â—ë —Ä–∞–∑ —Ä–æ–ª—å
        if role and not _role_ok(m):
            continue

        out.append(
            {
                "id": m.get("id"),
                "page": m.get("page"),
                "section_path": m.get("section_path"),
                "text": (m.get("text") or "").strip(),
                "score": float(sc),
                # attrs –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞—Ä—É–∂—É ‚Äî –Ω–æ –µ—Å–ª–∏ —Ç–µ–±–µ –Ω—É–∂–Ω–æ –¥–ª—è –¥–µ–±–∞–≥–∞, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å:
                # "attrs": _attrs_as_dict(m),
            }
        )
        if mid_norm is not None:
            used_ids.add(mid_norm)

    return out


# --- NEW: chart_matrix/chart_data ‚Üí rows/values_str (–¥–ª—è –¥–∏–∞–≥—Ä–∞–º–º –≤ DOCX)
def _chart_rows_from_attrs(attrs: str | dict | None) -> list[dict] | None:
    """
    –î–æ—Å—Ç–∞—ë–º rows –∏–∑ attrs –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–º—É –≤–∏–¥—É:
      {
        "label": str,        # –ø–æ–¥–ø–∏—Å—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–≤–æ–∑–º–æ–∂–µ–Ω –ø—Ä–µ—Ñ–∏–∫—Å —Å–µ—Ä–∏–∏)
        "value": float|Any,  # —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        "unit": str|None,    # "%", "—à—Ç", –∏ —Ç.–ø.
        "value_raw": str|None,   # –∫–∞–∫ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º XML/—Ç–∞–±–ª–∏—Ü–µ
        "series_name": str|None,
        "category": str|None,
      }

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º:
      - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π OOXML-–≤–∏–¥: attrs["chart_matrix"] =
            {"categories":[...],
             "series":[{"name":..., "values":[...], "unit":"%"/None, ...}],
             "unit": "...", "meta": {...}}
      - –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç: attrs["chart_data"] = [ {...}, {...} ]
      - —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç: attrs["chart_data"] = [ {"label": ..., "value": ...}, ... ]
      - dict –≤–∏–¥–∞ {"categories":[...], "series":[{"values":[...], "unit":"%"}]}
    """
    # attrs –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ —Å—Ç—Ä–æ–∫–æ–π JSON, –∏ —É–∂–µ dict'–æ–º
    if isinstance(attrs, str):
        try:
            a = json.loads(attrs or "{}")
        except Exception:
            return None
    elif isinstance(attrs, dict):
        a = attrs
    else:
        return None

    rows_norm: list[dict] = []

    # ---- —Å–ª—É—á–∞–π 0: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π chart_matrix ----
    cm = a.get("chart_matrix")
    if isinstance(cm, dict):
        cats = cm.get("categories") or []
        series_list = cm.get("series") or []
        if isinstance(cats, list) and isinstance(series_list, list) and cats and series_list:
            cats = [str(c) for c in cats]
            multi = len(series_list) > 1
            default_unit = cm.get("unit")
            for s in series_list:
                if not isinstance(s, dict):
                    continue
                s_name = (s.get("name") or s.get("series_name") or "").strip() or None
                vals = s.get("values") or s.get("data") or []
                unit = s.get("unit") or default_unit
                for idx, v in enumerate(vals):
                    cat = cats[idx] if idx < len(cats) else f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {idx + 1}"
                    label = f"{s_name}: {cat}" if (multi and s_name) else cat
                    rows_norm.append(
                        {
                            "label": label,
                            "value": v,
                            "unit": unit,
                            "value_raw": None,
                            "series_name": s_name,
                            "category": cat,
                        }
                    )
        if rows_norm:
            return rows_norm

    # ---- —Å–ª—É—á–∞–π 1: —É–∂–µ —Å–ø–∏—Å–æ–∫ (–Ω–æ–≤—ã–π/—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç chart_data) ----
    raw = (
        a.get("chart_data")
        or (a.get("chart") or {}).get("data")
        or a.get("data")
        or a.get("series")
    )

    if isinstance(raw, list) and raw:
        for item in raw:
            if isinstance(item, dict):
                label = (
                    item.get("label")
                    or item.get("category")
                    or item.get("name")
                    or ""
                )
                series_name = item.get("series_name") or item.get("series") or None
                category = item.get("category") or item.get("cat") or None

                value = item.get("value")
                value_raw = item.get("value_raw")
                unit = item.get("unit")

                if value_raw is None and isinstance(value, str):
                    value_raw = value

                rows_norm.append(
                    {
                        "label": str(label).strip(),
                        "value": value,
                        "unit": unit,
                        "value_raw": value_raw,
                        "series_name": series_name,
                        "category": category,
                    }
                )
            elif isinstance(item, (list, tuple)) and len(item) >= 2:
                rows_norm.append(
                    {
                        "label": str(item[0]),
                        "value": item[1],
                        "unit": None,
                        "value_raw": None,
                        "series_name": None,
                        "category": None,
                    }
                )

        return rows_norm or None

    # ---- —Å–ª—É—á–∞–π 2: dict —Å categories/series ----
    if isinstance(raw, dict) and raw.get("categories") and raw.get("series"):
        cats = list(raw.get("categories") or [])
        s0 = (raw.get("series") or [{}])[0] or {}
        vals = list(s0.get("values") or s0.get("data") or [])
        unit = s0.get("unit")
        series_name = s0.get("name") or s0.get("series_name")

        for i in range(min(len(cats), len(vals))):
            rows_norm.append(
                {
                    "label": str(cats[i]),
                    "value": vals[i],
                    "unit": unit,
                    "value_raw": None,
                    "series_name": series_name,
                    "category": str(cats[i]),
                }
            )

        return rows_norm or None

    return None


def _format_chart_values(rows: list[dict]) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–µ —Å—Ç—Ä–æ–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–π –¥–∏–∞–≥—Ä–∞–º–º—ã –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö rows.
    """
    def _fmt_percent(v: float) -> str:
        if abs(v - round(v)) < 0.05:
            return f"{int(round(v))}%"
        s = f"{v:.2f}".rstrip("0").rstrip(".")
        return f"{s}%"

    def _fmt_number(v: float) -> str:
        return f"{v:.6g}"

    lines: list[str] = []

    for r in rows or []:
        lab = str(
            r.get("label")
            or r.get("name")
            or r.get("category")
            or ""
        ).strip()

        raw = r.get("value_raw")
        if isinstance(raw, (int, float)):
            raw = str(raw)
        raw = (raw or "").strip()

        unit = r.get("unit")
        value = r.get("value")
        num_val: float | None = None
        if isinstance(value, (int, float)):
            num_val = float(value)

        vstr = ""
        if raw:
            if unit == "%" and "%" not in raw and isinstance(num_val, float):
                vstr = _fmt_percent(num_val)
            else:
                vstr = raw
        elif isinstance(num_val, float):
            if unit == "%":
                vstr = _fmt_percent(num_val)
            else:
                vstr = _fmt_number(num_val)
        else:
            other = r.get("value")
            vstr = str(other) if other is not None else ""

        if not lab and not vstr:
            continue

        unit_suffix = ""
        if isinstance(unit, str) and unit.strip() and unit.strip() != "%":
            unit_suffix = f" {unit.strip()}"

        if lab and vstr:
            line = f"‚Äî {lab}: {vstr}{unit_suffix}"
        elif lab:
            line = f"‚Äî {lab}"
        else:
            line = f"‚Äî {vstr}{unit_suffix}"

        lines.append(line.strip())

    return "\n".join(lines)


def retrieve_in_area(owner_id: int, doc_id: int, query: str, section_prefix: str, top_k: int = 8) -> List[Dict]:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –∂—ë—Å—Ç–∫–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –∑–∞–¥–∞–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç—å—é –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    return retrieve(owner_id, doc_id, query, top_k=top_k, section_prefix=section_prefix)

def build_context(snippets: List[Dict], max_chars: int = 6000) -> str:
    """
    –°–∫–ª–µ–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞—è —Å–ª—É–∂–µ–±–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É—è –ø—Ä–æ–±–µ–ª—ã/—Ç–∏—Ä–µ.
    –°—Ç—Ä–∞–Ω–∏—Ü—ã/—Ä–∞–∑–¥–µ–ª—ã –Ω–µ –≤—Å—Ç–∞–≤–ª—è–µ–º ‚Äî —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç.
    """
    parts: List[str] = []
    total = 0
    for s in snippets:
        raw = (s.get("text") or "")
        block = _clean_for_ctx(raw)
        if not block:
            continue
        if total + len(block) > max_chars:
            remaining = max_chars - total
            if remaining <= 0:
                break
            block = block[:remaining]
        parts.append(block)
        total += len(block)
        if total >= max_chars:
            break
    return "\n\n".join(parts)

def invalidate_cache(owner_id: int, doc_id: int):
    """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∫—ç—à –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–≤—ã–∑—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)."""
    _DOC_CACHE.pop((owner_id, doc_id), None)

def find_intro_passport(
    owner_id: int,
    doc_id: int,
    *,
    intro_prefix: Optional[str] = None,
) -> Dict[str, str]:
    """
    –î–æ—Å—Ç–∞—ë—Ç "–ø–∞—Å–ø–æ—Ä—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è" –∏–∑ –≤–≤–µ–¥–µ–Ω–∏—è:
    –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å, –æ–±—ä–µ–∫—Ç, –ø—Ä–µ–¥–º–µ—Ç, —Ü–µ–ª—å, –∑–∞–¥–∞—á–∏, –≥–∏–ø–æ—Ç–µ–∑–∞.

    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
      1) regex (keyword_find) —Å—Ç—Ä–æ–≥–æ –ø–æ –≤–≤–µ–¥–µ–Ω–∏—é (section_prefix/role=intro)
      2) fallback —Å–µ–º–∞–Ω—Ç–∏–∫–∞ retrieve(..., role=intro)
    """
    from .retrieval import keyword_find, retrieve  # –µ—Å–ª–∏ —ç—Ç–æ –≤ —Ç–æ–º –∂–µ —Ñ–∞–π–ª–µ ‚Äî —É–±–µ—Ä–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–π –Ω–∞–ø—Ä—è–º—É—é

    fields: List[Tuple[str, str, str]] = [
        ("relevance", "–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–º—ã:", r"(–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç[—å–∏]\s+—Ç–µ–º[—ã—ã])"),
        ("obj", "–û–±—ä–µ–∫—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:", r"(–æ–±—ä–µ–∫—Ç\s+–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω[–∏—è–µ])"),
        ("subj", "–ü—Ä–µ–¥–º–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:", r"(–ø—Ä–µ–¥–º–µ—Ç\s+–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω[–∏—è–µ])"),
        ("goal", "–¶–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:", r"(—Ü–µ–ª—å\s+–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω[–∏—è–µ])"),
        ("tasks", "–ó–∞–¥–∞—á–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:", r"(–∑–∞–¥–∞—á[–∞–∏]\s+–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω[–∏—è–µ])"),
        ("hypothesis", "–ì–∏–ø–æ—Ç–µ–∑–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:", r"(–≥–∏–ø–æ—Ç–µ–∑[–∞—ã]\s+–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω[–∏—è–µ])"),
    ]

    out: Dict[str, str] = {}

    def _join_snips(hits: List[Dict[str, Any]]) -> str:
        return " ".join((h.get("snippet") or "").strip() for h in hits if (h.get("snippet") or "").strip()).strip()

    for key, prefix, rgx in fields:
        hits = keyword_find(
            owner_id,
            doc_id,
            rgx,
            max_hits=3 if key in {"tasks"} else 2,
            section_prefix=intro_prefix,
            role="intro",
        ) or []
        if hits:
            txt = _join_snips(hits)
            if txt:
                out[key] = txt
                continue

        # fallback —Å–µ–º–∞–Ω—Ç–∏–∫–∞ (—Ç–æ–ª—å–∫–æ intro)
        q = prefix.replace(":", "")
        sem = retrieve(
            owner_id,
            doc_id,
            q,
            top_k=4 if key in {"tasks"} else 3,
            section_prefix=intro_prefix,
            role="intro",
        ) or []
        for h in sem:
            t = (h.get("text") or "").strip()
            if t:
                out[key] = t
                break

    return out


def get_chapter_conclusions(
    owner_id: int,
    doc_id: int,
    chapter_num: int,
    *,
    top_k: int = 4,
) -> List[Dict[str, Any]]:
    """
    –î–æ—Å—Ç–∞—ë—Ç –≤—ã–≤–æ–¥—ã/–∏—Ç–æ–≥–∏ –ø–æ –≥–ª–∞–≤–µ.
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
      1) keyword_find –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º "–≤—ã–≤–æ–¥—ã/–∏—Ç–æ–≥–∏/—Ä–µ–∑—é–º–µ"
      2) fallback: –ø–æ—Å–ª–µ–¥–Ω–∏–µ top_k —á–∞–Ω–∫–æ–≤ role=chapter_N (—á–µ—Ä–µ–∑ retrieve –ø–æ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É)
    """
    from .retrieval import keyword_find, retrieve  # –µ—Å–ª–∏ –≤ —Ç–æ–º –∂–µ —Ñ–∞–π–ª–µ ‚Äî —É–±–µ—Ä–∏

    role = f"chapter_{int(chapter_num)}"
    hits: List[Dict[str, Any]] = []

    for pat in [
        r"(–≤—ã–≤–æ–¥[—ã–∞]\s+–ø–æ\s+–≥–ª–∞–≤–µ|–≤—ã–≤–æ–¥[—ã–∞]\s+–ø–æ\s+\d+\s+–≥–ª–∞–≤–µ)",
        r"(–∏—Ç–æ–≥[–∏–æ–≤]\s+–ø–æ\s+–≥–ª–∞–≤–µ|–∏—Ç–æ–≥[–∏–æ–≤]\s+–ø–æ\s+\d+\s+–≥–ª–∞–≤–µ)",
        r"(—Ä–µ–∑—é–º–µ\s+–ø–æ\s+–≥–ª–∞–≤–µ|–∑–∞–∫–ª—é—á–µ–Ω–∏–µ\s+–ø–æ\s+–≥–ª–∞–≤–µ)",
        r"\b–≤—ã–≤–æ–¥[—ã–∞]\b",
        r"\b–∏—Ç–æ–≥[–∏–æ–≤]\b",
    ]:
        hits.extend(keyword_find(owner_id, doc_id, pat, max_hits=2, role=role) or [])
        if len(hits) >= 2:
            break

    if hits:
        # –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Ñ–æ—Ä–º–∞—Ç—É snippets (text)
        out = []
        for h in hits[: max(1, top_k)]:
            sn = (h.get("snippet") or "").strip()
            if sn:
                out.append({"id": None, "text": sn, "score": 1.0})
        return out

    # fallback: –¥–æ–±–∏—Ä–∞–µ–º ‚Äú—Ö–≤–æ—Å—Ç‚Äù –≥–ª–∞–≤—ã —Å–µ–º–∞–Ω—Ç–∏–∫–æ–π
    # (–∑–∞–ø—Ä–æ—Å –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, –Ω–æ role —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤—É)
    sem = retrieve(
        owner_id,
        doc_id,
        f"–≤—ã–≤–æ–¥—ã –ø–æ –≥–ª–∞–≤–µ {chapter_num}",
        top_k=max(4, top_k),
        role=role,
    ) or []

    # —Ö–æ—Ç–∏–º –∏–º–µ–Ω–Ω–æ ‚Äú—Ö–≤–æ—Å—Ç‚Äù: –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ (–æ–±—ã—á–Ω–æ retrieve –¥–∞—ë—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ,
    # –Ω–æ –µ—Å–ª–∏ –≤ –≥–ª–∞–≤–µ –Ω–µ—Ç —è–≤–Ω—ã—Ö "–≤—ã–≤–æ–¥–æ–≤", —ç—Ç–æ —Ö–æ—Ç—è –±—ã –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã).
    sem_tail = sem[-top_k:] if len(sem) > top_k else sem
    out = []
    for h in sem_tail:
        t = (h.get("text") or "").strip()
        if t:
            out.append({"id": h.get("id"), "text": t, "score": h.get("score", 0.0)})
    return out


def get_chapter_body(
    owner_id: int,
    doc_id: int,
    chapter_num: int,
    *,
    top_k: int = 6,
) -> List[Dict[str, Any]]:
    """
    –î–æ—Å—Ç–∞—ë—Ç ‚Äú—Å–µ—Ä–µ–¥–∏–Ω—É‚Äù –≥–ª–∞–≤—ã (–≥–ª–∞–≤–Ω—ã–µ –∏–¥–µ–∏).
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
      - –±–µ—Ä—ë–º –ø–æ–±–æ–ª—å—à–µ —á–∞–Ω–∫–æ–≤ role=chapter_N,
      - –∑–∞—Ç–µ–º —Ä–µ–∂–µ–º "—Å–µ—Ä–µ–¥–∏–Ω—É": –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ ~20% –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ ~20%,
        —á—Ç–æ–±—ã –Ω–µ —Ü–µ–ø–ª—è—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –≤—ã–≤–æ–¥—ã.
    """
    from .retrieval import retrieve  # –µ—Å–ª–∏ –≤ —Ç–æ–º –∂–µ —Ñ–∞–π–ª–µ ‚Äî —É–±–µ—Ä–∏

    role = f"chapter_{int(chapter_num)}"
    sem = retrieve(
        owner_id,
        doc_id,
        f"–æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≥–ª–∞–≤—ã {chapter_num}",
        top_k=max(12, top_k * 2),
        role=role,
    ) or []

    if not sem:
        return []

    # –±–µ—Ä—ë–º ‚Äú—Å–µ—Ä–µ–¥–∏–Ω—É‚Äù —Å–ø–∏—Å–∫–∞
    n = len(sem)
    lo = int(n * 0.2)
    hi = int(n * 0.8)
    mid = sem[lo:hi] if hi > lo else sem

    # —É–∂–º—ë–º –¥–æ top_k
    mid = mid[:top_k]

    out = []
    for h in mid:
        t = (h.get("text") or "").strip()
        if t:
            out.append({"id": h.get("id"), "text": t, "score": h.get("score", 0.0)})
    return out

# ---------------------------
# Keyword-fallback (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
# ---------------------------

def keyword_find(
    owner_id: int,
    doc_id: int,
    pattern: str,
    max_hits: int = 3,
    *,
    section_prefix: Optional[str] = None,
    role: Optional[str] = None,   # ‚úÖ NEW
) -> List[Dict]:
    rgx = re.compile(pattern, re.IGNORECASE)
    con = get_conn()
    cur = con.cursor()

    # –±–µ—Ä—ë–º attrs —Ç–æ–∂–µ, —á—Ç–æ–±—ã —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ role (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if section_prefix:
        cur.execute(
            "SELECT page, section_path, text, attrs FROM chunks "
            "WHERE owner_id=? AND doc_id=? AND section_path LIKE ?",
            (owner_id, doc_id, f"{section_prefix}%"),
        )
    else:
        cur.execute(
            "SELECT page, section_path, text, attrs FROM chunks "
            "WHERE owner_id=? AND doc_id=?",
            (owner_id, doc_id),
        )

    rows = cur.fetchall()
    con.close()

    hits: List[Dict] = []
    for r in rows:
        # ‚úÖ —Ñ–∏–ª—å—Ç—Ä –ø–æ —Ä–æ–ª–∏ (–µ—Å–ª–∏ –ø—Ä–æ—Å–∏–ª–∏)
        if role:
            raw_attrs = None
            try:
                raw_attrs = r["attrs"]  # sqlite3.Row
            except Exception:
                raw_attrs = None

            a = {}
            if isinstance(raw_attrs, dict):
                a = raw_attrs
            elif isinstance(raw_attrs, str) and raw_attrs.strip():
                try:
                    a = json.loads(raw_attrs)
                except Exception:
                    a = {}

            if (a.get("role") or "") != role:
                continue

        t = (r["text"] or "")
        m = rgx.search(t)
        if m:
            s = max(m.start() - 120, 0)
            e = min(m.end() + 120, len(t))
            hits.append(
                {
                    "page": r["page"],
                    "section_path": r["section_path"],
                    "snippet": t[s:e].strip(),
                }
            )
            if len(hits) >= max_hits:
                break
    return hits

def get_area_text(
    owner_id: int,
    doc_id: int,
    *,
    role: str,
    max_chars: int = 14000,
    head_ratio: float = 0.55,
) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∑–æ–Ω—ã –ø–æ attrs.role (intro/chapter_1/...).
    + Fallback: –µ—Å–ª–∏ role-—Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ/–Ω–µ—Ç, –±–µ—Ä—ë–º –¥–∏–∞–ø–∞–∑–æ–Ω —á–∞–Ω–∫–æ–≤ –º–µ–∂–¥—É –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
      (–í–í–ï–î–ï–ù–ò–ï->–ì–õ–ê–í–ê 1, –ì–õ–ê–í–ê 1->–ì–õ–ê–í–ê 2, –ì–õ–ê–í–ê 2->–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï/–°–ü–ò–°–û–ö –õ–ò–¢–ï–†–ê–¢–£–†–´).
    """

    r = (role or "").strip()
    if not r:
        return ""

    # –æ–≥—Ä–∞–Ω–∏—á–∏–º head_ratio
    try:
        head_ratio = float(head_ratio)
    except Exception:
        head_ratio = 0.55
    head_ratio = max(0.1, min(head_ratio, 0.9))

    con = get_conn()
    cur = con.cursor()

    # –∫–æ–ª–æ–Ω–∫–∏
    try:
        cur.execute("PRAGMA table_info(chunks)")
        cols = {row[1] for row in cur.fetchall()}
    except Exception:
        cols = set()

    if "text" not in cols or "id" not in cols:
        con.close()
        return ""

    has_attrs = "attrs" in cols
    has_element_type = "element_type" in cols

    # -----------------------------
    # 1) –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å: –ø–æ attrs.role
    # -----------------------------
    rows = []
    if has_attrs:
        like1 = f'%"role":"{r}"%'
        like2 = f'%"role": "{r}"%'

        if has_element_type:
            cur.execute(
                """
                SELECT id, text
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                  AND (attrs LIKE ? OR attrs LIKE ?)
                  AND (element_type IS NULL OR element_type NOT IN ('reference','table_row','ocr','chart'))
                ORDER BY id ASC
                """,
                (owner_id, doc_id, like1, like2),
            )
        else:
            cur.execute(
                """
                SELECT id, text
                FROM chunks
                WHERE owner_id=? AND doc_id=?
                  AND (attrs LIKE ? OR attrs LIKE ?)
                ORDER BY id ASC
                """,
                (owner_id, doc_id, like1, like2),
            )

        rows = cur.fetchall()

    def _rows_to_text(rows_) -> str:
        if not rows_:
            return ""

        all_txt: list[str] = []
        prev = None

        for row in rows_:
            # sqlite3.Row –æ–±—ã—á–Ω–æ –¥–∞—ë—Ç –¥–æ—Å—Ç—É–ø –ø–æ –∫–ª—é—á—É
            try:
                txt = row["text"]
            except Exception:
                try:
                    txt = row[1]
                except Exception:
                    txt = ""

            txt = (txt or "").strip()
            if not txt:
                continue
            if prev == txt:
                continue
            all_txt.append(txt)
            prev = txt

        if not all_txt:
            return ""

        total_full = sum(len(t) for t in all_txt) + 2 * (len(all_txt) - 1)
        if total_full <= max_chars:
            return "\n\n".join(all_txt).strip()

        head_budget = int(max_chars * head_ratio)
        tail_budget = max_chars - head_budget

        head_parts: list[str] = []
        head_total = 0
        for t in all_txt:
            need = len(t) + (2 if head_parts else 0)
            if head_total + need > head_budget:
                remain = head_budget - head_total
                if remain > 200:
                    head_parts.append(t[:remain].rstrip())
                break
            head_parts.append(t)
            head_total += need

        tail_parts: list[str] = []
        tail_total = 0
        for t in reversed(all_txt):
            need = len(t) + (2 if tail_parts else 0)
            if tail_total + need > tail_budget:
                remain = tail_budget - tail_total
                if remain > 200:
                    tail_parts.append(t[-remain:].lstrip())
                break
            tail_parts.append(t)
            tail_total += need
        tail_parts.reverse()

        head_text = "\n\n".join(head_parts).strip()
        tail_text = "\n\n".join(tail_parts).strip()

        if not tail_text:
            return head_text

        return (head_text + "\n\n---\n\n[...–∫–æ–Ω–µ—Ü —Ä–∞–∑–¥–µ–ª–∞...]\n\n" + tail_text).strip()

    text_by_role = _rows_to_text(rows)
    logger.info("get_area_text role=%s: by_role_len=%s", r, len(text_by_role))

    # 1) –î–ª—è –≥–ª–∞–≤ ‚Äî –∫–∞–∫ —Ä–∞–Ω—å—à–µ: –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ –¥–ª–∏–Ω–µ ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
    if r != "intro" and len(text_by_role) >= 1800:
        con.close()
        return text_by_role

    # 2) –î–ª—è –≤–≤–µ–¥–µ–Ω–∏—è: –¥–ª–∏–Ω–∞ —Å–∞–º–∞ –ø–æ —Å–µ–±–µ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—è.
    #    –ï—Å–ª–∏ –Ω–µ—Ç "–ø–∞—Å–ø–æ—Ä—Ç–∞" (–æ–±—ä–µ–∫—Ç/–ø—Ä–µ–¥–º–µ—Ç/—Ü–µ–ª—å/–∑–∞–¥–∞—á–∏/–≥–∏–ø–æ—Ç–µ–∑–∞) ‚Üí –∏–¥—ë–º –≤ fallback.
    if r == "intro":
        low = (text_by_role or "").lower()
        passport_markers = [
            "–æ–±—ä–µ–∫—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
            "–ø—Ä–µ–¥–º–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
            "—Ü–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
            "–∑–∞–¥–∞—á",      # –ª–æ–≤–∏—Ç "–∑–∞–¥–∞—á–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"
            "–≥–∏–ø–æ—Ç–µ–∑",    # –ª–æ–≤–∏—Ç "–≥–∏–ø–æ—Ç–µ–∑–∞/–≥–∏–ø–æ—Ç–µ–∑—ã"
        ]
        hits = sum(m in low for m in passport_markers)

        # –µ—Å–ª–∏ –≤–≤–µ–¥–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–∞—Å—Ç–æ—è—â–µ–µ (–µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 –º–∞—Ä–∫–µ—Ä–∞) ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
        if len(text_by_role) >= 1200 and hits >= 2:
            con.close()
            return text_by_role
        # –∏–Ω–∞—á–µ –ù–ï –∑–∞–∫—Ä—ã–≤–∞–µ–º con –∏ –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–µ–º ‚Äî –ø–æ–π–¥—ë–º –≤ fallback –Ω–∏–∂–µ

    # -----------------------------
    # 2) Fallback: –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º (–º–µ–∂–¥—É –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏)
    # -----------------------------
    role_markers = {
        "intro": (
            ["–í–í–ï–î–ï–ù–ò–ï"],
            ["–ì–õ–ê–í–ê 1", "–ì–õ–ê–í–ê I"],
        ),
        "chapter_1": (
            ["–ì–õ–ê–í–ê 1", "–ì–õ–ê–í–ê I"],
            ["–ì–õ–ê–í–ê 2", "–ì–õ–ê–í–ê II"],
        ),
        "chapter_2": (
            ["–ì–õ–ê–í–ê 2", "–ì–õ–ê–í–ê II"],
            ["–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï", "–í–´–í–û–î–´", "–°–ü–ò–°–û–ö –õ–ò–¢–ï–†–ê–¢–£–†–´", "–ë–ò–ë–õ–ò–û–ì–†–ê–§"],
        ),
    }

    if r not in role_markers:
        con.close()
        return text_by_role  # —á—Ç–æ –Ω–∞—à–ª–∏ –ø–æ —Ä–æ–ª–∏ (–ø—É—Å—Ç—å –∏ –º–∞–ª–æ)

    start_markers, end_markers = role_markers[r]

    def _find_heading_id_first(markers: list[str], after_id: int | None = None) -> int | None:
        # –ò—â–µ–º –¢–û–õ–¨–ö–û –∑–∞–≥–æ–ª–æ–≤–∫–∏: "[–ó–∞–≥–æ–ª–æ–≤–æ–∫] ..."
        where_after = ""
        params = [owner_id, doc_id]
        if after_id is not None:
            where_after = " AND id > ?"
            params.append(after_id)

        likes = []
        for m in markers:
            m = (m or "").strip()
            if not m:
                continue
            likes.append("UPPER(text) LIKE ?")
            params.append(f"%{m.upper()}%")

        if not likes:
            return None

        sql = f"""
        SELECT id
        FROM chunks
        WHERE owner_id=? AND doc_id=? {where_after}
        AND text LIKE '[–ó–∞–≥–æ–ª–æ–≤–æ–∫]%'
        AND LENGTH(text) < 200
        AND text NOT LIKE '%...%'
        AND text NOT LIKE '%..%'
        AND text NOT LIKE '%—Å—Ç—Ä.%'
        AND text NOT LIKE '%page%'
        AND ({' OR '.join(likes)})
        ORDER BY id ASC
        LIMIT 1
        """
        cur.execute(sql, tuple(params))
        row = cur.fetchone()
        if not row:
            return None
        try:
            return int(row["id"])
        except Exception:
            return int(row[0])

    def _find_heading_id_last_before(markers: list[str], before_id: int) -> int | None:
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ markers –î–û before_id
        params = [owner_id, doc_id, before_id]

        likes = []
        for m in markers:
            m = (m or "").strip()
            if not m:
                continue
            likes.append("UPPER(text) LIKE ?")
            params.append(f"%{m.upper()}%")

        if not likes:
            return None

        sql = f"""
        SELECT id
        FROM chunks
        WHERE owner_id=? AND doc_id=?
        AND id < ?
        AND text LIKE '[–ó–∞–≥–æ–ª–æ–≤–æ–∫]%'
        AND LENGTH(text) < 200
        AND text NOT LIKE '%...%'
        AND text NOT LIKE '%..%'
        AND text NOT LIKE '%—Å—Ç—Ä.%'
        AND text NOT LIKE '%page%'
        AND ({' OR '.join(likes)})
        ORDER BY id DESC
        LIMIT 1
        """
        cur.execute(sql, tuple(params))
        row = cur.fetchone()
        if not row:
            return None
        try:
            return int(row["id"])
        except Exception:
            return int(row[0])


    # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º (–∞ –Ω–µ –ø–æ –ª—é–±—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º –≤ —Ç–µ–∫—Å—Ç–µ)

    if r == "intro":
        # 1) –Ω–∞–π–¥—ë–º –ø–µ—Ä–≤—É—é "–ì–õ–ê–í–ê 1" –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        end_id = _find_heading_id_first(["–ì–õ–ê–í–ê 1", "–ì–õ–ê–í–ê I"], after_id=None)
        if end_id is None:
            con.close()
            return text_by_role

        # 2) –≤–æ–∑—å–º—ë–º –ü–û–°–õ–ï–î–ù–ò–ô "–í–í–ï–î–ï–ù–ò–ï" –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–µ—Ä–µ–¥ –≥–ª–∞–≤–æ–π 1
        # 0) –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –°–û–î–ï–†–ñ–ê–ù–ò–ï / –û–ì–õ–ê–í–õ–ï–ù–ò–ï
        toc_id = _find_heading_id_first(
            ["–°–û–î–ï–†–ñ–ê–ù–ò–ï", "–û–ì–õ–ê–í–õ–ï–ù–ò–ï"],
            after_id=None,
        )

        # 1) –∏—â–µ–º –ü–ï–†–í–û–ï —Ä–µ–∞–ª—å–Ω–æ–µ "–í–í–ï–î–ï–ù–ò–ï" –ü–û–°–õ–ï —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        start_id = _find_heading_id_first(
            ["–í–í–ï–î–ï–ù–ò–ï"],
            after_id=toc_id if toc_id is not None else None,
        )

        if start_id is None:
            con.close()
            return text_by_role


    elif r == "chapter_1":
        start_id = _find_heading_id_first(["–ì–õ–ê–í–ê 1", "–ì–õ–ê–í–ê I"], after_id=None)
        if start_id is None:
            con.close()
            return text_by_role

        end_id = _find_heading_id_first(["–ì–õ–ê–í–ê 2", "–ì–õ–ê–í–ê II"], after_id=start_id)
        if end_id is None:
            end_id = start_id + 10_000_000

    elif r == "chapter_2":
        start_id = _find_heading_id_first(["–ì–õ–ê–í–ê 2", "–ì–õ–ê–í–ê II"], after_id=None)
        if start_id is None:
            con.close()
            return text_by_role

        end_id = _find_heading_id_first(
            ["–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï", "–í–´–í–û–î–´", "–°–ü–ò–°–û–ö –õ–ò–¢–ï–†–ê–¢–£–†–´", "–ë–ò–ë–õ–ò–û–ì–†–ê–§"],
            after_id=start_id,
        )
        if end_id is None:
            end_id = start_id + 10_000_000
        
        logger.info("get_area_text fallback role=%s: start_id=%s end_id=%s", r, start_id, end_id)

    else:
        con.close()
        return text_by_role


    # –í—ã—Ç–∞—â–∏–º —Ç–µ–∫—Å—Ç—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
    if has_element_type:
        cur.execute(
            """
            SELECT id, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
              AND id >= ? AND id < ?
              AND (element_type IS NULL OR element_type NOT IN ('reference','table_row','ocr','chart'))
            ORDER BY id ASC
            """,
            (owner_id, doc_id, start_id, end_id),
        )
    else:
        cur.execute(
            """
            SELECT id, text
            FROM chunks
            WHERE owner_id=? AND doc_id=?
              AND id >= ? AND id < ?
            ORDER BY id ASC
            """,
            (owner_id, doc_id, start_id, end_id),
        )

    range_rows = cur.fetchall()
    con.close()

    text_by_range = _rows_to_text(range_rows)

    head_preview = " ".join(text_by_range[:180].splitlines())
    logger.info("get_area_text fallback role=%s: range_len=%s head=%s", r, len(text_by_range), head_preview)

    if r == "intro":
        low_role = (text_by_role or "").lower()
        low_range = (text_by_range or "").lower()

        markers = [
            "–æ–±—ä–µ–∫—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
            "–ø—Ä–µ–¥–º–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
            "—Ü–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
            "–∑–∞–¥–∞—á",
            "–≥–∏–ø–æ—Ç–µ–∑",
        ]

        hits_role = sum(m in low_role for m in markers)
        hits_range = sum(m in low_range for m in markers)

        # –≤—ã–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ë–û–õ–¨–®–ò–ú —á–∏—Å–ª–æ–º –ø–∞—Å–ø–æ—Ä—Ç–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        if hits_range > hits_role and len(text_by_range) >= 800:
            return text_by_range

        if hits_role >= hits_range and len(text_by_role) >= 800:
            return text_by_role

        # –µ—Å–ª–∏ –æ–±–∞ –ø–ª–æ—Ö–∏–µ ‚Äî –≤—Å—ë —Ä–∞–≤–Ω–æ –≤–µ—Ä–Ω—ë–º range (–æ–Ω –æ–±—ã—á–Ω–æ —à–∏—Ä–µ)
        return text_by_range or text_by_role

    if r == "intro":
        low_role = (text_by_role or "").lower()
        low_range = (text_by_range or "").lower()

        markers = ["–æ–±—ä–µ–∫—Ç", "–ø—Ä–µ–¥–º–µ—Ç", "—Ü–µ–ª—å", "–∑–∞–¥–∞—á", "–≥–∏–ø–æ—Ç–µ–∑"]
        hits_role = sum(m in low_role for m in markers)
        hits_range = sum(m in low_range for m in markers)

        if hits_range >= hits_role and len(text_by_range) >= 800:
            return text_by_range

    # –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö ‚Äî –∫–∞–∫ –±—ã–ª–æ
    if len(text_by_range) > len(text_by_role):
        return text_by_range
    return text_by_role

# =======================================================================
#                –ù–û–í–û–ï: ¬´–†–ò–°–£–ù–ö–ò¬ª ‚Äî –°–ß–Å–¢–ß–ò–ö, –°–ü–ò–°–û–ö, –ö–ê–†–¢–û–ß–ö–ò
# =======================================================================

# ¬´–†–∏—Å—É–Ω–æ–∫ 3.2 ‚Äî ...¬ª, ¬´–†–∏—Å—É–Ω–æ–∫ 5: ...¬ª, ¬´Figure 2 ‚Äî ...¬ª
# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º: ¬´–†–∏—Å—É–Ω–æ–∫ 3.2¬ª, ¬´–†–∏—Å. 3.2¬ª, ¬´–†–∏—Å.3.2.¬ª, ¬´Figure 2¬ª, ¬´Fig. 2¬ª
_FIG_TITLE_RE = re.compile(
    r"(?i)\b(?:—Ä–∏—Å(?:\.|—É–Ω–æ–∫)?|fig(?:\.|ure)?)\s*([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*\.?)\s*(?:[‚Äî\-‚Äì:\u2013\u2014]\s*(.+))?"
)


def _shorten(s: str, limit: int = 120) -> str:
    s = (s or "").strip()
    if len(s) <= limit:
        return s
    return s[:limit - 1].rstrip() + "‚Ä¶"

def _last_segment(name: str) -> str:
    """–ë–µ—Ä—ë–º ¬´—Ö–≤–æ—Å—Ç¬ª –∏–∑ –¥–ª–∏–Ω–Ω—ã—Ö –ø—É—Ç–µ–π section_path."""
    s = (name or "").strip()
    if "/" in s:
        s = s.split("/")[-1].strip()
    s = re.sub(r"^\[\s*—Ä–∏—Å—É–Ω–æ–∫\s*\]\s*", "", s, flags=re.IGNORECASE)
    s = re.sub(r"\s*[-‚Äì‚Äî]\s*", " ‚Äî ", s)
    s = re.sub(r"\s+", " ", s).strip(" .")
    return s

def _parse_figure_title(text: str) -> Tuple[Optional[str], Optional[str]]:
    t = (text or "").strip()
    m = _FIG_TITLE_RE.search(t)
    if not m:
        return (None, None)
    raw_num = (m.group(1) or "").strip().replace(" ", "")
    num = _num_norm(raw_num)
    title = (m.group(2) or "").strip() or None
    return (num or None, title)

def count_figures(uid: int, doc_id: int) -> int:
    """
    –°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ ¬´—Ä–∏—Å—É–Ω–∫–æ–≤¬ª –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.
    –¢–µ–ø–µ—Ä—å —Å—á–∏—Ç–∞–µ–º –ø–æ —Ç–∞–±–ª–∏—Ü–µ figures, –∞ –Ω–µ –ø–æ chunks/regex.
    """
    figs = get_figures_for_doc(doc_id)
    return len(figs or [])


def list_figures(uid: int, doc_id: int, limit: int = 25) -> Dict[str, object]:
    """
    –ö–æ—Ä–æ—Ç–∫–∏–π —Å–ø–∏—Å–æ–∫ ¬´—Ä–∏—Å—É–Ω–∫–æ–≤¬ª –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∞–±–ª–∏—Ü—ã figures:
      { "count": N, "list": [ '–†–∏—Å—É–Ω–æ–∫ 2.1 ‚Äî –°—Ö–µ–º–∞ ‚Ä¶', ... ], "more": M }
    """
    figs = get_figures_for_doc(doc_id) or []

    items: List[str] = []
    for f in figs:
        num = _num_norm(str(f.get("label") or f.get("figure_label") or f.get("number") or ""))
        caption = (f.get("caption") or "").strip()
        if num and caption:
            items.append(f"–†–∏—Å—É–Ω–æ–∫ {num} ‚Äî {_shorten(caption)}")
        elif num:
            items.append(f"–†–∏—Å—É–Ω–æ–∫ {num}")
        elif caption:
            items.append(_shorten(caption))
        else:
            items.append("–†–∏—Å—É–Ω–æ–∫")

    items = [x for x in items if x]
    count = len(items)
    return {
        "count": count,
        "list": items[:limit],
        "more": max(0, count - limit),
    }


def get_figure_record(
    doc_id: int,
    *,
    figure_label: Optional[str] = None,
    figure_id: Optional[int] = None,
    ensure_analysis: bool = True,
) -> Optional[Dict[str, Any]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–Ω—É –∑–∞–ø–∏—Å—å –æ —Ä–∏—Å—É–Ω–∫–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã figures + —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ (vision_analyzer).

    –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –ª–∏–±–æ figure_id, –ª–∏–±–æ –Ω–æ–º–µ—Ä —Ä–∏—Å—É–Ω–∫–∞ (figure_label, –Ω–∞–ø—Ä. '2.3').
    –ï—Å–ª–∏ ensure_analysis=True ‚Äî –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –æ–Ω –±—É–¥–µ—Ç –ø–æ—Å—á–∏—Ç–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω
    —á–µ—Ä–µ–∑ figures.analyze_figure_for_db(..).
    """
    if figure_id is None and not figure_label:
        return None

    figs = get_figures_for_doc(doc_id)
    row: Optional[Dict[str, Any]] = None

    if figure_id is not None:
        for f in figs:
            if int(f.get("figure_id")) == int(figure_id):
                row = f
                break
    else:
        # –∏—â–µ–º –ø–æ –Ω–æ–º–µ—Ä—É —Ä–∏—Å—É–Ω–∫–∞ (label), —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π '2.3'/'2,3'
        wanted = _num_norm(str(figure_label))
        for f in figs:
            lab = _num_norm(str(f.get("label") or f.get("figure_label") or ""))
            if lab and lab == wanted:
                row = f
                break

    if not row:
        return None

    analysis: Optional[Dict[str, Any]] = None
    if ensure_analysis:
        try:
            # –ª–µ–Ω–∏–≤—ã–π –∏–º–ø–æ—Ä—Ç, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–æ–≤
            from .figures import analyze_figure_for_db
            analysis = analyze_figure_for_db(
                doc_id=doc_id,
                figure_id=int(row["figure_id"]),
                intent="describe-with-numbers",
            )
        except Exception:
            analysis = None

    return {
        "figure": row,
        "analysis": analysis,
    }

def _figure_values_str_from_row(fig_row: Dict[str, Any]) -> Optional[str]:
    """
    –î–æ—Å—Ç–∞—ë–º values_str –¥–ª—è –¥–∏–∞–≥—Ä–∞–º–º—ã –∏–∑ attrs —Ñ–∏–≥—É—Ä—ã (—Ç–∞–±–ª–∏—Ü–∞ figures):
      - attrs.chart_data
      - –∏–ª–∏ attrs.chart_matrix / data / series.
    –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–µ _chart_rows_from_attrs + _format_chart_values.
    """
    attrs = fig_row.get("attrs")
    if isinstance(attrs, str):
        try:
            attrs_obj = json.loads(attrs or "{}")
        except Exception:
            attrs_obj = {}
    elif isinstance(attrs, dict):
        attrs_obj = attrs
    else:
        attrs_obj = {}

    if not isinstance(attrs_obj, dict):
        return None

    rows = _chart_rows_from_attrs(attrs_obj)
    if not rows:
        return None
    return _format_chart_values(rows)

def _build_figure_context_text(rec: Dict[str, Any]) -> str:
    """
    –°–æ–±–∏—Ä–∞–µ–º —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ RAG-–∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–ø–∏—Å–∏ figure+analysis.
    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–º–µ—à–∏–≤–∞–µ–º values_str –¥–ª—è –¥–∏–∞–≥—Ä–∞–º–º—ã –∏–∑ attrs.chart_data/chart_matrix.
    """
    fig = rec.get("figure") or {}
    analysis = rec.get("analysis") or {}

    num = _num_norm(str(fig.get("label") or fig.get("figure_label") or "")) or None
    caption = (fig.get("caption") or "").strip()
    base_text = (analysis.get("text") or "").strip()

    prefix = f"–†–∏—Å—É–Ω–æ–∫ {num}" if num else "–†–∏—Å—É–Ω–æ–∫"

    if not base_text and caption:
        base_text = caption
    if not base_text:
        return ""

    # –ï—Å–ª–∏ caption –µ—Å—Ç—å –∏ –Ω–µ –≤—Ö–æ–¥–∏—Ç –≤ –æ—Å–Ω–æ–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ ‚Äî –¥–æ–±–∞–≤–∏–º –µ–≥–æ –≤ –∑–∞–≥–æ–ª–æ–≤–æ–∫
    if caption and caption not in base_text:
        head = f"{prefix} ‚Äî {caption}"
        if not base_text.startswith(head):
            text = f"{head}. {base_text}"
        else:
            text = base_text
    else:
        if not base_text.lower().startswith(prefix.lower()):
            text = f"{prefix}: {base_text}"
        else:
            text = base_text

    # –ü–æ–¥–º–µ—à–∏–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º—ã (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ attrs —Ñ–∏–≥—É—Ä—ã)
    vals_str = _figure_values_str_from_row(fig)
    if vals_str:
        # —á—Ç–æ–±—ã –Ω–µ –∑–∞–¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–∏—Ö —Å—Ç—Ä–æ–∫ –µ—â—ë –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ
        if vals_str not in text:
            text = f"{text}\n\n{vals_str}".strip()

    meta = (analysis.get("meta") or {}) if isinstance(analysis, dict) else {}
    caveat = (meta.get("caveat") or "").strip()
    if caveat and caveat not in text:
        text = f"{text} {caveat}".strip()

    return text


def _figure_context_snippets_for_query(doc_id: int, query: str, max_items: int = 2) -> List[Dict[str, Any]]:
    """
    –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —è–≤–Ω–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ ¬´–†–∏—Å—É–Ω–æ–∫ N¬ª, –ø–æ–¥–º–µ—à–∏–≤–∞–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç 1‚Äì2
    —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Å–Ω–∏–ø–ø–µ—Ç–∞ —Å –∫—Ä–∞—Ç–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ä–∏—Å—É–Ω–∫–æ–≤.
    """
    nums = extract_figure_numbers(query) if extract_figure_numbers else []  # type: ignore
    if not nums:
        return []

    out: List[Dict[str, Any]] = []
    seen: set[str] = set()

    for raw in nums:
        num = _num_norm(raw)
        if not num or num in seen:
            continue
        seen.add(num)

        rec = get_figure_record(doc_id, figure_label=num, ensure_analysis=True)
        if not rec or not rec.get("analysis"):
            continue

        text = _build_figure_context_text(rec)
        if not text:
            continue

        fig = rec["figure"]
        fid = int(fig.get("figure_id"))
        synthetic_id = -int(1_000_000 + fid)

        out.append(
            {
                "id": synthetic_id,
                "page": fig.get("page"),
                "section_path": (fig.get("section") or ""),
                "text": text,
                "score": 0.99,
            }
        )
        if len(out) >= max_items:
            break

    return out

def _collect_images_for_section(cur, uid: int, doc_id: int, section_path: str) -> List[str]:
    """–ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º images –∏–∑ attrs –ø–æ –≤—Å–µ–º —á–∞–Ω–∫–∞–º –¥–∞–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏ (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –Ω–µ –≤ –ø–µ—Ä–≤–æ–º)."""
    images: List[str] = []
    try:
        cur.execute(
            """
            SELECT attrs FROM chunks
            WHERE owner_id=? AND doc_id=? AND section_path=? AND attrs IS NOT NULL
            ORDER BY id ASC LIMIT 10
            """,
            (uid, doc_id, section_path),
        )
        rows = cur.fetchall() or []
        for rr in rows:
            try:
                obj = json.loads(rr["attrs"] or "{}")
                imgs = obj.get("images") or []
                for p in imgs:
                    if p and p not in images:
                        images.append(p)
            except Exception:
                continue
    except Exception:
        pass
    return images

def describe_figures_by_numbers(
    uid: int,
    doc_id: int,
    numbers: List[str],
    sample_chunks: int = 2,
    *,
    use_vision: bool = True,
    lang: str = "ru",
    vision_first_image_only: bool = True,
) -> List[Dict[str, Any]]:
    """
    –°—Ç—Ä–æ–∏—Ç RAG-–∫–∞—Ä—Ç–æ—á–∫–∏ –ø–æ –Ω–æ–º–µ—Ä–∞–º —Ä–∏—Å—É–Ω–∫–æ–≤.

    –ï–î–ò–ù–´–ô –ø—É—Ç—å:
      1) –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–æ–º–µ—Ä–∞ ('6', '2.3', '–†–∏—Å—É–Ω–æ–∫ 2.3' ‚Üí '6' / '2.3');
      2) –ø–æ –∫–∞–∂–¥–æ–º—É –Ω–æ–º–µ—Ä—É –±–µ—Ä—ë–º –∑–∞–ø–∏—Å—å –∏–∑ —Ç–∞–±–ª–∏—Ü—ã figures —á–µ—Ä–µ–∑ get_figure_record();
      3) –∏–∑ figures:
          - page / section,
          - image_path,
          - attrs (chart_data / chart_matrix ‚Üí values_str);
      4) —Å–æ—Å–µ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç –ø–æ–¥–Ω–∏–º–∞–µ–º –∏–∑ chunks –ø–æ section_path –∏–ª–∏ page;
      5) (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) vision-–æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ image_path.
    """
    if not numbers:
        return []

    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –¥–µ–¥—É–ø–∏–º –Ω–æ–º–µ—Ä–∞
    want = sorted(
        {
            _num_norm(str(n))
            for n in numbers
            if n and str(n).strip()
        }
    )
    want = [n for n in want if n]

    con = get_conn()
    cur = con.cursor()
    cards: List[Dict[str, Any]] = []

    for num in want:
        # 1) –¥–æ—Å—Ç–∞—ë–º —Ñ–∏–≥—É—Ä—É + –∞–Ω–∞–ª–∏–∑ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã figures / figure_analysis
        rec = get_figure_record(doc_id, figure_label=num, ensure_analysis=use_vision)
        if not rec or not rec.get("figure"):
            continue

        fig = rec["figure"]
        analysis = rec.get("analysis") or {}

        # –Ω–æ–º–µ—Ä / –ø–æ–¥–ø–∏—Å—å
        label = _num_norm(str(fig.get("label") or fig.get("figure_label") or num))
        caption = (fig.get("caption") or "").strip()

        display = f"–†–∏—Å—É–Ω–æ–∫ {label}"
        if caption:
            display += f" ‚Äî {caption}"

        page = fig.get("page")
        sec = fig.get("section") or ""

        # 2) –ø—É—Ç—å(–∏) –¥–æ –∫–∞—Ä—Ç–∏–Ω–∫–∏ ‚Äî –∏–∑ —Ç–∞–±–ª–∏—Ü—ã figures (image_path)
        images: List[str] = []
        img = fig.get("image_path")
        if img:
            images.append(img)

        # 3) 1‚Äì2 —Å–æ—Å–µ–¥–Ω–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
        highlights: List[str] = []

        if sec:
            cur.execute(
                """
                SELECT text FROM chunks
                WHERE owner_id=? AND doc_id=? AND section_path=?
                ORDER BY id ASC LIMIT ?
                """,
                (uid, doc_id, sec, max(1, sample_chunks)),
            )
            rows = cur.fetchall() or []
            for rr in rows:
                t = (rr["text"] or "").strip()
                if not t:
                    continue
                t = re.sub(r"^\[\s*–†–∏—Å—É–Ω–æ–∫\s*\]\s*", "", t, flags=re.IGNORECASE)
                highlights.append(_shorten(t, 200))

        if not highlights and page is not None:
            cur.execute(
                """
                SELECT text FROM chunks
                WHERE owner_id=? AND doc_id=? AND page=?
                ORDER BY id ASC LIMIT 1
                """,
                (uid, doc_id, page),
            )
            rr = cur.fetchone()
            if rr and rr["text"]:
                highlights.append(_shorten(rr["text"], 200))

        # 4) chart_matrix/values_str ‚Äî –∏–∑ attrs —Ñ–∏–≥—É—Ä—ã
                # 4) chart_matrix/values_str ‚Äî –∏–∑ attrs —Ñ–∏–≥—É—Ä—ã
        chart_matrix = None
        try:
            attrs = fig.get("attrs")
            if isinstance(attrs, str):
                attrs_obj = json.loads(attrs or "{}") or {}
            elif isinstance(attrs, dict):
                attrs_obj = attrs
            else:
                attrs_obj = {}
        except Exception:
            attrs_obj = {}

        if isinstance(attrs_obj, dict) and attrs_obj.get("chart_matrix") is not None:
            chart_matrix = attrs_obj.get("chart_matrix")

        # –∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ –°–ê–ú–û–ô –¥–∏–∞–≥—Ä–∞–º–º–µ (chart_data/chart_matrix)
        values_str = _figure_values_str_from_row(fig)

        # NEW: –µ—Å–ª–∏ —Å–≤–æ–∏—Ö —á–∏—Å–µ–ª —É —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç, –Ω–æ –ø–æ–¥–ø–∏—Å—å/—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥
        # —è–≤–Ω–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ —Ç–∞–±–ª–∏—Ü—É-–∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî –ø–æ–¥—Ç—è–≥–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.
        if not values_str:
            # –±–µ—Ä—ë–º –ø–æ–¥–ø–∏—Å—å + —Å–æ—Å–µ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã –∏—Å–∫–∞—Ç—å —Ñ—Ä–∞–∑—ã "–ø–æ –¥–∞–Ω–Ω—ã–º —Ç–∞–±–ª–∏—Ü—ã 2.1"
            hint_text_parts: List[str] = []
            if caption:
                hint_text_parts.append(caption)
            hint_text_parts.extend(h for h in (highlights or []) if h)
            hint_text = " ".join(hint_text_parts)

            src_table_num = _extract_table_source_from_text(hint_text)
            if src_table_num:
                try:
                    table_vals = _table_values_for_figure_from_doc(
                        uid,
                        doc_id,
                        src_table_num,
                        max_rows=12,
                    )
                except Exception:
                    table_vals = None

                if table_vals:
                    values_str = (
                        f"–ó–Ω–∞—á–µ–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º—ã –ø–æ–ª—É—á–µ–Ω—ã –ø–æ –¥–∞–Ω–Ω—ã–º —Ç–∞–±–ª–∏—Ü—ã {src_table_num}:\n"
                        f"{table_vals}"
                    )


        # 5) vision-–æ–ø–∏—Å–∞–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                # 5) vision-–æ–ø–∏—Å–∞–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) + –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        vision_desc: str = ""
        vision_raw_text: str = ""

        if use_vision and images:
            try:
                img_for_vision = images[0] if vision_first_image_only else images
                vp = vision_describe(img_for_vision, lang=lang)
            except Exception:
                vp = {
                    "description": "–æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.",
                    "tags": [],
                }

            # vp –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—ë–º ‚Äî –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            if isinstance(vp, str):
                vision_desc = vp.strip()
            elif isinstance(vp, dict):
                vision_desc = (vp.get("description") or "").strip()
                # –≥–ª–∞–≤–Ω–æ–µ: –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç / OCR
                vision_raw_text = (vp.get("raw_text") or vp.get("text") or "").strip()

        # –¥–æ–ø–æ–ª–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–º –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏–≥—É—Ä—ã, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if isinstance(analysis, dict):
            ana_text = (analysis.get("text") or analysis.get("raw_text") or "").strip()
            if ana_text:
                if vision_raw_text:
                    vision_raw_text = f"{vision_raw_text}\n{ana_text}"
                else:
                    vision_raw_text = ana_text

        vision_payload = None
        if vision_desc or vision_raw_text:
            vision_payload = {
                "description": vision_desc,
                "raw_text": vision_raw_text,
            }

        card: Dict[str, Any] = {
            "num": label,
            "display": display,
            "where": {"page": page, "section_path": sec},
            "images": images,
            "highlights": highlights[:2],
        }

        if chart_matrix is not None:
            card["chart_matrix"] = chart_matrix
        if values_str:
            card["values_str"] = values_str
        if vision_payload is not None:
            card["vision"] = vision_payload
        if analysis:
            card["analysis"] = analysis


        cards.append(card)

    con.close()
    return cards

# =======================================================================
#        –ù–û–í–û–ï: –¢–∞–±–ª–∏—Ü—ã ‚Äî –ø–æ–∏—Å–∫/—Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –∏ ¬´–≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª
# =======================================================================

# ¬´–¢–∞–±–ª–∏—Ü–∞ 2.1 ‚Äî ...¬ª, ¬´Table A.1 ‚Äî ...¬ª
_TABLE_TITLE_RE = re.compile(
    r"(?i)\b(?:—Ç–∞–±–ª(?:–∏—Ü–∞)?|table)\s+([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*)\b(?:\s*[‚Äî\-‚Äì:]\s*(.+))?",
)
_ROW_TAG_RE = re.compile(r"\[\s*row\s+(\d+)\s*\]", re.IGNORECASE)

def _num_norm(s: str | None) -> str:
    s = (s or "").strip()
    s = s.replace(" ", "").replace(",", ".")
    s = re.sub(r"[.)]+$", "", s)  # ¬´3.1.¬ª -> ¬´3.1¬ª, ¬´(3.1)¬ª -> ¬´3.1¬ª
    return s

# —Å–∏–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ —Å–≤—è–∑–∏ —Å —Ç–∞–±–ª–∏—Ü–µ–π:
# "–ø–æ –¥–∞–Ω–Ω—ã–º —Ç–∞–±–ª–∏—Ü—ã 2.1", "–ø–æ –¥–∞–Ω–Ω—ã–º —Ç–∞–±–ª. 2.1",
# "–Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü—ã 2.1", "—Å–æ–≥–ª–∞—Å–Ω–æ —Ç–∞–±–ª–∏—Ü–µ 2.1" –∏ —Ç.–ø.
_TABLE_SOURCE_RE = re.compile(
    r"(?i)"
    r"(?:–ø–æ\s+–¥–∞–Ω–Ω\w+\s+—Ç–∞–±–ª(?:–∏—Ü\w*)?\s+"
    r"|–Ω–∞\s+–æ—Å–Ω–æ–≤–∞–Ω–∏[–∏–∏—è]\s+–¥–∞–Ω–Ω\w+\—Å+—Ç–∞–±–ª(?:–∏—Ü\w*)?\s+"
    r"|—Å–æ–≥–ª–∞—Å–Ω–æ\s+—Ç–∞–±–ª(?:–∏—Ü\w*)?\s+)"
    r"([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*)"
)

def _extract_table_source_from_text(text: str) -> Optional[str]:
    """
    –ò—â–µ–º –≤ –ø–æ–¥–ø–∏—Å–∏/—Ä—è–¥–æ–º —Å —Ä–∏—Å—É–Ω–∫–æ–º —Å–∏–ª—å–Ω—É—é —Ñ—Ä–∞–∑—É –≤–∏–¥–∞
    ¬´–ø–æ –¥–∞–Ω–Ω—ã–º —Ç–∞–±–ª–∏—Ü—ã 2.1 / —Ç–∞–±–ª. 2.1 / —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–∞–±–ª–∏—Ü–µ 2.1¬ª.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –Ω–æ–º–µ—Ä —Ç–∞–±–ª–∏—Ü—ã –∏–ª–∏ None.
    """
    if not text:
        return None
    m = _TABLE_SOURCE_RE.search(text)
    if not m:
        return None
    return _num_norm(m.group(1))


def _table_base_from_section(section_path: str) -> str:
    """–£–±–∏—Ä–∞–µ–º —Ö–≤–æ—Å—Ç ' [row k]' –µ—Å–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç."""
    if not section_path:
        return ""
    pos = section_path.lower().find(" [row ")
    return section_path if pos < 0 else section_path[:pos]

def _parse_table_title(text: str) -> Tuple[Optional[str], Optional[str]]:
    t = (text or "").strip()
    m = _TABLE_TITLE_RE.search(t)
    if not m:
        return (None, None)
    num = _num_norm(m.group(1))
    title = (m.group(2) or "").strip() or None
    return (num or None, title)

# --- Fallback-—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏, –µ—Å–ª–∏ extract_* –Ω–µ –ø—Ä–∏–µ—Ö–∞–ª–∏ –∏–∑ .intents ---

if extract_table_numbers is None:  # type: ignore
    def extract_table_numbers(q: str) -> List[str]:  # type: ignore
        out: List[str] = []
        for m in re.finditer(
            r"(?:—Ç–∞–±–ª(?:–∏—Ü–∞)?|table)\s*(?:‚Ññ\s*|no\.?\s*|–Ω–æ–º–µ—Ä\s*)?([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*)",
            q or "",
            re.IGNORECASE,
        ):
            out.append(_num_norm(m.group(1)))
        return [x for x in out if x]

if extract_figure_numbers is None:  # type: ignore
    def extract_figure_numbers(q: str) -> List[str]:  # type: ignore
        out: List[str] = []
        for m in re.finditer(
            r"(?:—Ä–∏—Å(?:—É–Ω–æ–∫)?|fig(?:ure)?\.?)\s*(?:‚Ññ\s*|no\.?\s*|–Ω–æ–º–µ—Ä\s*)?([A-Za-z–ê-–Ø–∞-—è]?\s*\d+(?:[.,]\d+)*)",
            q or "",
            re.IGNORECASE,
        ):
            out.append(_num_norm(m.group(1)))
        return [x for x in out if x]

def _wants_all_values(ask: str) -> bool:
    """
    –•–∏–Ω—Ç ¬´–≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è / –≤—Å—é —Ç–∞–±–ª–∏—Ü—É¬ª:
      - –ª–∏–±–æ —á–µ—Ä–µ–∑ TABLE_ALL_VALUES_RE –∏–∑ .intents,
      - –ª–∏–±–æ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–π fallback (–æ–Ω —É–∂–µ –∑–∞—à–∏—Ç –≤ TABLE_ALL_VALUES_RE –≤—ã—à–µ).
    """
    return bool(TABLE_ALL_VALUES_RE.search(ask or ""))

def _find_table_bases_by_number(pack: dict, num: str) -> List[str]:
    """
    –ò—â–µ–º —Å–µ–∫—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü –ø–æ –Ω–æ–º–µ—Ä—É:
      - –ø–æ attrs.caption_num/label,
      - –ø–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—é '–¢–∞–±–ª–∏—Ü–∞/–¢–∞–±–ª./Table N' –≤ section_path/text
        —Å —Ç–µ—Ä–ø–∏–º–æ—Å—Ç—å—é –∫ '2.1' / '2,1' / '2.1.'.
      - –ï–°–õ–ò –Ω–µ –Ω–∞—à–ª–∏ ‚Äî —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Ä–µ–∂–∏–º: –ø–æ –ø–æ—Ä—è–¥–∫–æ–≤–æ–º—É –Ω–æ–º–µ—Ä—É —Ç–∞–±–ª–∏—Ü—ã (attrs.order_index),
        —á—Ç–æ–±—ã —É–º–µ—Ç—å –Ω–∞—Ö–æ–¥–∏—Ç—å –¥–∞–∂–µ ¬´–∫—Ä–∏–≤—ã–µ¬ª —Ç–∞–±–ª–∏—Ü—ã –±–µ–∑ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –ø–æ–¥–ø–∏—Å–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –±–∞–∑–æ–≤—ã—Ö section_path (–±–µ–∑ [row k]).
    """
    bases: List[str] = []
    needle = _num_norm(num)
    if not needle:
        return bases

    # –î–æ–ø—É—Å–∫–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —á–∏—Å–ª–∞: 2.1 / 2,1 / 2.1.
    core = re.escape(needle).replace(r"\.", r"[.,]")
    num_pat = rf"{core}\s*[.)]?"          # '2.1', '2,1', '2.1.' –∏ —Ç.–ø.
    ru_pat = re.compile(rf"(?i)\b—Ç–∞–±–ª(?:–∏—Ü–∞)?\s+{num_pat}")
    en_pat = re.compile(rf"(?i)\btable\s+{num_pat}")

    # --- 1) –û–±—ã—á–Ω—ã–π –ø—É—Ç—å: –ø–æ caption_num/label –∏ –≤—Ö–æ–∂–¥–µ–Ω–∏—è–º "–¢–∞–±–ª–∏—Ü–∞ N" –≤ —Ç–µ–∫—Å—Ç–µ/–ø—É—Ç–∏ ---
    for m in pack["meta"]:
        et = _chunk_type(m)
        if et not in {"table", "table_row"}:
            continue

        attrs = m.get("attrs") or {}
        cand = _num_norm(str(attrs.get("caption_num") or attrs.get("label") or ""))
        if cand and cand == needle:
            base = _table_base_from_section(m.get("section_path") or "")
            if base and base not in bases:
                bases.append(base)
            continue

        # fallback –ø–æ —Ç–µ–∫—Å—Ç—É/–ø—É—Ç–∏ (RU/EN, '–¢–∞–±–ª–∏—Ü–∞' –∏ '–¢–∞–±–ª.')
        sp = (m.get("section_path") or "")
        tx = (m.get("text") or "")
        if ru_pat.search(sp) or ru_pat.search(tx) or en_pat.search(sp) or en_pat.search(tx):
            base = _table_base_from_section(sp)
            if base and base not in bases:
                bases.append(base)

    # –ï—Å–ª–∏ —É–∂–µ —á—Ç–æ-—Ç–æ –Ω–∞—à–ª–∏ ‚Äî —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Ä–µ–∂–∏–º –Ω–µ –Ω—É–∂–µ–Ω
    if bases:
        return bases

    # --- 2) –†–µ–∑–µ—Ä–≤–Ω—ã–π —Ä–µ–∂–∏–º: –ø–æ–∏—Å–∫ –ø–æ –ø–æ—Ä—è–¥–∫–æ–≤–æ–º—É –Ω–æ–º–µ—Ä—É —Ç–∞–±–ª–∏—Ü—ã (attrs.order_index) ---
    # –†–∞–±–æ—Ç–∞–µ—Ç, –∫–æ–≥–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç "—Ç–∞–±–ª–∏—Ü–∞ 6", –∞ –ø–æ–¥–ø–∏—Å—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∫—Ä–∏–≤–∞—è
    # –∏–ª–∏ –≤–æ–æ–±—â–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –Ω–æ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –º—ã –≤—Å—ë —Ä–∞–≤–Ω–æ –ø—Ä–∏—Å–≤–æ–∏–ª–∏ order_index=6.
    try:
        # –ë–µ—Ä—ë–º —Ü–µ–ª—É—é —á–∞—Å—Ç—å –Ω–æ–º–µ—Ä–∞ (–¥–ª—è '6', '6.1' ‚Üí 6)
        ordinal = int(float(needle.split(".")[0]))
    except Exception:
        return bases

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã —Å order_index –∏ –∏—Ö –±–∞–∑–æ–≤—ã–µ –ø—É—Ç–∏
    candidates: List[Tuple[int, str]] = []
    for m in pack["meta"]:
        et = _chunk_type(m)
        if et not in {"table", "table_row"}:
            continue

        attrs = m.get("attrs") or {}
        oi = attrs.get("order_index")
        if isinstance(oi, str):
            try:
                oi = int(oi)
            except Exception:
                oi = None
        if not isinstance(oi, int):
            continue

        base = _table_base_from_section(m.get("section_path") or "")
        if not base:
            continue
        candidates.append((oi, base))

    if not candidates:
        return bases

    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ base –æ—Å—Ç–∞–≤–ª—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π order_index (–∑–∞–≥–æ–ª–æ–≤–æ—á–Ω—ã–π —á–∞–Ω–∫ —Ç–∞–±–ª–∏—Ü—ã)
    best_by_base: Dict[str, int] = {}
    for oi, base in candidates:
        prev = best_by_base.get(base)
        if prev is None or oi < prev:
            best_by_base[base] = oi

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Ä—è–¥–∫—É –ø–æ—è–≤–ª–µ–Ω–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
    sorted_bases = sorted(best_by_base.items(), key=lambda x: x[1])

    # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—É —Å –Ω—É–∂–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–≤—ã–º –Ω–æ–º–µ—Ä–æ–º
    for oi, base in sorted_bases:
        if oi == ordinal and base not in bases:
            bases.append(base)

    return bases


def _row_index(section_path: str) -> int:
    m = _ROW_TAG_RE.search(section_path or "")
    try:
        return int(m.group(1)) if m else 10**9
    except Exception:
        return 10**9

def _gather_table_chunks_for_base(pack: dict, base: str, *, include_header: bool = True, row_limit: Optional[int] = None) -> List[Dict]:
    """
    –°–æ–±–∏—Ä–∞–µ–º —á–∞–Ω–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ç–∞–±–ª–∏—Ü—ã (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏ –≤—Å–µ –µ—ë —Å—Ç—Ä–æ–∫–∏ (table_row) –ø–æ base.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ retrieve(): {id, page, section_path, text, score}
    """
    rows: List[Dict] = []
    header: Optional[Dict] = None

    for m in pack["meta"]:
        sp = (m.get("section_path") or "")
        if not sp:
            continue
        if _table_base_from_section(sp) != base:
            continue
        et = _chunk_type(m)
        item = {
            "id": m["id"],
            "page": m["page"],
            "section_path": sp,
            "text": (m.get("text") or "").strip(),
            "score": 0.9 if et == "table" else 0.85
        }
        if et == "table":
            if include_header and not header:
                header = item
        elif et == "table_row" or "[row " in sp.lower():
            rows.append(item)

    rows.sort(key=lambda x: _row_index(x.get("section_path") or ""))

    if row_limit is not None and row_limit >= 0:
        rows = rows[:row_limit]

    out: List[Dict] = []
    if include_header and header:
        out.append(header)
    out.extend(rows)
    return out

def _table_values_for_figure_from_doc(
    owner_id: int,
    doc_id: int,
    table_num: str,
    *,
    max_rows: int = 12,
) -> Optional[str]:
    """
    –î–ª—è —Ä–∏—Å—É–Ω–∫–∞, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ ¬´–ø–æ –¥–∞–Ω–Ω—ã–º —Ç–∞–±–ª–∏—Ü—ã N¬ª, –ø–æ–¥–Ω–∏–º–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ
    –∑–Ω–∞—á–µ–Ω–∏—è —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏–∑ chunks –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –∏—Ö –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¢–û–õ–¨–ö–û –∫–æ–≥–¥–∞ —É —Å–∞–º–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –Ω–µ—Ç —Å–≤–æ–∏—Ö —á–∏—Å–µ–ª.
    """
    pack = _load_doc(owner_id, doc_id)
    bases = _find_table_bases_by_number(pack, table_num)
    if not bases:
        return None

    parts: List[str] = []
    seen: set[str] = set()

    # –æ–±—ã—á–Ω–æ –ø–µ—Ä–≤–∞—è –±–∞–∑–∞ ‚Äî –Ω—É–∂–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞; –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
    base = bases[0]
    chunks = _gather_table_chunks_for_base(
        pack,
        base,
        include_header=True,
        row_limit=max_rows,
    )

    for ch in chunks:
        t = _clean_for_ctx(ch.get("text") or "")
        if not t:
            continue
        if t in seen:
            continue
        seen.add(t)
        parts.append(t)

    if not parts:
        return None

    return "\n".join(parts)


def _inject_special_sources_for_item(pack: dict, ask: str, used_ids: set[int], *, doc_id: int) -> List[Dict]:
    """
    –ï—Å–ª–∏ –ø–æ–¥–ø—É–Ω–∫—Ç —è–≤–Ω–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ ¬´–¢–∞–±–ª–∏—Ü–∞ N¬ª / ¬´–†–∏—Å—É–Ω–æ–∫ N¬ª, –¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ
    –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã –ø—Ä–∏ ¬´–¥–∞–π –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª –∏ –∫—Ä–∞—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Ä–∏—Å—É–Ω–∫–æ–≤).
    """

    added: List[Dict] = []

    # –¢–∞–±–ª–∏—Ü—ã
    table_nums = extract_table_numbers(ask) if extract_table_numbers else []  # type: ignore
    want_all = _wants_all_values(ask)
    for num in table_nums:
        bases = _find_table_bases_by_number(pack, num)
        for base in bases:
            chunks = _gather_table_chunks_for_base(
                pack,
                base,
                include_header=True,
                row_limit=(None if want_all else 8),
            )
            for ch in chunks:
                if ch["id"] in used_ids:
                    continue
                ch["for_item"] = None  # –±—É–¥–µ—Ç –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–æ –≤—ã—à–µ –ø–æ –º–µ—Å—Ç—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                added.append(ch)
                used_ids.add(ch["id"])

    # –†–∏—Å—É–Ω–∫–∏: —Ä–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ —Ç–∞–±–ª–∏—Ü—É figures (–±–µ–∑ regex-–ø–æ–∏—Å–∫–∞ –ø–æ chunks)
    fig_nums = extract_figure_numbers(ask) if extract_figure_numbers else []  # type: ignore

    if fig_nums:
        for raw_num in fig_nums:
            num = _num_norm(raw_num)
            if not num:
                continue

            try:
                rec = get_figure_record(doc_id, figure_label=num, ensure_analysis=True)
            except Exception:
                rec = None
            if not rec or not rec.get("figure"):
                continue

            fig = rec["figure"]
            text = _build_figure_context_text(rec)
            if not text:
                continue

            fid = int(fig.get("figure_id"))
            synthetic_id = -int(10_000_000 + fid)
            if synthetic_id in used_ids:
                continue

            # –æ—Å–Ω–æ–≤–Ω–æ–π —Å–Ω–∏–ø–ø–µ—Ç ‚Äî —Ñ–∏–≥—É—Ä–∞ + –∞–Ω–∞–ª–∏–∑ + values_str
            added.append(
                {
                    "id": synthetic_id,
                    "page": fig.get("page"),
                    "section_path": (fig.get("section") or ""),
                    "text": text,
                    "score": 0.96,
                    "for_item": None,
                }
            )
            used_ids.add(synthetic_id)

            # –ø—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ–±–∞–≤–∏–º 1 –Ω–µ–±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏–∑ —Ç–æ–π –∂–µ —Å–µ–∫—Ü–∏–∏
            sec = fig.get("section") or ""
            if sec:
                for m in pack["meta"]:
                    if (m.get("section_path") or "") != sec:
                        continue
                    if m["id"] in used_ids:
                        continue
                    t = (m.get("text") or "").strip()
                    if not t:
                        continue
                    added.append(
                        {
                            "id": m["id"],
                            "page": m["page"],
                            "section_path": m["section_path"],
                            "text": _shorten(t, 200),
                            "score": 0.9,
                            "for_item": None,
                        }
                    )
                    used_ids.add(m["id"])
                    break  # –æ–¥–Ω–æ–≥–æ —Å–æ—Å–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ

        return added

    # –ï—Å–ª–∏ –Ω–∏ —Ç–∞–±–ª–∏—Ü, –Ω–∏ —Ä–∏—Å—É–Ω–∫–æ–≤ –ø–æ –≤–æ–ø—Ä–æ—Å—É –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
    return added


# =======================================================================
#   –ü—É–±–ª–∏—á–Ω—ã–µ —Ö–µ–ª–ø–µ—Ä—ã: —Ç–æ—á–µ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º, —Ä–∏—Å—É–Ω–∫–∞–º –∏ –≥–ª–∞–≤–∞–º
# =======================================================================

def get_table_context_for_numbers(
    owner_id: int,
    doc_id: int,
    numbers: List[str],
    *,
    include_all_values: bool = False,
    rows_limit: Optional[int] = None,
) -> List[Dict[str, Any]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –ø–æ –Ω–æ–º–µ—Ä–∞–º —Ç–∞–±–ª–∏—Ü:
      - –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã (–µ—Å–ª–∏ –µ—Å—Ç—å),
      - —Å—Ç—Ä–æ–∫–∏ (table_row) –≤ –ø–æ—Ä—è–¥–∫–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
      include_all_values=True  ‚Üí –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º rows_limit –∏ –æ—Ç–¥–∞—ë–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏;
      rows_limit=N             ‚Üí –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ (–µ—Å–ª–∏ include_all_values=False).
    """
    if not numbers:
        return []

    pack = _load_doc(owner_id, doc_id)
    used_ids: set[int] = set()
    snippets: List[Dict[str, Any]] = []

    for raw in numbers:
        num = _num_norm(str(raw))
        if not num:
            continue

        bases = _find_table_bases_by_number(pack, num)
        for base in bases:
            chunks = _gather_table_chunks_for_base(
                pack,
                base,
                include_header=True,
                row_limit=None if include_all_values else rows_limit,
            )
            for ch in chunks:
                cid = int(ch["id"])
                if cid in used_ids:
                    continue
                snippets.append(ch)
                used_ids.add(cid)

    return snippets


def get_figure_context_for_numbers(
    owner_id: int,
    doc_id: int,
    numbers: List[str],
    *,
    use_vision: bool = True,
    lang: str = "ru",
) -> List[Dict[str, Any]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç RAG-—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–æ –Ω–æ–º–µ—Ä–∞–º —Ä–∏—Å—É–Ω–∫–æ–≤:
      - page/section_path,
      - —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–∞–Ω–∞–ª–∏–∑ + –ø–æ–¥–ø–∏—Å—å + values_str –¥–ª—è –¥–∏–∞–≥—Ä–∞–º–º),
      - –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ vision-–æ–ø–∏—Å–∞–Ω–∏–µ –∏ –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º.

    –§–æ—Ä–º–∞—Ç —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–æ–∫ –∫ —ç–ª–µ–º–µ–Ω—Ç–∞–º retrieve():
      {id, page, section_path, text, score}
    """
    if not numbers:
        return []

    cards = describe_figures_by_numbers(
        uid=owner_id,
        doc_id=doc_id,
        numbers=numbers,
        sample_chunks=2,
        use_vision=use_vision,
        lang=lang,
    )

    snippets: List[Dict[str, Any]] = []
    used_ids: set[int] = set()

    for card in cards:
        num = card.get("num")
        where = card.get("where") or {}
        page = where.get("page")
        sec = where.get("section_path") or ""

        # —Å—Ç—Ä–æ–∏–º —Ç–µ–∫—Å—Ç —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ _build_figure_context_text
        analysis = card.get("analysis") or {}
        fig_stub = {
            "label": num,
            "figure_label": num,
            "caption": "",
            "attrs": {},
            "page": page,
            "section": sec,
        }
        rec = {"figure": fig_stub, "analysis": analysis}
        text = _build_figure_context_text(rec)
        if not text:
            text = ""

        # NEW: –ø–æ–¥–º–µ—à–∏–≤–∞–µ–º values_str –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ (—Å—é–¥–∞ –º–æ–≥—É—Ç –ø–æ–ø–∞—Å—Ç—å
        # –ª–∏–±–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –¥–∏–∞–≥—Ä–∞–º–º—ã, –ª–∏–±–æ –∑–Ω–∞—á–µ–Ω–∏—è, –ø–æ–¥—Ç—è–Ω—É—Ç—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã)
        values_str = card.get("values_str")
        if values_str and values_str not in text:
            text = f"{text}\n\n{values_str}".strip()

        if not text:
            continue

        # synthetic_id ‚Äî —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å id —á–∞–Ω–∫–æ–≤
        synthetic_id = -int(20_000_000 + hash(str(num)))
        if synthetic_id in used_ids:
            continue

        snippets.append(
            {
                "id": synthetic_id,
                "page": page,
                "section_path": sec,
                "text": text,
                "score": 0.97,
            }
        )
        used_ids.add(synthetic_id)

    return snippets


def get_section_context_for_hints(
    owner_id: int,
    doc_id: int,
    section_hints: List[str],
    *,
    per_section_k: int = 3,
) -> List[Dict[str, Any]]:
    """
    NEW:
      - –µ—Å–ª–∏ –≤ —á–∞–Ω–∫–∞—Ö –µ—Å—Ç—å attrs.role, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ (intro/chapter_1/...)
      - –¥–ª—è –≥–ª–∞–≤ –±–µ—Ä—ë–º –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ, –Ω–æ –∏ –∫–æ–Ω–µ—Ü (–≥–æ–ª–æ–≤–∞+—Ö–≤–æ—Å—Ç),
        —á—Ç–æ–±—ã –ø–æ–ø–∞–¥–∞–ª–∏ "–≤—ã–≤–æ–¥—ã", "–∏—Ç–æ–≥–∏" –∏ —Ç.–ø.
    """
    if not section_hints:
        return []

    pack = _load_doc(owner_id, doc_id)
    meta = pack.get("meta") or []

    snippets: List[Dict[str, Any]] = []
    used_ids: set[int] = set()

    def _get_role(m: Dict[str, Any]) -> str:
        a = m.get("attrs")
        if isinstance(a, dict):
            return str(a.get("role") or "").strip()
        if isinstance(a, str) and a.strip():
            try:
                a2 = json.loads(a)
                if isinstance(a2, dict):
                    return str(a2.get("role") or "").strip()
            except Exception:
                return ""
        return ""

    for h in section_hints:
        head = str(h or "").strip()
        if not head:
            continue

        head_lower = head.lower()

        # –æ–∂–∏–¥–∞–µ–º–∞—è —Ä–æ–ª—å –ø–æ hint
        expected_role: Optional[str] = None
        if head_lower in {"intro", "–≤–≤–µ–¥–µ–Ω–∏–µ", "vvedenie"}:
            expected_role = "intro"
        else:
            # –µ—Å–ª–∏ hint —ç—Ç–æ "1" / "2" / "3" ‚Üí chapter_1 / chapter_2 ...
            if head.isdigit():
                expected_role = f"chapter_{head}"

        candidates: List[Tuple[int, Dict[str, Any]]] = []

        for m in meta:
            sp = str(m.get("section_path") or "")
            if not sp:
                continue
            sp_lower = sp.lower()

            role = _get_role(m)

            # 1) –µ—Å–ª–∏ —Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ä–æ–ª–∏ (—Å–∞–º—ã–π –Ω–∞–¥—ë–∂–Ω—ã–π –ø—É—Ç—å)
            if expected_role:
                if role != expected_role:
                    # fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ role:
                    if expected_role == "intro":
                        if "–≤–≤–µ–¥–µ–Ω" not in sp_lower:
                            continue
                    else:
                        if not (
                            sp.startswith(head)
                            or f"/{head}" in sp
                            or f" {head}" in sp
                        ):
                            continue
            else:
                # 2) —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å: –ø–æ section_path
                if head_lower in {"intro", "–≤–≤–µ–¥–µ–Ω–∏–µ", "vvedenie"}:
                    if "–≤–≤–µ–¥–µ–Ω" not in sp_lower:
                        continue
                else:
                    if not (
                        sp.startswith(head)
                        or f"/{head}" in sp
                        or f" {head}" in sp
                    ):
                        continue

            candidates.append(
                (
                    int(m["id"]),
                    {
                        "id": m["id"],
                        "page": m.get("page"),
                        "section_path": sp,
                        "text": (m.get("text") or "").strip(),
                        "score": 0.9,
                    },
                )
            )

        if not candidates:
            continue

        candidates.sort(key=lambda t: t[0])

        # ‚úÖ –∫–ª—é—á–µ–≤–∞—è —á–∞—Å—Ç—å: –¥–ª—è –≥–ª–∞–≤ –±–µ—Ä—ë–º –Ω–∞—á–∞–ª–æ + –∫–æ–Ω–µ—Ü
        # —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–æ–ø–∞–¥–∞–ª–∏ "–≤—ã–≤–æ–¥—ã/–∏—Ç–æ–≥–∏" (–æ–Ω–∏ –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –≤ —Ö–≤–æ—Å—Ç–µ).
        want_tail = bool(expected_role and expected_role.startswith("chapter_"))
        k = max(int(per_section_k or 3), 1)

        if not want_tail or len(candidates) <= k:
            picked = candidates[:k]
        else:
            head_k = max(1, (k + 1) // 2)   # –Ω–∞–ø—Ä–∏–º–µ—Ä k=3 ‚Üí 2 –∏–∑ –≥–æ–ª–æ–≤—ã
            tail_k = k - head_k            # –∏ 1 –∏–∑ —Ö–≤–æ—Å—Ç–∞
            picked = candidates[:head_k] + candidates[-tail_k:] if tail_k > 0 else candidates[:head_k]

        for cid, item in picked:
            if cid in used_ids:
                continue
            if not item.get("text"):
                continue
            snippets.append(item)
            used_ids.add(cid)

    return snippets

# =======================================================================
#        –ù–û–í–û–ï: Coverage-aware –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–¥ –º–Ω–æ–≥–æ–ø—É–Ω–∫—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
#               —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π —Ç–∞–±–ª–∏—Ü/—Ä–∏—Å—É–Ω–∫–æ–≤
# =======================================================================

# –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤
_ENUM_LINE = re.compile(r"(?m)^\s*(?:\d+[).\)]|[-‚Äî‚Ä¢])\s+")

_ENUM_INLINE = re.compile(r"(?:\s|^)\(?\d{1,2}[)\.]\s+")

def plan_subitems(question: str, *, min_items: int = 2, max_items: int = 12) -> List[str]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –≤—ã—Ç–∞—â–∏—Ç—å –ø–æ–¥–ø—É–Ω–∫—Ç—ã –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.
    """
    q = (question or "").strip()
    items: List[str] = []

    # 1) –ø–æ—Å—Ç—Ä–æ—á–Ω–æ —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏
    if _ENUM_LINE.search(q):
        for line in q.splitlines():
            if _ENUM_LINE.match(line):
                items.append(
                    re.sub(r"^\s*(?:\d+[).\)]|[-‚Äî‚Ä¢])\s+", "", line).strip()
                )

    # 2) inline ¬´1) 2) 3)¬ª
    if not items and _ENUM_INLINE.search(q):
        buf, cur = [], None
        for token in re.finditer(r"\(?\d{1,2}[)\.]\s+|[^()]+", q):
            s = token.group(0)
            if re.fullmatch(r"\(?\d{1,2}[)\.]\s+", s):
                if cur:
                    items.append(cur.strip())
                cur = ""
            else:
                cur = (cur or "") + s
        if cur:
            items.append(cur.strip())

    # 3) –ø–æ ';'
    if not items and q.count(";") >= 2:
        items = [p.strip() for p in q.split(";") if p.strip()]

    items = [re.sub(r"\s+", " ", it).strip(" .;‚Äî-") for it in items if it and len(it.strip()) >= 2]
    if len(items) < min_items:
        return []
    return items[:max_items]

def retrieve_for_items(
    owner_id: int,
    doc_id: int,
    items: List[str],
    *,
    per_item_k: int = 2,
    prelim_factor: int = 4,
    avoid_refs_when_not_asked: bool = True,
    overall_backfill_from_query: Optional[str] = None,
    backfill_k: int = 4,
) -> Dict[str, List[Dict]]:
    """
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ per_item_k —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–æ–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—â–∏—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è ¬´—Å–≤—è–∑–Ω–æ—Å—Ç–∏¬ª.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å id->—Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ (–∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –∫–∞–∫ –≤ retrieve()).
    """
    if not items:
        return {}

    pack = _load_doc(owner_id, doc_id)
    by_id: Dict[str, List[Dict]] = {}
    used_ids: set[int] = set()

    for idx, ask in enumerate(items, start=1):
        # 0) –°–Ω–∞—á–∞–ª–∞ ‚Äî —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: —Ç–∞–±–ª–∏—Ü—ã/—Ä–∏—Å—É–Ω–∫–∏, –µ—Å–ª–∏ —è–≤–Ω–æ —É–ø–æ–º—è–Ω—É—Ç—ã
        special = _inject_special_sources_for_item(pack, ask, used_ids, doc_id=doc_id)
        for sp in special:
            sp["for_item"] = str(idx)
        picked: List[Dict] = list(special)

        # 1) –ó–∞—Ç–µ–º –æ–±—ã—á–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        rescored = _score_and_rank(pack, ask, prelim_k=max(per_item_k * prelim_factor, 16))
        for i, sc in rescored:
            m = pack["meta"][int(i)]
            if m["id"] in used_ids:
                continue
            if avoid_refs_when_not_asked and _chunk_type(m) == "reference":
                if not re.search(r"\b(–∏—Å—Ç–æ—á–Ω–∏–∫|–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä|reference|bibliograph)\w*\b", ask.lower()):
                    continue
            picked.append(
                {
                    "id": m["id"],
                    "page": m["page"],
                    "section_path": m["section_path"],
                    "text": (m["text"] or "").strip(),
                    "score": float(sc),
                    "for_item": str(idx),
                }
            )
            used_ids.add(m["id"])

        by_id[str(idx)] = picked

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è ¬´—Å–∫–ª–µ–π–∫–∞¬ª –æ–±—â–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ ‚Äî backfill
    if overall_backfill_from_query:
        rescored_q = _score_and_rank(pack, overall_backfill_from_query, prelim_k=max(backfill_k * 3, 16))
        extra = []
        for i, sc in rescored_q:
            m = pack["meta"][int(i)]
            if m["id"] in used_ids:
                continue
            if _chunk_type(m) == "reference":
                continue
            extra.append(
                {
                    "id": m["id"],
                    "page": m["page"],
                    "section_path": m["section_path"],
                    "text": (m["text"] or "").strip(),
                    "score": float(sc),
                    "for_item": None,
                }
            )
            used_ids.add(m["id"])
            if len(extra) >= backfill_k:
                break
        if extra:
            by_id["_backfill"] = extra

    return by_id

def retrieve_coverage(
    owner_id: int,
    doc_id: int,
    question: str,
    subitems: Optional[List[Any]] = None,
    *,
    per_item_k: int = 2,
    prelim_factor: int = 4,
    backfill_k: int = 4,
) -> Dict[str, Any]:
    """
    Coverage-aware –≤—ã–±–æ—Ä–∫–∞ –ø–æ–¥ –º–Ω–æ–≥–æ–ø—É–Ω–∫—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      {
        "items": [{"id": 1, "ask": "..."}, ...],
        "by_item": {"1":[...], "2":[...], ...},
        "by": {"1":[idx, ...], ...},          # –∫–∞—Ä—Ç–∞: –ø–æ–¥–ø—É–Ω–∫—Ç -> –∏–Ω–¥–µ–∫—Å—ã –≤ snippets (–¥–ª—è bot.py)
        "snippets": [...],
      }

    subitems –º–æ–∂–µ—Ç –±—ã—Ç—å:
      - —Å–ø–∏—Å–∫–æ–º —Å—Ç—Ä–æ–∫: ["–æ–ø–∏—à–∏ —Ä–∏—Å—É–Ω–æ–∫ 2.2", "—Ç–∞–±–ª–∏—Ü—É 2.1", "–≥–ª–∞–≤—É 1"];
      - —Å–ø–∏—Å–∫–æ–º —Å–ª–æ–≤–∞—Ä–µ–π –∏–∑ intents.detect_intents():
        [{"id": 1, "ask": "...", ...}, ...].
    """
    # –ï—Å–ª–∏ subitems –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –ø—ã—Ç–∞–µ–º—Å—è —Å–∞–º–∏ —Ä–∞—Å–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–ø—É–Ω–∫—Ç—ã
    if subitems is None:
        items_list: List[str] = list(plan_subitems(question)) if question else []
        items_norm = [{"id": i + 1, "ask": it} for i, it in enumerate(items_list)]
    else:
        # subitems –ø–µ—Ä–µ–¥–∞–Ω —è–≤–Ω–æ: –º–æ–∂–µ—Ç –±—ã—Ç—å list[str] –∏–ª–∏ list[dict]
        if subitems and isinstance(subitems[0], dict):
            # –§–æ—Ä–º–∞—Ç –∏–∑ intents.detect_intents(): —É–∂–µ –µ—Å—Ç—å id –∏ ask
            items_norm = []
            items_list = []
            for it in subitems:
                ask = (it.get("ask") or "").strip()
                if not ask:
                    continue
                # –µ—Å–ª–∏ id —É–∂–µ –µ—Å—Ç—å ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –ø—Ä–æ–Ω—É–º–µ—Ä—É–µ–º –ø–æ–∑–∂–µ
                items_norm.append({"id": it.get("id"), "ask": ask, **{k: v for k, v in it.items() if k not in {"id", "ask"}}})
                items_list.append(ask)
            # –ü—Ä–æ–Ω—É–º–µ—Ä—É–µ–º —Ç–µ —ç–ª–µ–º–µ–Ω—Ç—ã, —É –∫–æ–≥–æ id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            next_id = 1
            for it in items_norm:
                if it.get("id") is None:
                    it["id"] = next_id
                    next_id += 1
        else:
            # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç: —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫
            items_list = [str(it or "").strip() for it in subitems if str(it or "").strip()]
            items_norm = [{"id": i + 1, "ask": it} for i, it in enumerate(items_list)]

    if not items_list:
        # –ï—Å–ª–∏ –ø–æ–¥–ø—É–Ω–∫—Ç—ã –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã ‚Äî –æ–±—ã—á–Ω—ã–π retrieve —Å –Ω–µ–±–æ–ª—å—à–∏–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º k
        base = retrieve(owner_id, doc_id, question, top_k=max(10, backfill_k * 2))
        by_indices = {"_single": list(range(len(base)))}
        return {"items": [], "by_item": {"_single": base}, "by": by_indices, "snippets": base}

    # –Ω–∏–∂–µ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ª–æ–≥–∏–∫—É –∫–∞–∫ –µ—Å—Ç—å


    if items_list:
        by_item = retrieve_for_items(
            owner_id,
            doc_id,
            items_list,
            per_item_k=per_item_k,
            prelim_factor=prelim_factor,
            overall_backfill_from_query=question,
            backfill_k=backfill_k,
        )

        # round-robin –ø–æ—Ä—è–¥–æ–∫: 1-–π –∏–∑ –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞, –∑–∞—Ç–µ–º 2-–π –∏ —Ç.–¥.
        buckets = [by_item.get(str(i + 1), []) for i in range(len(items_list))]
        merged: List[Dict] = []
        for r in range(per_item_k):
            for b in buckets:
                if r < len(b):
                    merged.append(b[r])

        # –î–ª—è –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ –≤–∏–¥–∞ ¬´–¥–∞–π –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è¬ª ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –û–°–¢–ê–õ–¨–ù–´–ï —Å—Ç—Ä–æ—á–∫–∏ –∏—Ö bucket
        for i, ask in enumerate(items_list, start=1):
            if _wants_all_values(ask):
                bucket = by_item.get(str(i), [])
                if len(bucket) > per_item_k:
                    merged.extend(bucket[per_item_k:])

        # –¥–æ–±–∞–≤–∏–º backfill –≤ –∫–æ–Ω—Ü–µ
        if by_item.get("_backfill"):
            merged.extend(by_item["_backfill"])

        # –ü–æ—Å—Ç—Ä–æ–∏–º –∫–∞—Ä—Ç—É –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è bot.py: { "1": [indices in merged], ... }
        by_indices: Dict[str, List[int]] = {}
        for idx, sn in enumerate(merged):
            fid = sn.get("for_item")
            if fid:
                by_indices.setdefault(str(fid), []).append(idx)

        return {"items": items_norm, "by_item": by_item, "by": by_indices, "snippets": merged}

    # –ï—Å–ª–∏ –ø–æ–¥–ø—É–Ω–∫—Ç—ã –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã ‚Äî –æ–±—ã—á–Ω—ã–π retrieve —Å –Ω–µ–±–æ–ª—å—à–∏–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º k
    base = retrieve(owner_id, doc_id, question, top_k=max(10, backfill_k * 2))
    # –ï–¥–∏–Ω—ã–π ¬´by¬ª –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞: –≤—Å—è –≤—ã–¥–∞—á–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç "_single"
    by_indices = {"_single": list(range(len(base)))}
    return {"items": [], "by_item": {"_single": base}, "by": by_indices, "snippets": base}

def build_context_coverage(
    snippets: List[Dict],
    * ,
    items_count: Optional[int] = None,
    base_chars: int = 6000,
    per_item_bonus: int = 900,
    hard_limit: int = 18000,
) -> str:
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–±–æ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ –º–Ω–æ–≥–æ–ø—É–Ω–∫—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã:
      - –∏—Ç–æ–≥–æ–≤—ã–π –ª–∏–º–∏—Ç = base_chars + per_item_bonus*(items_count-1), –Ω–æ –Ω–µ –±–æ–ª—å—à–µ hard_limit;
      - —Ç–µ–∫—Å—Ç—ã —Å–∫–ª–µ–∏–≤–∞—é—Ç—Å—è –≤ round-robin –ø–æ—Ä—è–¥–∫–µ –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å for_item),
        —á—Ç–æ–±—ã –∫–∞–∂–¥—ã–π –ø–æ–¥–ø—É–Ω–∫—Ç –ø–æ—è–≤–∏–ª—Å—è –≤ –Ω–∞—á–∞–ª–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ö–æ—Ç—è –±—ã —Ä–∞–∑.
      - —É–¥–∞–ª—è—é—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø–æ id/section_path/—Ç–µ–∫—Å—Ç—É).
    """
    if not snippets:
        return ""

    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ –∏–∑ –º–µ—Ç–æ–∫ for_item, –µ—Å–ª–∏ —è–≤–Ω–æ –Ω–µ –∑–∞–¥–∞–Ω–æ
    if items_count is None:
        items_count = len({s.get("for_item") for s in snippets if s.get("for_item")}) or 1

    max_chars = min(hard_limit, int(base_chars + max(0, items_count - 1) * per_item_bonus))

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º
    groups: Dict[str, List[Dict]] = {}
    extras: List[Dict] = []
    for s in snippets:
        fid = s.get("for_item")
        if fid:
            groups.setdefault(str(fid), []).append(s)
        else:
            extras.append(s)

    # round-robin
    ordered: List[Dict] = []
    if groups:
        keys = sorted(groups.keys(), key=lambda x: (len(x), x))
        round_idx = 0
        more = True
        while more:
            more = False
            for k in keys:
                bucket = groups.get(k, [])
                if round_idx < len(bucket):
                    ordered.append(bucket[round_idx])
                    more = True
            round_idx += 1
        ordered.extend(extras)
    else:
        ordered = snippets[:]

    # –¥–µ–¥—É–ø –ø–æ id/section_path/—Ç–µ–∫—Å—Ç—É
    seen_ids: set[int] = set()
    seen_keys: set[str] = set()
    parts: List[str] = []
    total = 0
    for s in ordered:
        sid = s.get("id")
        spath = (s.get("section_path") or "").strip()
        raw = (s.get("text") or "")
        block = _clean_for_ctx(raw)
        if not block:
            continue

        k = f"{sid or 0}|{spath}|{hash(block)}"
        if sid and sid in seen_ids:
            continue
        if k in seen_keys:
            continue

        if total + len(block) > max_chars:
            remaining = max_chars - total
            if remaining <= 0:
                break
            block = block[:remaining]

        parts.append(block)
        total += len(block)
        if sid:
            seen_ids.add(sid)
        seen_keys.add(k)

        if total >= max_chars:
            break

    return "\n\n".join(parts)
