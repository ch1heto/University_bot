# app/indexing.py
import re
import json
import hashlib
import logging
import numpy as np
from typing import List, Dict, Any, Iterable, Optional
from decimal import Decimal

from .db import get_conn
from .polza_client import embeddings
from .chunking import split_into_chunks
from .config import Cfg
from .db import set_document_meta

# –º—è–≥–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è OCR
try:
    import pytesseract  # type: ignore
    from PIL import Image  # type: ignore
except Exception:
    pytesseract = None
    Image = None

# --------- OCR –∫–æ–Ω—Ñ–∏–≥ —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –¥–µ—Ñ–æ–ª—Ç–∞–º–∏ ---------
try:
    OCR_ENABLED = bool(getattr(Cfg, "OCR_ENABLED", True))
    OCR_LANG = getattr(Cfg, "OCR_LANG", "rus+eng")
    OCR_MIN_CHARS = int(getattr(Cfg, "OCR_MIN_CHARS", 12))
    OCR_MAX_IMAGES = int(getattr(Cfg, "OCR_MAX_IMAGES_PER_SECTION", 6))
except Exception:
    OCR_ENABLED = True
    OCR_LANG = "eng"
    OCR_MIN_CHARS = 12
    OCR_MAX_IMAGES = 6

logger = logging.getLogger(__name__)

# ---------- helpers ----------

def _norm(s: str | None) -> str:
    return (s or "").strip()


def _make_anchor_id(section_path: str, page: int | None, title: str | None) -> str:
    base = f"{_norm(section_path)}|p={page or ''}|t={_norm(title)}"
    h = hashlib.sha1(base.encode("utf-8")).hexdigest()
    return f"anch-{h[:16]}"


def _prefix(section: Dict[str, Any]) -> str:
    et = (section.get("element_type") or "").lower()
    title = _norm(section.get("title")) or "–î–æ–∫—É–º–µ–Ω—Ç"
    page = section.get("page")

    tag = "[–¢–µ–∫—Å—Ç]"
    if et == "heading":
        tag = "[–ó–∞–≥–æ–ª–æ–≤–æ–∫]"
    elif et == "table":
        tag = "[–¢–∞–±–ª–∏—Ü–∞]"
    elif et == "figure":
        tag = "[–†–∏—Å—É–Ω–æ–∫]"
    elif et == "page":
        tag = "[–°—Ç—Ä–∞–Ω–∏—Ü–∞]"

    parts = [tag]
    if page is not None:
        parts.append(f"—Å—Ç—Ä.{page}")
    parts.append(title)
    return " ".join(parts)


def _attach_anchors(attrs: dict, *, section_path: str, page: int | None, title: str | None) -> dict:
    out = dict(attrs or {})
    out.setdefault("section_title", _norm(title))
    out.setdefault("section_path_norm", _norm(section_path))
    out.setdefault("loc", {"page": page, "section_path": _norm(section_path)})
    out.setdefault("anchor_id", _make_anchor_id(section_path, page, title))
    return out


def _chunks_table_has(con, cols: List[str]) -> bool:
    cur = con.cursor()
    cur.execute("PRAGMA table_info(chunks)")
    have = {row[1] for row in cur.fetchall()}
    return all(c in have for c in cols)


def _batched(items: List[str], n: int) -> Iterable[List[str]]:
    for i in range(0, len(items), n):
        yield items[i:i + n]


def _limit_table_row_columns(row: str, max_cols: int) -> str:
    """
    –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —á–∏—Å–ª–æ –∫–æ–ª–æ–Ω–æ–∫ –≤ —Å—Ç—Ä–æ–∫–µ —Ç–∞–±–ª–∏—Ü—ã, –Ω–æ:
      - –ù–ï —É–¥–∞–ª—è–µ—Ç –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏ (–∏–Ω–∞—á–µ —Å–¥–≤–∏–≥–∞—é—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∏ –∏ "—Ç–µ—Ä—è—é—Ç—Å—è" –∑–Ω–∞—á–µ–Ω–∏—è)
      - –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–æ–∫ –±–æ–ª—å—à–µ max_cols ‚Äî –Ω–µ –æ—Ç—Ä–µ–∑–∞–µ—Ç —Ö–≤–æ—Å—Ç, –∞ —Å–∫–ª–µ–∏–≤–∞–µ—Ç —Ö–≤–æ—Å—Ç –≤ –ø–æ—Å–ª–µ–¥–Ω—é—é –∫–æ–ª–æ–Ω–∫—É
        (—Ç–∞–∫ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ–ø–∞–¥–∞—é—Ç, –ø—Ä–æ—Å—Ç–æ "—Å–∂–∏–º–∞—é—Ç—Å—è")
    """
    row = row or ""
    if max_cols <= 0:
        return row.strip()

    # split —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏ –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏, –µ—Å–ª–∏ –Ω–µ –¥–µ–ª–∞—Ç—å .strip() –ø–æ –≤—Å–µ–º—É —Å–ø–∏—Å–∫—É
    raw_parts = row.split(" | ")

    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–Ω—É—Ç—Ä–∏ —è—á–µ–µ–∫, –Ω–æ –ù–ï –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ
    parts = [(p or "").strip() for p in raw_parts]

    # –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–æ–∫ –º–µ–Ω—å—à–µ/—Ä–∞–≤–Ω–æ –ª–∏–º–∏—Ç—É ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å (—Å–æ—Ö—Ä–∞–Ω—è—è –ø—É—Å—Ç—ã–µ –ø–æ–∑–∏—Ü–∏–∏)
    if len(parts) <= max_cols:
        return " | ".join(parts).strip()

    # –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–æ–∫ –±–æ–ª—å—à–µ –ª–∏–º–∏—Ç–∞ ‚Äî —Ö–≤–æ—Å—Ç —Å–∫–ª–∞–¥—ã–≤–∞–µ–º –≤ –ø–æ—Å–ª–µ–¥–Ω—é—é –∫–æ–ª–æ–Ω–∫—É
    head = parts[: max_cols - 1]
    tail = parts[max_cols - 1 :]

    # —Å–∫–ª–µ–∏–º tail –æ–±—Ä–∞—Ç–Ω–æ, –Ω–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: –≤–Ω—É—Ç—Ä–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π —è—á–µ–π–∫–∏ —Ä–∞–∑–¥–µ–ª–∏–º " / "
    # (–º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥–æ–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –≥–ª–∞–≤–Ω–æ–µ ‚Äî —á—Ç–æ–±—ã –Ω–µ " | ", –∏–Ω–∞—á–µ —Å–Ω–æ–≤–∞ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—Å—è –∫–∞–∫ –∫–æ–ª–æ–Ω–∫–∏)
    last = " / ".join([t for t in tail if t != ""])
    # –µ—Å–ª–∏ –≤—Å–µ —Ö–≤–æ—Å—Ç–æ–≤—ã–µ –ø—É—Å—Ç—ã–µ ‚Äî –æ—Å—Ç–∞–≤–∏–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫
    if last == "" and any(t == "" for t in tail):
        last = ""

    out = head + [last]
    return " | ".join(out).strip()


def _json_safe(obj: Any) -> Any:
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–∏–≤–æ–¥–∏–º attrs –∫ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º—É –≤–∏–¥—É:
    - Decimal -> float
    - dict/list/tuple -> –æ–±—Ö–æ–¥–∏–º –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º
    –û—Å—Ç–∞–ª—å–Ω–æ–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å.
    """
    if isinstance(obj, Decimal):
        return float(obj)
    if isinstance(obj, dict):
        return {k: _json_safe(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        return [_json_safe(v) for v in obj]
    return obj



# ---------- OCR & CHART synthesis ----------

def _run_ocr_on_images_if_needed(base_attrs: dict) -> Optional[str]:
    """
    –ï—Å–ª–∏ –Ω–µ—Ç ocr_text, –Ω–æ –µ—Å—Ç—å images ‚Äî –¥–µ–ª–∞–µ–º OCR.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π) –∏–ª–∏ None, –µ—Å–ª–∏ OCR –Ω–µ –≤—ã–ø–æ–ª–Ω—è–ª—Å—è.
    """
    if not OCR_ENABLED or pytesseract is None or Image is None:
        return None

    if not isinstance(base_attrs, dict):
        return None

    ocr_text = (base_attrs.get("ocr_text") or "").strip()
    if ocr_text:
        return ocr_text  # —É–∂–µ –µ—Å—Ç—å

    images = base_attrs.get("images") or []
    if not isinstance(images, (list, tuple)) or not images:
        return None

    out_lines: List[str] = []
    processed = 0
    for pth in images:
        if processed >= max(1, OCR_MAX_IMAGES):
            break
        try:
            img = Image.open(pth)
        except Exception:
            continue
        try:
            txt = pytesseract.image_to_string(img, lang=OCR_LANG) or ""
        except Exception:
            txt = ""
        txt = txt.replace("\x0c", "").strip()
        if txt:
            out_lines.append(txt)
        processed += 1

    merged = "\n".join([t for t in out_lines if t.strip()]).strip()
    if merged and len(merged) >= OCR_MIN_CHARS:
        # –ø–æ–ª–æ–∂–∏–º –≤ attrs, —á—Ç–æ–±—ã –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–∞—Ö/–ø–æ–≤—Ç–æ—Ä–∞—Ö
        base_attrs["ocr_text"] = merged
        return merged
    return "" if merged else None


def _synth_chart_text(base_attrs: dict, section: Dict[str, Any]) -> Optional[str]:
    """
    –°—Ç—Ä–æ–∏–º —Ç–µ–∫—Å—Ç/—Ç–∞–±–ª–∏—á–∫—É –ø–æ –¥–∞–Ω–Ω—ã–º –¥–∏–∞–≥—Ä–∞–º–º—ã.

    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
    1) –ï—Å–ª–∏ –µ—Å—Ç—å attrs.chart_matrix (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤–∏–¥ –∏–∑ OOXML) ‚Äî
       —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ categories/series/values.
    2) –ï—Å–ª–∏ chart_matrix –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å attrs.chart_data (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç) ‚Äî
       —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ ¬´–º–µ—Ç–∫–∞ ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ¬ª.

    chart_matrix –Ω–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º, —Ç–æ–ª—å–∫–æ —á–∏—Ç–∞–µ–º.
    """
    chart_matrix = base_attrs.get("chart_matrix")
    chart_rows = base_attrs.get("chart_data")

    if not chart_matrix and (not isinstance(chart_rows, list) or not chart_rows):
        return None

    lines: List[str] = []
    numeric_vals: List[float] = []

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫/–∫–æ–Ω—Ç–µ–∫—Å—Ç
    cap_num = base_attrs.get("caption_num") or base_attrs.get("label")
    cap_tail = base_attrs.get("caption_tail") or base_attrs.get("title")
    head = None
    if (section.get("element_type") or "").lower() == "figure":
        if cap_num and cap_tail:
            head = f"–î–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã ¬´–†–∏—Å—É–Ω–æ–∫ {cap_num} ‚Äî {cap_tail}¬ª"
        elif cap_num:
            head = f"–î–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã ¬´–†–∏—Å—É–Ω–æ–∫ {cap_num}¬ª"
        elif cap_tail:
            head = f"–î–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã: {cap_tail}"
    if head:
        lines.append(head)

    def _fmt_percent(v: float) -> str:
        # 70.0 -> "70%", 70.35 -> "70.35%"
        if abs(v - round(v)) < 0.05:
            return f"{int(round(v))}%"
        s = f"{v:.2f}".rstrip("0").rstrip(".")
        return f"{s}%"

    def _fmt_number(v: float) -> str:
        return f"{v:.6g}"

    # --- 1) –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π matrix ‚Üí —Ç–∞–±–ª–∏—á–∫–∞ ---
    if isinstance(chart_matrix, dict):
        cats = chart_matrix.get("categories") or []
        series_list = chart_matrix.get("series") or []
        if isinstance(cats, list) and cats and isinstance(series_list, list) and series_list:
            # –∏–º–µ–Ω–∞ —Å–µ—Ä–∏–π
            ser_names: List[str] = []
            for i, s in enumerate(series_list):
                nm = (s or {}).get("name")
                nm = _norm(nm) if isinstance(nm, str) else ""
                ser_names.append(nm or f"–°–µ—Ä–∏—è {i + 1}")

            # –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
            header_cells = ["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"] + ser_names
            lines.append(" | ".join(header_cells))

            # —Å—Ç—Ä–æ–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            for ri, cat in enumerate(cats):
                cat_name = _norm(cat) or f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {ri + 1}"
                row_cells = [cat_name]
                for s in series_list:
                    vals = (s or {}).get("values") or []
                    unit_s = (s or {}).get("unit") or chart_matrix.get("unit")
                    v = vals[ri] if ri < len(vals) else None

                    if isinstance(v, (int, float)):
                        fv = float(v)
                        numeric_vals.append(fv)
                        if unit_s == "%":
                            cell = _fmt_percent(fv)
                        else:
                            cell = _fmt_number(fv)
                    else:
                        cell = "-"
                    row_cells.append(cell)
                lines.append(" | ".join(row_cells))

            # –∫–æ—Ä–æ—Ç–∫–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ –ø–æ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏/—Ç–∏–ø—É (–µ—Å–ª–∏ –µ—Å—Ç—å)
            meta = chart_matrix.get("meta") or {}
            bar_dir = meta.get("bar_dir")
            if bar_dir:
                lines.append(f"–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–∏–∞–≥—Ä–∞–º–º—ã: {bar_dir}.")

    # --- 2) –§–æ–ª–±—ç–∫: —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç chart_data ---
    if (not lines or len(lines) == (1 if head else 0)) and isinstance(chart_rows, list) and chart_rows:
        for r in chart_rows:
            if not isinstance(r, dict):
                continue

            label = _norm(str(r.get("label", "")))

            raw = (r.get("value_raw") or "").strip()
            unit = r.get("unit")
            val = r.get("value", None)

            num_val: Optional[float] = None
            if isinstance(val, (int, float)):
                num_val = float(val)
                numeric_vals.append(num_val)

            vstr = ""
            if raw:
                if (unit == "%") and ("%" not in raw) and isinstance(num_val, float):
                    vstr = _fmt_percent(num_val)
                else:
                    vstr = raw
            elif isinstance(num_val, float):
                if unit == "%":
                    vstr = _fmt_percent(num_val)
                else:
                    vstr = _fmt_number(num_val)
            else:
                if r.get("value") is not None:
                    vstr = str(r.get("value"))

            if label and vstr:
                lines.append(f"{label} ‚Äî {vstr}")
            elif label:
                lines.append(label)
            elif vstr:
                lines.append(vstr)

    # –ü—Ä–æ—Å—Ç–∞—è —á–∏—Å–ª–æ–≤–∞—è —Å–≤–æ–¥–∫–∞ (–¥–ª—è –ª—é–±—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —á–∏—Å–µ–ª)
    if numeric_vals:
        s = sum(numeric_vals)
        mn = min(numeric_vals)
        mx = max(numeric_vals)
        lines.append(f"–°—É–º–º–∞ = {s:g}; –º–∏–Ω = {mn:g}; –º–∞–∫—Å = {mx:g}")

    txt = "\n".join([ln for ln in lines if ln.strip()]).strip()
    if txt:
        base_attrs["chart_text"] = txt  # —Å–æ—Ö—Ä–∞–Ω–∏–º, matrix –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
        return txt
    return None


def _yield_ocr_chunks_if_any(
    section: Dict[str, Any],
    base_attrs: dict,
) -> Iterable[tuple[str, Dict[str, Any], str, dict]]:
    """
    –ï—Å–ª–∏ —É —Å–µ–∫—Ü–∏–∏ –µ—Å—Ç—å ocr_text ‚Äî —Ä–µ–∂–µ–º –∏ –æ—Ç–¥–∞—ë–º.
    –ï—Å–ª–∏ ocr_text –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å images ‚Äî –ø—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å OCR –∏ —Ç–∞–∫–∂–µ –æ—Ç–¥–∞—ë–º.
    subtype = "ocr".

    –í–ê–ñ–ù–û:
    - –î–ª—è —Ñ–∏–≥—É—Ä —Å chart_data (–¥–∏–∞–≥—Ä–∞–º–º—ã –∏–∑ OOXML/–¥–æ–∫x) OCR –ù–ï –≤—ã–ø–æ–ª–Ω—è–µ–º,
      —á—Ç–æ–±—ã –Ω–µ –ø–æ–¥–º–µ—à–∏–≤–∞—Ç—å ¬´—à—É–º–Ω—ã–µ¬ª —á–∏—Å–ª–∞ –∏–∑ –∫–∞—Ä—Ç–∏–Ω–∫–∏.
    """
    et = (section.get("element_type") or "").lower()
    if not isinstance(base_attrs, dict):
        return

    # —Ñ–∏–≥—É—Ä–∞ —Å chart_data -> –Ω–µ –¥–µ–ª–∞–µ–º OCR-—á–∞–Ω–∫–∏
    if et == "figure":
        chart_rows = base_attrs.get("chart_data")
        if isinstance(chart_rows, list) and chart_rows:
            return

    # OCR –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    ocr_text_existing = (base_attrs.get("ocr_text") or "").strip()
    if not ocr_text_existing:
        maybe = _run_ocr_on_images_if_needed(base_attrs)
        if isinstance(maybe, str):
            ocr_text_existing = maybe.strip()

    if not ocr_text_existing:
        return

    page = section.get("page")
    section_path = _norm(section.get("section_path")) or _norm(section.get("title")) or "–î–æ–∫—É–º–µ–Ω—Ç"
    for ch in split_into_chunks(ocr_text_existing):
        if not ch.strip():
            continue
        attrs = dict(base_attrs)
        attrs["subtype"] = "ocr"
        # –¥–ª—è OCR –æ—Å—Ç–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–∏–ø —Å–µ–∫—Ü–∏–∏ (table/figure/...), –∞ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç ‚Äî —Å—á–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç–æ–º
        out_et = et or "text"
        yield ch, {"page": page, "section_path": section_path}, out_et, attrs


def _yield_chart_chunks_if_any(
    section: Dict[str, Any],
    base_attrs: dict,
) -> Iterable[tuple[str, Dict[str, Any], str, dict]]:
    """
    –ï—Å–ª–∏ —É —Å–µ–∫—Ü–∏–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã (chart_matrix –∏–ª–∏ chart_data) ‚Äî
    —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∏ –æ—Ç–¥–∞—ë–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏.
    subtype = "chart".

    chart_matrix —Ü–µ–ª–∏–∫–æ–º –ø—Ä–æ–∫–∏–¥—ã–≤–∞–µ–º –¥–∞–ª—å—à–µ –≤ attrs —á–∞–Ω–∫–æ–≤.
    """
    et = (section.get("element_type") or "").lower()
    if not isinstance(base_attrs, dict):
        return

    chart_txt = base_attrs.get("chart_text")
    if not chart_txt:
        chart_txt = _synth_chart_text(base_attrs, section)

    if not chart_txt:
        return

    page = section.get("page")
    section_path = _norm(section.get("section_path")) or _norm(section.get("title")) or "–î–æ–∫—É–º–µ–Ω—Ç"
    for ch in split_into_chunks(chart_txt):
        if not ch.strip():
            continue
        attrs = dict(base_attrs)  # chart_matrix / chart_data —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å
        attrs["subtype"] = "chart"
        out_et = et or "figure"
        yield ch, {"page": page, "section_path": section_path}, out_et, attrs


def _yield_chunks_for_section(
    section: Dict[str, Any],
    * ,
    max_table_rows: Optional[int] = None,
    max_table_cols: Optional[int] = None,
) -> Iterable[tuple[str, Dict[str, Any], str, dict]]:
    et = (section.get("element_type") or "").lower()
    title = _norm(section.get("title")) or "–î–æ–∫—É–º–µ–Ω—Ç"
    section_path = _norm(section.get("section_path")) or title
    page = section.get("page")
    base_attrs = dict(section.get("attrs") or {})
    text = section.get("text") or ""

    # —è–∫–æ—Ä—è
    base_attrs = _attach_anchors(base_attrs, section_path=section_path, page=page, title=title)

    # –∑–∞–≥–æ–ª–æ–≤–æ–∫ ‚Äî –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ò –Ω–∞–∑–≤–∞–Ω–∏–µ, –ò —Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if et == "heading":
        head_txt = _prefix(section)
        yield head_txt, {"page": page, "section_path": section_path}, "heading", base_attrs
        # –¢–∞–∫–∂–µ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å–µ–∫—Ü–∏–∏, –µ—Å–ª–∏ –æ–Ω –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞
        body = (text or "").strip()
        if body and body != title and len(body) > len(title) + 10:
            for ch in split_into_chunks(body):
                if ch.strip():
                    yield ch, {"page": page, "section_path": section_path}, "text", base_attrs
        return

    # –∏—Å—Ç–æ—á–Ω–∏–∫
    if et == "reference":
        ref_text = (text or "").strip()
        if ref_text:
            yield ref_text, {"page": page, "section_path": section_path}, "reference", base_attrs
        return

    # —Ç–∞–±–ª–∏—Ü–∞
        # —Ç–∞–±–ª–∏—Ü–∞
    if et == "table":
        cap_tail = base_attrs.get("caption_tail") or base_attrs.get("title")
        header_preview = base_attrs.get("header_preview")

        tail_from_title = None
        m = re.search(
            r"(?i)\b—Ç–∞–±–ª(?:–∏—Ü–∞)?\.?\s*(?:‚Ññ\s*)?(?:[A-Za-z–ê-–Ø–∞-—è]\.?[\s-]*\d+(?:[.,]\d+)*|\d+(?:[.,]\d+)*)\s*[‚Äî\-‚Äì:\u2013\u2014]\s*(.+)",
            _norm(title)
        )
        if m:
            tail_from_title = _norm(m.group(1))

        root_text = cap_tail or header_preview or tail_from_title or "(—Ç–∞–±–ª–∏—Ü–∞)"
        yield root_text, {"page": page, "section_path": section_path}, "table", base_attrs

        lines = [ln.strip() for ln in (text or "").splitlines() if ln and ln.strip()]
        if not lines:
            # –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º OCR –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º –≤ attrs.images
            yield from _yield_ocr_chunks_if_any(section, base_attrs)
            # –∏ —Ç–µ–∫—Å—Ç –∏–∑ –¥–∏–∞–≥—Ä–∞–º–º—ã –≤–Ω—É—Ç—Ä–∏ —Ç–∞–±–ª–∏—Ü—ã (–µ—Å–ª–∏ –ø–∞—Ä—Å–µ—Ä –ø–æ–ª–æ–∂–∏–ª chart_matrix/chart_data)
            yield from _yield_chart_chunks_if_any(section, base_attrs)
            return

        n_max_rows = max_table_rows if max_table_rows is not None else Cfg.FULL_TABLE_MAX_ROWS
        n_max_cols = max_table_cols if max_table_cols is not None else Cfg.FULL_TABLE_MAX_COLS

        for i, row in enumerate(lines[: max(1, n_max_rows)], 1):
            trimmed = _limit_table_row_columns(row, max(0, n_max_cols or 0))
            attrs = dict(base_attrs)
            attrs["row_index"] = i
            row_section_path = f"{section_path} [row {i}]"
            attrs = _attach_anchors(attrs, section_path=row_section_path, page=page, title=title)
            yield trimmed, {"page": page, "section_path": row_section_path}, "table_row", attrs

        # OCR –∏ –¥–∏–∞–≥—Ä–∞–º–º–∞-—Ç–µ–∫—Å—Ç
            # –¥–æ–±–∞–≤–∏–º OCR-—á–∞–Ω–∫–∏ –∏ –¥–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º
        for txt, meta, etype, a in (_yield_ocr_chunks_if_any(section, base_attrs) or []):
            a = dict(a or {})
            # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ role/chapter_num –Ω–µ –ø–æ—Ç–µ—Ä—è—é—Ç—Å—è
            if "role" in base_attrs and not a.get("role"):
                a["role"] = base_attrs.get("role")
            if "chapter_num" in base_attrs and "chapter_num" not in a:
                a["chapter_num"] = base_attrs.get("chapter_num")
            yield txt, meta, etype, a

        for txt, meta, etype, a in (_yield_chart_chunks_if_any(section, base_attrs) or []):
            a = dict(a or {})
            if "role" in base_attrs and not a.get("role"):
                a["role"] = base_attrs.get("role")
            if "chapter_num" in base_attrs and "chapter_num" not in a:
                a["chapter_num"] = base_attrs.get("chapter_num")
            yield txt, meta, etype, a

        return


    # —Ñ–∏–≥—É—Ä—ã / —Å—Ç—Ä–∞–Ω–∏—Ü—ã / –æ–±—ã—á–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
    base = text if isinstance(text, str) else str(text)
    if et == "figure" and not base.strip():
        cap_num = base_attrs.get("caption_num") or base_attrs.get("label")
        cap_tail = base_attrs.get("caption_tail") or base_attrs.get("title")
        if cap_num and cap_tail:
            base = f"–†–∏—Å—É–Ω–æ–∫ {cap_num} ‚Äî {cap_tail}"
        elif cap_num:
            base = f"–†–∏—Å—É–Ω–æ–∫ {cap_num}"
        elif cap_tail:
            base = str(cap_tail)
        else:
            base = "–†–∏—Å—É–Ω–æ–∫"

    prefix = _prefix(section)
    for ch in split_into_chunks(base):
        if not ch.strip():
            continue
        # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏–ø: –¥–µ—Ñ–æ–ª—Ç ‚Äî text
        if et in {"page", "figure", "table", "heading", "reference", "text"}:
            out_et = et
        elif not et:
            out_et = "text"
        else:
            out_et = et
        yield f"{prefix}\n{ch}", {"page": page, "section_path": section_path}, out_et, base_attrs

    # –¥–æ–±–∞–≤–∏–º OCR-—á–∞–Ω–∫–∏ –∏ –¥–∞–Ω–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º
    yield from _yield_ocr_chunks_if_any(section, base_attrs)
    yield from _yield_chart_chunks_if_any(section, base_attrs)


# ---------- API ----------

# indexing.py

def index_document(
    owner_id: int,
    doc_id: int,
    sections: List[Dict[str, Any]],
    *,
    batch_size: int = 128
) -> None:
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç attrs (–≤–∫–ª—é—á–∞—è role/chapter_num) –≤ chunks.attrs.
    """
    try:
        meta = _extract_intro_meta_from_sections(sections or [])
        if meta:
            set_document_meta(doc_id, meta)
    except Exception:
        logger.exception("index_document: failed to extract intro meta for doc_id=%s", doc_id)

    rows_text: List[str] = []
    rows_meta: List[Dict[str, Any]] = []
    rows_type: List[str] = []
    rows_attrs: List[dict] = []

    for s in sections or []:
        for txt, meta, etype, attrs in _yield_chunks_for_section(
            s,
            max_table_rows=Cfg.FULL_TABLE_MAX_ROWS,
            max_table_cols=Cfg.FULL_TABLE_MAX_COLS,
        ):
            if not txt.strip():
                continue

            rows_text.append(txt)
            rows_meta.append(meta)
            rows_type.append(etype)

            sec_attrs = s.get("attrs") if isinstance(s.get("attrs"), dict) else {}
            chunk_attrs = attrs if isinstance(attrs, dict) else {}

            merged_attrs = dict(sec_attrs)
            merged_attrs.update(chunk_attrs)

            # üîí –∑–∞—â–∏—Ç–∞: chunk_attrs –Ω–µ –¥–æ–ª–∂–µ–Ω —É–±–∏—Ç—å —Ä–æ–ª—å/–Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã
            if sec_attrs.get("role") and not merged_attrs.get("role"):
                merged_attrs["role"] = sec_attrs.get("role")

            if "chapter_num" in sec_attrs and "chapter_num" not in merged_attrs:
                merged_attrs["chapter_num"] = sec_attrs.get("chapter_num")

            rows_attrs.append(merged_attrs)

    if not rows_text:
        return

    con = get_conn()
    cur = con.cursor()
    has_extended_cols = _chunks_table_has(con, ["element_type", "attrs"])

    idx = 0
    for batch in _batched(rows_text, batch_size):
        vecs = embeddings(batch)
        if not vecs or len(vecs) != len(batch):
            raise RuntimeError("embeddings() –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤.")

        for j, vec in enumerate(vecs):
            k = idx + j
            meta = rows_meta[k]
            blob = np.asarray(vec, dtype=np.float32).tobytes()

            if has_extended_cols:
                safe_attrs = _json_safe(rows_attrs[k])
                cur.execute(
                    "INSERT INTO chunks(doc_id, owner_id, page, section_path, text, element_type, attrs, embedding) "
                    "VALUES(?,?,?,?,?,?,?,?)",
                    (
                        doc_id,
                        owner_id,
                        meta.get("page"),
                        meta.get("section_path"),
                        batch[j],
                        rows_type[k],
                        json.dumps(safe_attrs, ensure_ascii=False),
                        blob,
                    ),
                )
            else:
                cur.execute(
                    "INSERT INTO chunks(doc_id, owner_id, page, section_path, text, embedding) "
                    "VALUES(?,?,?,?,?,?)",
                    (doc_id, owner_id, meta.get("page"), meta.get("section_path"), batch[j], blob),
                )

        idx += len(batch)

    con.commit()
    con.close()

def _extract_intro_meta_from_sections(
    sections: List[Dict[str, Any]],
) -> Optional[Dict[str, Any]]:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –±–ª–æ–∫ –í–í–ï–î–ï–ù–ò–ï –∏ –≤—ã—Ç–∞—â–∏—Ç—å –∏–∑ –Ω–µ–≥–æ:
    - –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å (–ø–µ—Ä–≤—ã–µ –∞–±–∑–∞—Ü—ã),
    - –æ–±—ä–µ–∫—Ç,
    - –ø—Ä–µ–¥–º–µ—Ç,
    - —Ü–µ–ª—å,
    - –∑–∞–¥–∞—á–∏ (—Å–ø–∏—Å–æ–∫),
    - –≥–∏–ø–æ—Ç–µ–∑—É.

    –†–∞–±–æ—Ç–∞–µ—Ç —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏, –Ω–æ –ø–æ–¥ —Ç–∏–ø–æ–≤—ã–µ –í–ö–† –ø–æ–¥—Ö–æ–¥–∏—Ç.
    """
    if not sections:
        return None

    intro_text_parts: List[str] = []
    in_intro = False

    for s in sections:
        title_u = (s.get("title") or "").strip().upper()
        text = (s.get("text") or "").strip()
        sp_u = (s.get("section_path") or "").strip().upper()

        # 1) –°—Ç–∞—Ä—Ç –≤–≤–µ–¥–µ–Ω–∏—è
        is_intro_start = (
            "–í–í–ï–î–ï–ù–ò–ï" in title_u or
            text.upper().startswith("–í–í–ï–î–ï–ù–ò–ï") or
            "INTRODUCTION" in title_u or
            text.upper().startswith("INTRODUCTION") or
            ("–í–í–ï–î–ï–ù–ò–ï" in sp_u) or
            ("INTRODUCTION" in sp_u)
        )

        if is_intro_start:
            in_intro = True
            if text:
                intro_text_parts.append(text)
            continue

        # 2) –ï—Å–ª–∏ –º—ã —É–∂–µ –≤–Ω—É—Ç—Ä–∏ –≤–≤–µ–¥–µ–Ω–∏—è ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–æ–±–∏—Ä–∞—Ç—å, –ø–æ–∫–∞ –Ω–µ –Ω–∞—á–∞–ª—Å—è —Å–ª–µ–¥—É—é—â–∏–π –∫—Ä—É–ø–Ω—ã–π —Ä–∞–∑–¥–µ–ª
        if in_intro:
            # —Å—Ç–æ–ø-—É—Å–ª–æ–≤–∏—è: —Å–ª–µ–¥—É—é—â–∏–π –∫—Ä—É–ø–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫/–≥–ª–∞–≤–∞/–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª/–∑–∞–∫–ª—é—á–µ–Ω–∏–µ
            looks_like_new_major = (
                ("–ì–õ–ê–í–ê" in title_u) or
                ("–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï" in title_u) or
                re.match(r"^\s*\d+(\.\d+)*\b", (s.get("title") or "").strip()) is not None or
                re.match(r"^\s*–ì–õ–ê–í–ê\s+\d+", text.upper()) is not None or
                re.match(r"^\s*\d+(\.\d+)*\b", text) is not None
            )

            # –µ—Å–ª–∏ section_path –±–æ–ª—å—à–µ –Ω–µ –ø—Ä–æ –≤–≤–µ–¥–µ–Ω–∏–µ –∏ –ø—Ä–∏ —ç—Ç–æ–º –Ω–∞—á–∞–ª—Å—è –Ω–æ–≤—ã–π –∫—Ä—É–ø–Ω—ã–π —Ä–∞–∑–¥–µ–ª ‚Äî –≤—ã—Ö–æ–¥–∏–º
            if looks_like_new_major and ("–í–í–ï–î–ï–ù–ò–ï" not in sp_u) and ("INTRODUCTION" not in sp_u):
                break

            if text:
                intro_text_parts.append(text)


        intro_text = "\n".join(intro_text_parts)

    # –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º
    def _find_after(label: str) -> Optional[str]:
        idx = intro_text.lower().find(label.lower())
        if idx < 0:
            return None
        tail = intro_text[idx + len(label) :]
        # –±–µ—Ä—ë–º –¥–æ –ø–µ—Ä–≤–æ–π —Ç–æ—á–∫–∏ / –ø–µ—Ä–µ–≤–æ–¥–∞ —Å—Ç—Ä–æ–∫–∏
        for sep in [".", "\n"]:
            cut = tail.find(sep)
            if cut > 0:
                return tail[:cut].strip(" :;\n\t")
        return tail.strip(" :;\n\t")

    relevance = None
    # –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –æ–±—ã—á–Ω–æ –≤ –ø–µ—Ä–≤—ã—Ö –∞–±–∑–∞—Ü–∞—Ö ‚Äî –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å 1‚Äì2 –ø–µ—Ä–≤—ã—Ö –∞–±–∑–∞—Ü–∞
    paragraphs = [p.strip() for p in intro_text.split("\n") if p.strip()]
    if paragraphs:
        relevance = paragraphs[0]
        # –µ—Å–ª–∏ –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ‚Äî –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤—Ç–æ—Ä–æ–π
        if len(relevance) < 200 and len(paragraphs) > 1:
            relevance = relevance + " " + paragraphs[1]

    obj = _find_after("–æ–±—ä–µ–∫—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    subj = _find_after("–ø—Ä–µ–¥–º–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    goal = _find_after("—Ü–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    if goal is None:
        goal = _find_after("—Ü–µ–ª—å—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    if goal is None:
        goal = _find_after("—Ü–µ–ª—å—é —Ä–∞–±–æ—Ç—ã")

    # –∑–∞–¥–∞—á–∏ —á–∞—Å—Ç–æ –∏–¥—É—Ç —Å–ø–∏—Å–∫–æ–º –ø–æ—Å–ª–µ —Ñ—Ä–∞–∑—ã
    tasks_block = None
    # üîß —Ä–∞—Å—à–∏—Ä–∏–ª–∏ —Å–ø–∏—Å–æ–∫ –º–∞—Ä–∫–µ—Ä–æ–≤ –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –≤–æ –≤–≤–µ–¥–µ–Ω–∏—è—Ö –í–ö–†
    for marker in [
        "–∑–∞–¥–∞—á–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
        "–±—ã–ª–∏ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏",
        "–ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏",
        "–±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
        "—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
        "–±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏",
        "—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏",
        "–¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ—à–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏",
        "–¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ—à–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏",
        "–æ—Å–Ω–æ–≤–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —è–≤–ª—è—é—Ç—Å—è",
    ]:
        idx = intro_text.lower().find(marker.lower())
        if idx >= 0:
            tasks_block = intro_text[idx + len(marker) :]
            break

    tasks: List[str] = []
    if tasks_block:
        for line in tasks_block.split("\n"):
            line = line.strip(" \t-‚Ä¢‚Äî;:")
            if not line:
                continue
            # –ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å —Ü–∏—Ñ—Ä—ã / –º–∞—Ä–∫–µ—Ä–∞, —Å—á–∏—Ç–∞–µ–º –∑–∞–¥–∞—á–∞–º–∏
            if line[0].isdigit() or line.startswith(("-", "‚Äî")):
                tasks.append(line)
            elif tasks:
                # –µ—Å–ª–∏ —É–∂–µ –Ω–∞—á–∞–ª—Å—è —Å–ø–∏—Å–æ–∫, –∞ —Å—Ç—Ä–æ–∫–∞ –±–µ–∑ —Ü–∏—Ñ—Ä—ã ‚Äî –º–æ–∂–Ω–æ –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–∏—Ç—å –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π
                tasks[-1] += " " + line

    # üîß fallback: –µ—Å–ª–∏ –±–ª–æ–∫ –∑–∞–¥–∞—á –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º –Ω–µ –≤—ã—Ç–∞—â–∏–ª—Å—è, –Ω–æ —Å–ª–æ–≤–æ ¬´–∑–∞–¥–∞—á¬ª –µ—Å—Ç—å –≤ –≤–≤–µ–¥–µ–Ω–∏–∏,
    # –ø—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∑–∞–¥–∞—á–∞–º–∏ –∏–∑ —Ö–≤–æ—Å—Ç–∞ —Ç–µ–∫—Å—Ç–∞
    if not tasks and "–∑–∞–¥–∞—á" in intro_text.lower():
        tail = intro_text.split("–∑–∞–¥–∞—á", 1)[1]
        for line in tail.split("\n"):
            line = line.strip(" \t-‚Ä¢‚Äî;:")
            if not line:
                continue
            if line[0].isdigit() or line.startswith(("-", "‚Äî")):
                tasks.append(line)
            elif tasks:
                tasks[-1] += " " + line


    hypothesis = _find_after("–≥–∏–ø–æ—Ç–µ–∑–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    if hypothesis is None:
        hypothesis = _find_after("–≥–∏–ø–æ—Ç–µ–∑–∞:")

    meta = {
        "relevance": relevance,
        "object": obj,
        "subject": subj,
        "goal": goal,
        "tasks": tasks,
        "hypothesis": hypothesis,
    }

    # –µ—Å–ª–∏ –≤—Å—ë –ø—É—Å—Ç–æ ‚Äî –Ω–µ—Ç —Å–º—ã—Å–ª–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å
    if not any(meta.values()):
        return None

    return meta
